**You:**

1805.00907v3.pdf
PDF
Several prior systems use a compiler-oriented approach to optimizing neural networks. TensorFlow‚Äôs
XLA [2] compiles neural networks for CPUs, GPUs
and accelerators. It is a practical compiler that solves
actual problems. XLA is used in production to drive
a massive fleet of accelerators at Google. It lowers
nodes into primitive linear algebra operations, and
then calls into a backend-specific library for different
backend (such as Eigen [9] for CPUs, or cuDNN [10]
for GPUs) to perform the bulk of computation. We
point out that XLA emits vectorized LLVM intermediate representation (IR) [11] for some nodes (such
as dot), and relies on the LLVM vectorizer [12] for
other nodes. It aims to provide a backend flexibility
for (...) A[ i] = 3;
for (...) A[ i] = 4;
return A [0];
Figure 1: Compilers struggle to analyze and optimize
this code when the two loops come from two different
nodes in the dataflow graph.
for TensorFlow, in a similar way that Glow is working toward providing for PyTorch and other neural
network frameworks.
TVM/NNVM [3, 13] lowers nodes into a low-level
Halide-based IR wherein loop-based optimizations can
be performed. Halide [14] is then used to generate
LLVM or CUDA/Metal/OpenCL source code. On the
other hand, DLVM [15] lowers DLVM IR into LLVM
IR, benefiting from the LLVM‚Äôs mature optimizer and
code generator.
Yet another approach is taken by nGraph [16],
which consumes a framework‚Äôs (such as Tensorflow)
compute graph to represent internally in a single level
IR, and then lowers that to different backends such
as cuDNN and MKL-DNN [17].
Finally, Tensor Comprehensions [18] provides a language for neural network developers to specify their
networks such that a JIT compiler can algorithmically
search for the most efficient execution plan possible.
This execution plan is then generated in a language
suited for a specific backend, such as CUDA for a
GPU, and compiled by a compiler for that language.
Tensor Comprehensions is a good solution for programmers that seek to create new operators that do
not exist today and execute them efficiently.
Similar to Glow, these systems include one or more
levels of IR (Section 3) which represent the compute
graph of some neural network model. Additionally,
like Glow, many represent tensors as first-class members with a shape and an element type. Glow uses
multiple levels of its own IR in the entire stack, and
leaves it up to each backend to implement further lowering if desired. For example, Glow‚Äôs CPU backend
executes low-level Glow instructions and calls into its
own libjit standard library kernels impleme

From this paragraph, what are the main pros and cons of each of these. Why is Glow better or worse

* * *

**ChatGPT:**

Here‚Äôs a summary of the main pros and cons of each compiler system discussed in the paragraph ‚Äî XLA, TVM/NNVM, DLVM, nGraph, Tensor Comprehensions, and Glow ‚Äî followed by how Glow compares to them overall.

üß© 1. TensorFlow XLA

Pros:

Production-ready and widely deployed ‚Äî used across Google‚Äôs accelerator fleet, solving real-world efficiency problems.

Flexible backend support for CPUs, GPUs, and TPUs.

Integrates with existing backend libraries (e.g., Eigen for CPU, cuDNN for GPU), avoiding reimplementation.

Emits LLVM IR, benefiting from LLVM optimizations and portability.

Cons:

Relies on external libraries for much of the computation, which can limit optimization opportunities.

Limited visibility into kernel internals, since most heavy lifting happens in backend libraries (harder to fuse ops or perform end-to-end graph optimizations).

Optimization granularity is inconsistent ‚Äî some ops (like dot) are vectorized manually, others depend on LLVM‚Äôs general-purpose vectorizer

1805.00907v3

.

üß± 2. TVM / NNVM

Pros:

Strong low-level optimization using a Halide-based IR, enabling loop-level transformations and scheduling.

Can target multiple hardware backends (LLVM, CUDA, Metal, OpenCL).

Good modularity: clear separation between frontend compute graph (NNVM) and backend schedule search (TVM).

Cons:

Complex autotuning process (especially before AutoTVM/meta-scheduling matured).

Relies heavily on Halide scheduling primitives, which can make some optimizations difficult for irregular workloads.

Lacks strong type semantics for tensors compared to Glow‚Äôs IR design.

üß¨ 3. DLVM

Pros:

Leverages LLVM‚Äôs mature optimizer by lowering DLVM IR directly to LLVM IR.

Simple design with a clear, single IR representation.

Cons:

Less domain-specific optimization for neural networks; relies heavily on LLVM rather than custom graph-level transformations.

Limited layering: a single IR misses opportunities for domain-aware and memory-aware passes at different abstraction levels.

üßÆ 4. nGraph

Pros:

Unified intermediate representation that consumes compute graphs from multiple frameworks (e.g., TensorFlow).

Multiple backend support (cuDNN, MKL-DNN), offering hardware flexibility.

Cons:

Single-level IR ‚Äî less expressive for domain-specific and low-level memory optimizations.

Relies on backend libraries rather than compiler-generated kernels, reducing optimization control.

üßë‚Äçüíª 5. Tensor Comprehensions

Pros:

Highly flexible language for defining new neural operators.

JIT-based autotuning automatically searches for optimal execution plans per backend (e.g., CUDA).

Ideal for research and prototyping new custom ops efficiently.

Cons:

Less suited for end-to-end model compilation (better for defining individual kernels).

Requires users to write custom mathematical specifications, which increases complexity.

JIT search overhead can be high for production environments.

‚ö°Ô∏è 6. Glow (Facebook)

Pros:

Multi-level IR design:

High-level IR for graph and domain-specific optimizations.

Low-level IR for memory scheduling, instruction selection, and backend specialization

1805.00907v3

.

Node lowering phase reduces the number of high-level operators each backend must support (e.g., FullyConnected ‚Üí MatMul + Add)

1805.00907v3

.

Backend-agnostic scalability ‚Äî backends can reuse common infrastructure and only implement primitives.

Strong typing system with explicit tensor shapes and element types improves correctness and compile-time reasoning.

Per-backend extensibility ‚Äî CPU backend, for example, uses LLVM to JIT specialized kernels, perform operator stacking/fusion, and exploit SIMD

1805.00907v3

.

Quantization-aware optimization pipeline, including profile-guided quantization and rescale folding.

Runtime partitioning and scheduling across heterogeneous accelerators.

Cons:

Higher implementation complexity (multiple IRs, lowering phases, custom libjit).

Less dynamic flexibility than JIT-based systems like Tensor Comprehensions or PyTorch 2.0‚Äôs Dynamo/AOT Autograd.

Performance depends heavily on backend implementation ‚Äî a weak backend may limit Glow‚Äôs potential.

üß† Why Glow Can Be Considered ‚ÄúBetter‚Äù

Glow strikes a balance between generality (like XLA) and hardware-specific control (like TVM):

It separates high-level graph optimizations from low-level code generation, letting each layer specialize.

Its node-lowering strategy avoids the combinatorial explosion of implementing every operator for every device.

Static memory allocation and operator stacking yield strong performance on CPUs without external libraries.

In evaluations, Glow achieved up to 2.7√ó speedup over TensorFlow-XLA and 1.3√ó over TVM on ResNet-50

1805.00907v3

.

However, Glow is less flexible for rapidly changing dynamic workloads or for exploratory operator design ‚Äî areas where Tensor Comprehensions and TVM excel.

‚úÖ In short
System	Strength	Weakness
XLA	Production-ready, stable, multi-backend	Limited kernel-level optimization, relies on external libs
TVM/NNVM	Halide-based low-level optimization, portable	Complex tuning, less rich type semantics
DLVM	Simplicity, LLVM integration	Limited domain-specific optimization
nGraph	Unified IR, backend flexibility	Shallow IR, backend library reliance
Tensor Comprehensions	Custom op autotuning, research-friendly	JIT overhead, narrow scope
Glow	Multi-level IR, node lowering, backend scalability, strong typing	Complex compiler, backend-dependent performance

Would you like me to make a comparison table showing how each system handles IR structure, backend extensibility, and optimization level (e.g., graph-, loop-, or instruction-level)? That would make the distinctions even clearer.

* * *

**You:**

These are all for graph lowering right?

* * *

**ChatGPT:**

Excellent question ‚Äî and yes, they‚Äôre all part of the ‚Äúgraph-lowering‚Äù family of compiler-oriented approaches, but they sit at different levels of abstraction and make different trade-offs in how they lower neural network graphs into executable code.

Let‚Äôs unpack that precisely:

üß© What ‚ÄúGraph Lowering‚Äù Means

In ML compilers, graph lowering refers to the process of progressively transforming a high-level computational graph (e.g., Conv ‚Üí BatchNorm ‚Üí ReLU) into:

Lower-level primitives (like matrix multiplies, adds, and elementwise ops),

Loop-based or tensor-IR representations that describe how computation happens,

Backend-specific code (e.g., LLVM IR, CUDA kernels, or Metal shaders).

This typically happens through multiple IR stages, where each stage exposes the graph in a simpler, more hardware-specific way ‚Äî until it‚Äôs ready to be compiled or executed.

‚öôÔ∏è How Each System Fits the ‚ÄúGraph Lowering‚Äù Pattern
System	Graph Lowering Stage	Style / Target	Key Mechanism
TensorFlow XLA	Lowers TensorFlow graph ‚Üí primitive linear algebra ops	LLVM IR, Eigen/cuDNN backends	Performs partial lowering, relying on external libraries for most compute (e.g., matmul ‚Üí cuDNN call).
TVM / NNVM	Lowers graph ‚Üí Halide IR (loop nests)	LLVM / CUDA / OpenCL	Does full lowering to a loop-level IR, enabling schedule and memory layout optimization.
DLVM	Lowers DLVM IR ‚Üí LLVM IR	LLVM backends	Performs a single-step lowering into LLVM IR, relying on LLVM‚Äôs generic optimizations.
nGraph	Converts framework graph ‚Üí single unified IR ‚Üí cuDNN / MKL-DNN	Vendor backends	Performs shallow lowering, mapping ops 1:1 to library calls.
Tensor Comprehensions	Translates op definitions ‚Üí polyhedral IR ‚Üí CUDA	CUDA compiler	On-demand JIT lowering, searching for optimal loop schedules via autotuning.
Glow	Performs multi-stage lowering:
‚Ä¢ High-level IR (graph ops)
‚Ä¢ Node-lowered IR (linear algebra ops)
‚Ä¢ Low-level IR (address-based instructions)	LLVM / libjit / accelerator code	Implements progressive graph lowering across multiple IRs ‚Äî enabling both domain-specific and memory-level optimization.
üí° The Key Difference

All of them lower graphs, but Glow formalizes graph lowering as its core design philosophy:

It introduces two explicit IR layers ‚Äî high-level (semantic) and low-level (address-based) ‚Äî rather than just lowering once into loops or LLVM IR.

Glow‚Äôs ‚Äúnode lowering‚Äù phase explicitly decomposes complex ops into basic linear algebra primitives (e.g., FullyConnected ‚Üí MatMul + Add) before backend codegen

1805.00907v3

.

Each backend can optionally perform additional lowering from Glow‚Äôs low-level IR into device-specific instructions.

üß† So in summary

‚úÖ Yes, all these systems perform graph lowering.

üîç But Glow stands out because it:

Treats graph lowering as a multi-layered, type-safe transformation pipeline,

Balances semantic optimizations (like op fusion and constant folding) with hardware-aware lowering (like memory scheduling and operator stacking),

Reduces the backend‚Äôs workload by only requiring a few primitive ops to be implemented per device.

If you want, I can diagram how ‚Äúgraph lowering depth‚Äù differs visually across them ‚Äî from shallow (nGraph) to deep (Glow/TVM). Would you like me to include that?

* * *

**You:**

where does MLIR fit into this space

* * *

**ChatGPT:**

Fantastic follow-up ‚Äî MLIR (Multi-Level Intermediate Representation) is actually a generalization of all the graph-lowering ideas we just discussed.
It was designed after these systems (Glow, XLA, TVM, DLVM, etc.) specifically to unify their compiler pipelines under one flexible, extensible IR framework.

Let‚Äôs break down exactly how MLIR fits in this ecosystem üëá

üß© 1. The Problem MLIR Solves

Before MLIR (circa 2019), every ML compiler‚ÄîGlow, XLA, TVM, DLVM, nGraph‚Äî
had built its own intermediate representations (IRs) and its own lowering pipelines.
That caused fragmentation:

Glow had two IRs (high-level + low-level).

XLA used HLO ‚Üí LLVM IR.

TVM used Halide IR.

DLVM used DLVM IR ‚Üí LLVM IR.

Tensor Comprehensions used a polyhedral IR.

Each duplicated infrastructure for parsing, scheduling, type inference, passes, etc.
MLIR was created to unify these efforts.

üí° In essence: MLIR is not another graph-lowering compiler ‚Äî it‚Äôs a framework for building graph-lowering compilers.

üß± 2. What MLIR Actually Provides

MLIR introduces a multi-dialect IR system, where each ‚Äúdialect‚Äù corresponds to a level of abstraction:

Dialect	Role	Example Ops
TF / TOSA / Torch / StableHLO	Framework or frontend-level graph	tf.Conv2D, torch.add
Linalg / Affine / Vector / SCF	Tensor or loop-level lowering	linalg.matmul, affine.for, vector.contract
LLVM Dialect	Hardware/ISA-level	LLVM ops like llvm.load, llvm.add

Each lowering step just converts from one dialect to another, staying inside MLIR.

This modular design:

Lets compilers share infrastructure (e.g., memory optimizers, shape inference).

Makes new hardware support easier ‚Äî just add a dialect for your accelerator.

Keeps optimizations composable and reusable across frameworks.

üß¨ 3. Relation to Other Systems

Here‚Äôs how MLIR conceptually absorbs or replaces older systems:

System	What MLIR Generalizes / Replaces
XLA	XLA‚Äôs HLO is now implemented as StableHLO / MHLO dialects in MLIR.
Glow	Glow‚Äôs high-IR and low-IR phases can both be represented as MLIR dialects (and some ideas directly inspired MLIR‚Äôs layering model).
TVM	TVM‚Äôs tensor-IR could, in principle, be an MLIR dialect (and TVM now has experimental MLIR bridges).
DLVM	MLIR adopted many DLVM design ideas ‚Äî strong typing, SSA form, compiler pipelines.
Tensor Comprehensions	MLIR‚Äôs affine and polyhedral dialects subsume TC‚Äôs scheduling capabilities.

So MLIR is the ‚Äúcompiler infrastructure of compilers‚Äù ‚Äî a shared substrate that others can plug into rather than reinvent.

‚ö° 4. Why MLIR Is Important for Graph Lowering

It formalizes progressive lowering ‚Äî the exact idea all these systems tried independently.

It provides a common IR ecosystem where you can:

Start from TensorFlow, PyTorch, or ONNX graphs,

Lower through structured tensor dialects,

Optimize memory layouts, fusion, and vectorization,

And end at LLVM IR or a custom accelerator dialect.

Example flow:

css
TorchScript Graph
   ‚Üì (Torch dialect)
TOSA / Linalg / Affine dialects
   ‚Üì
Vector / GPU dialect
   ‚Üì
LLVM dialect ‚Üí Machine Code


Each step is just one pass in MLIR‚Äôs framework ‚Äî not a hand-rolled lowering pipeline like Glow or TVM.

üß† 5. Summary: Where MLIR Fits
Aspect	Glow / XLA / TVM	MLIR
Type	Stand-alone ML compilers	Compiler infrastructure framework
Goal	Compile a NN graph to machine code	Provide reusable IRs for multi-level lowering
IR Model	Fixed (e.g., Glow high/low IR, XLA HLO)	Extensible dialect system
Scope	Neural networks	Any domain (ML, DSP, quantum, etc.)
Status	Application-specific	Foundation for many projects (TF, PyTorch, IREE, OpenXLA)
‚úÖ In Short

MLIR doesn‚Äôt replace graph-lowering systems ‚Äî it absorbs them.

Glow, XLA, and TVM each performed their own graph lowering.

MLIR codifies that concept into a unified, reusable infrastructure with first-class support for progressive lowering.

Modern compilers like IREE, OpenXLA, Torch-MLIR, and OneDNN Graph all use MLIR as their foundation.

If you‚Äôd like, I can show a side-by-side diagram comparing Glow‚Äôs two-IR stack vs. MLIR‚Äôs dialect hierarchy, to visually show how Glow‚Äôs design directly inspired MLIR‚Äôs multi-level structure. Want me to make that?

* * *

**You:**

I am very interested by the profile-driven quantization of this paper, was this method being explored at this time or was this one of the first examples of using profiling for quantization?

* * *

**ChatGPT:**

Short answer: Glow wasn‚Äôt the first to use profiling (‚Äúcalibration‚Äù) for post-training quantization, but it packaged the idea cleanly into a compiler pipeline.

Prior art (before/around Glow)

TensorRT INT8 calibration (2017) ‚Äî NVIDIA popularized post-training INT8 with a representative dataset and KL-divergence‚Äìbased threshold selection; i.e., run the model, collect activation histograms/ranges, then pick scales/zero-points from the observed data (no retraining). 
IIT Delhi CSE
+1

Google Jacob et al., CVPR 2018 ‚Äî ‚ÄúInteger-arithmetic-only inference‚Äù + quantization(-aware) training; also describes calibrating ranges for activations/weights and setting scale/zero-point from observed min/max (and simulating quantization during training). 
CVF Open Access
+1

Ristretto (2016) ‚Äî earlier dynamic fixed-point quantization that explicitly analyzes value distributions (weights/activations) to pick bit-widths/ranges. 
KITopen
+1

What Glow contributed

Glow‚Äôs ‚Äúprofile-guided quantization‚Äù makes calibration a first-class compiler pass:

It instruments the graph with profiling nodes, runs inference to record activation ranges, then recompiles the graph into a quantized form using those ranges. 

1805.00907v3

The compiler then performs static graph optimizations specific to quantization (e.g., minimizing float‚Üîint conversions; folding rescale ops; forming ‚Äúislands of integer computation‚Äù). 

1805.00907v3

 

1805.00907v3

Conceptually, this integrates calibration with node lowering and memory/layout passes, so more of the end-to-end pipeline benefits (not just kernels). 

1805.00907v3

Bottom line

By 2017‚Äì2018, profiling-based/PTQ calibration was already being explored and deployed (TensorRT, Google/CVPR‚Äô18, Ristretto). Glow‚Äôs novelty wasn‚Äôt inventing calibration‚Äîit was baking it into a multi-level IR compiler, where profiling ‚Üí quantize ‚Üí optimize is a coherent, repeatable pipeline rather than an external tool step. 

1805.00907v3

* * *

**You:**

Can you explain the operator stacking in the paper?

* * *

**ChatGPT:**

In Glow, operator stacking is a CPU-backend optimization that fuses consecutive data-parallel elementwise operations into a single efficient kernel execution.

Here‚Äôs the key idea from Section 5.2 of the paper:

üîß The problem

When you have sequential elementwise operators ‚Äî for example ReLU, then Add, then Sub ‚Äî a na√Øve implementation would iterate over the same large tensor three separate times. Each pass reloads data from memory and thrashes the cache, wasting bandwidth and CPU cycles

1805.00907v3

.

‚öôÔ∏è The solution: operator stacking

Glow detects such sequences and stacks them, so that multiple simple elementwise ops are executed in a single traversal of memory:

Each data element is loaded once.

All relevant arithmetic (e.g., ReLU ‚Üí Add ‚Üí Sub) is applied sequentially while the value is still in a register.

The result is written back once.

This optimization is performed at a high-level in Glow‚Äôs IR because general-purpose compilers like LLVM cannot see across neural-network operator boundaries

1805.00907v3

.

üß© Difference from operator fusion

Stacking is conceptually similar to operator fusion, but much more scalable:

In classic fusion, you‚Äôd need to write a new hand-tuned kernel for every possible combination (e.g., Conv+ReLU, Conv+ReLU+BatchNorm).

With stacking, Glow automatically generates fused kernels for all permutations of data-parallel ops ‚Äî no backend-specific implementation required

1805.00907v3

.

üöÄ Benefits

Immediate performance gains for chains of elementwise ops by eliminating redundant memory reads/writes.

Backend simplicity ‚Äî backends don‚Äôt need custom fused kernels for every op pattern.

Better lowering flexibility ‚Äî Glow can lower high-level ops confidently, knowing the backend can still recover fusion-like performance via stacking

1805.00907v3

.

It even helps on GPUs, where fewer kernel launches reduce overhead

1805.00907v3

.

In short, operator stacking in Glow generalizes fusion into an automated, memory-efficient pass that exploits data locality without requiring hand-crafted composite operators.

* * *

**You:**

so its basically automating operator fusion?

* * *

**ChatGPT:**

‚úÖ Exactly ‚Äî that‚Äôs the right intuition.

Glow‚Äôs operator stacking is essentially automated operator fusion, but with a key difference in how it‚Äôs achieved and what scope it applies to.

Let‚Äôs break down the nuance:

üß© Traditional Operator Fusion

In frameworks like TensorFlow XLA or TVM, ‚Äúfusion‚Äù usually means manually or semi-manually combining a known set of ops into one kernel for efficiency.
Examples:

Conv2D + BatchNorm + ReLU ‚Üí a single fused kernel

MatMul + BiasAdd ‚Üí a fused GEMM kernel

These fused patterns:

Are predefined (the compiler recognizes a few patterns),

Require custom kernels for each pattern on each backend,

Are not scalable ‚Äî as new ops are added, the number of possible combinations explodes.

‚öôÔ∏è Glow‚Äôs Operator Stacking

Glow takes a more systematic, compiler-driven approach:

It identifies sequences of data-parallel (elementwise) ops automatically (e.g., Add ‚Üí Sub ‚Üí ReLU ‚Üí Mul).

It stacks them into a single loop kernel that:

Loads each element once,

Applies all operations while the value is in registers,

Writes back once.

This is done programmatically, not by writing new kernels.

üí° Think of stacking as on-the-fly fusion that doesn‚Äôt require handcrafted fused ops.

üî¨ Why This Matters
Aspect	Manual Fusion	Glow‚Äôs Operator Stacking
Who defines fusion	Backend developer (hard-coded patterns)	Compiler pass (automatic)
Op scope	Predefined fused ops (e.g., Conv+BN+ReLU)	Any consecutive elementwise ops
Backend effort	Must write one kernel per combination	Backend just supports basic ops
Scalability	Poor ‚Äî combinatorial explosion	Excellent ‚Äî dynamic fusion at compile time
Optimization level	Graph-pattern level	IR-level (data-parallel loop fusion)
üß† Summary

Yes ‚Äî operator stacking is operator fusion,
but Glow makes it:

automatic (no manual pattern library),

generalized (any chain of pointwise ops), and

integrated into lowering so that backends remain simple while still getting fused performance.

It‚Äôs the compiler-generalized version of operator fusion ‚Äî turning what used to be manual kernel engineering into an optimization pass.

* * *

**You:**

I guess, why weren't we doing this before. It seems obvious to not handwrite kernels

* * *

**ChatGPT:**

That‚Äôs an incredibly insightful question ‚Äî and you‚Äôre right, it seems obvious in hindsight that compilers should automatically fuse simple operators instead of forcing people to hand-write kernels.

But there are very real historical and technical reasons why it took until systems like Glow (2018) and later MLIR, TVM, and XLA‚Äôs newer HLO fusion passes for this idea to become practical.

Let‚Äôs unpack why we weren‚Äôt doing this earlier üëá

üßÆ 1. The Legacy of Framework Design: ‚ÄúInterpreter‚Äù vs ‚ÄúCompiler‚Äù

Before 2016‚Äì2018, most deep-learning frameworks (TensorFlow 1.x, Caffe, Theano, Torch7) were interpreters, not compilers.

They executed each node in the compute graph one-by-one in Python or C++.

Each op (Conv, Add, ReLU, etc.) simply called into a vendor library (cuDNN, MKL, Eigen).

There was no unified IR where a compiler could even ‚Äúsee‚Äù multiple ops at once.

‚û°Ô∏è Result:
No global view of the program ‚Üí impossible to automatically fuse or stack operators.

‚öôÔ∏è 2. Kernel Libraries Were the ‚ÄúCompiler‚Äù

Vendors like NVIDIA and Intel invested huge effort into hand-optimized libraries:

cuDNN, MKL-DNN (oneDNN), Eigen, etc.

They already implemented fused variants (Conv+BiasAdd+ReLU, etc.) by hand because those gave massive speedups.

So frameworks simply delegated optimization to these libraries rather than building compilers themselves.

‚û°Ô∏è Result:
Developers didn‚Äôt need a compiler; the library was effectively the optimizer.
But it meant new fused patterns required manual kernel work.

üß† 3. Lack of Proper Intermediate Representations (IRs)

Until systems like XLA (2017), TVM (2018), and Glow (2018),
there wasn‚Äôt a suitable IR that could represent both:

High-level neural-net semantics (‚ÄúConv2D over float32 tensors‚Äù), and

Low-level loop/elementwise execution details (‚Äúapply op on each element‚Äù).

Glow‚Äôs multi-IR design was revolutionary because it bridged that gap:

High-level IR ‚Üí graph-level optimization.

Low-level IR ‚Üí memory layout, loop fusion, and instruction scheduling.

‚û°Ô∏è Only then could a compiler reason about ‚Äúthese ops can share a loop‚Äù and implement operator stacking.

üß© 4. General Compilers (LLVM, GCC) Couldn‚Äôt See Through Library Calls

Even though LLVM could fuse loops or vectorize code,
it had no visibility into neural-net ops, because those were opaque function calls like:

cpp
Eigen::Tensor<float> relu(Eigen::Tensor<float> x);


To LLVM, that‚Äôs just a black box ‚Äî it can‚Äôt tell that relu is a simple elementwise max.

‚û°Ô∏è Result:
You couldn‚Äôt rely on LLVM or GCC to automatically fuse these without a domain-specific compiler sitting on top (which is what Glow became).

üî¨ 5. Memory, Scheduling, and Graph Semantics Are ML-Specific

In neural networks, memory reuse, in-place operations, and tensor layout choices matter immensely.
General compilers don‚Äôt model those well ‚Äî they work on scalar loops, not tensor DAGs.

So it took ML-specific compiler research to:

Represent tensors as first-class IR objects,

Model memory lifetimes, and

Safely apply in-place or fused transformations.

Glow‚Äôs stacking was one of the first clean demonstrations of this idea in practice

1805.00907v3

.

‚ö° 6. The Ecosystem Shift (2017 ‚Üí 2019)

Once models grew huge and hardware diversified (TPUs, NPUs, edge accelerators), the old ‚Äújust call cuDNN‚Äù approach broke down:

You couldn‚Äôt rely on a single vendor library for every new accelerator.

You needed a compiler to automatically generate efficient kernels.

That‚Äôs when the field moved from ‚Äústatic graph interpreters‚Äù to graph-lowering compilers (XLA, TVM, Glow, MLIR).

üß† In summary
Before Glow	After Glow (and MLIR)
Framework executes each op separately via vendor library	Compiler sees full graph and can lower + fuse ops
Performance via hand-written fused kernels	Performance via automatic operator stacking / fusion
No IR connecting high-level ops to low-level code	Multi-level IR pipeline (domain ‚Üí tensor ‚Üí instruction)
Hardware-specific	Hardware-agnostic and scalable

So, yes ‚Äî it seems obvious now, but it wasn‚Äôt until we:

Had IRs expressive enough to represent both high- and low-level tensor operations, and

Began treating ML frameworks as compilers, not interpreters ‚Äî
that automation like Glow‚Äôs operator stacking became both possible and reliable.

If you‚Äôd like, I can show you how Glow‚Äôs operator stacking idea directly influenced later systems ‚Äî like XLA‚Äôs HLO fusion, TVM‚Äôs ‚Äúelementwise fusion‚Äù, and MLIR‚Äôs Linalg+Fusion passes ‚Äî to show the historical lineage. Would you like that?

* * *

**You:**

Is glow only for CPU backend? It seems to focus a lot of CPU

* * *

**ChatGPT:**

That‚Äôs a really sharp observation ‚Äî and you‚Äôre absolutely right that the Glow paper emphasizes the CPU backend heavily, but Glow itself was designed as a general, retargetable compiler framework, not a CPU-only system.

Let‚Äôs unpack that clearly üëá

‚öôÔ∏è 1. Glow‚Äôs Architecture Is Backend-Agnostic

From the very first paragraph of the paper:

‚ÄúGlow lowers the traditional neural network dataflow graph into a two-phase strongly-typed IR‚Ä¶ At the lowest level, the optimizer performs machine-specific code generation to take advantage of specialized hardware features.‚Äù

1805.00907v3

The design goal was heterogeneous hardware:

CPUs

GPUs

DSPs and ASIC accelerators

Specialized AI chips from partners like Intel, Qualcomm, Cadence, Habana, Esperanto ‚Äî all mentioned as supporting Glow

1805.00907v3

Glow‚Äôs name (‚ÄúGraph-Lowering‚Äù) literally refers to its mission: provide a common compiler layer that can lower high-level compute graphs (from PyTorch, Caffe2, etc.) into optimized code for any backend.

So structurally, Glow ‚â† CPU-only ‚Äî it‚Äôs a multi-backend compiler.

üß© 2. Why the Paper Focuses on CPU

The paper‚Äôs experiments and implementation details focus on the CPU backend for two reasons:

a. Demonstration platform

Glow‚Äôs CPU backend served as the reference implementation ‚Äî a proof of concept showing how its multi-level IR pipeline, lowering, quantization, and runtime work end-to-end.

They used the CPU backend to show:

LLVM-based JIT compilation,

Operator stacking/fusion,

Quantization passes,

ResNet-50 benchmark results vs TensorFlow & TVM.

It was easier to illustrate Glow‚Äôs design principles there because LLVM‚Äôs CPU codegen is mature and portable.

b. GPU & accelerator backends were vendor-specific

Other backends (e.g., GPUs, Habana, Qualcomm DSPs) were developed by hardware partners and not fully open-sourced at the time.
They relied on internal or NDA-protected toolchains (e.g., custom LLVM backends or C-level APIs).

So, the paper highlights what they could publicly demonstrate ‚Äî the CPU path.

üß† 3. How Glow Handles Multiple Backends

Glow defines a backend interface:

cpp
class Backend {
  virtual void compile(Function *F);
  virtual void optimize(Function *F);
  virtual void generateCode(Function *F);
};


Each backend implements:

Its own target-specific lowering (optional),

Custom instruction selection / scheduling,

Possibly its own memory allocator or kernel library.

For example:

The CPU backend lowers to LLVM IR and generates x86/ARM machine code.

A GPU backend could instead emit CUDA, OpenCL, or Metal kernels.

An ASIC backend could directly output firmware instructions or tensor microcode.

Glow‚Äôs IR design supports this flexibility by:

Having a high-level graph IR (target-independent optimizations),

A low-level address-based IR (memory scheduling and DMA operations),

Optional backend-specific lowering passes

1805.00907v3

1805.00907v3

.

üß™ 4. Example: Quantization + Heterogeneous Support

One big Glow feature ‚Äî profile-guided quantization ‚Äî is backend-independent.
Once the compiler converts a network into an 8-bit quantized form, any backend that supports INT8 arithmetic can reuse the same lowered graph.

This is precisely what made Glow attractive to accelerator vendors:
they could focus on a small number of primitive ops (e.g., matmul, add, conv) and let Glow handle all the high-level graph plumbing

1805.00907v3

.

üìä 5. Summary
Aspect	Glow‚Äôs Design
Primary demo backend in paper	‚úÖ CPU (LLVM codegen, JIT)
Supported backends (conceptually)	CPU, GPU, DSP, ASIC
Open-source maturity	CPU fully available, others partial/vendor
IR layers	High-level graph IR ‚Üí Low-level instruction IR ‚Üí Backend code
Main purpose	Provide a general graph-lowering compiler for heterogeneous ML hardware

So:
üß† Glow is not CPU-only ‚Äî it‚Äôs a CPU-centric demonstration of a general multi-target ML compiler.

The paper focused on CPUs because that was the cleanest open-source path to show its ideas (operator stacking, profile-driven quantization, lowering).
But the architecture was explicitly built to scale to accelerators, GPUs, and custom silicon, and indeed later versions of Glow were integrated into Facebook‚Äôs PyTorch/XNNPACK/NNPI stacks and partner chips.