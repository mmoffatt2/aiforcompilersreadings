**You:**

2304.01433v3 (1).pdf
PDF
Can you summarize this paper?

* * *

**ChatGPT:**

Here is a clear, complete summary of the paper
‚ÄúTPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings‚Äù
using the actual contents of the uploaded PDF.


2304.01433v3 (1)

üìå TPU v4 ‚Äî Summary

TPU v4 is Google‚Äôs 5th-generation Machine Learning accelerator and its 3rd-generation ML supercomputer. It introduces two major innovations:

Optical Circuit Switches (OCS) that reconfigure the interconnect topology across thousands of TPU chips.

SparseCore, a dedicated on-die accelerator optimized for embedding workloads (used heavily in recommendation systems).

These features provide large gains in scale, reliability, performance, energy efficiency, and support for modern LLMs and large-scale DLRMs.

üî∂ 1. Optical Circuit Switch (OCS): Reconfigurable Interconnect

TPU v4 replaces fixed electrical/optical cabling with optical circuit switches (MEMS-based) that allow the system topology to be dynamically reprogrammed.

Key benefits

Scales to 4096 chips (4√ó the TPU v3 max of 1024).

Massive reliability improvement: OCS can route around failed CPU hosts, maintaining high ‚Äúgoodput‚Äù even with 99.0‚Äì99.5% host availability.

Topology flexibility:

Standard 3D torus

Twisted 3D torus (gives 1.3‚Äì1.6√ó higher all-to-all throughput)

Arbitrary shapes (e.g., cigar-shaped slices for pipeline parallelism)

Simplifies scheduling: slices no longer need to be physically contiguous.

Faster deployment: racks can go online independently.

Security: optical isolation enables air-gapped multi-tenant slices.

Low cost (<5%) and low power (<3%) overhead.

Why this matters

Large LLMs require:

high bisection bandwidth,

configurable parallelism strategies,

fault tolerance at ~4k-chip scale,

which fixed topologies cannot provide.

üî∂ 2. SparseCore (SC): Hardware for Embeddings

DLRM-style recommendation models use huge embedding tables with sparse lookups. These operations:

are memory-bound and low compute-intensity,

require all-to-all communication,

scale to hundreds of billions of parameters,

and dominate workloads at Google.

TPU v4 incorporates 4 SparseCores per chip, each acting as a dataflow processor optimized for:

fast HBM-backed gather/scatter,

deduplication of embedding IDs,

variable-length sparse vector ops,

globally addressable ~128 TiB memory across the entire pod.

Performance impact

5√ó‚Äì7√ó faster embedding training vs CPU-hosted embeddings.

TPU v4 with SC is 30√ó faster than CPU clusters on internal DLRM workloads.

Embedding performance scales with bisection bandwidth; TPU v4‚Äôs 3D torus gives 1.1√ó‚Äì2√ó improvement over TPU v3.

üî∂ 3. TPU v4 Architectural Improvements

Compared to TPU v3:

Component	TPU v4	TPU v3
Process	7nm	16nm
Peak FLOPs	275 TFLOP/s	123 TFLOP/s
On-chip memory	160 MiB (incl. 128 MiB CMEM)	32 MiB
HBM bandwidth	1.2 TB/s	0.9 TB/s
SparseCores	4	2
Interconnect	16√ó50 GB/s	8√ó70 GB/s
Max system size	4096 chips	1024 chips

TPU v4 achieves:

2.1√ó performance

2.7√ó performance/Watt
vs TPU v3 across production workloads.

RNNs see 3.3√ó speedups due to CMEM bandwidth.

üî∂ 4. System-level AutoML (PA-NAS) + Topology Search

Google uses Platform-Aware Neural Architecture Search (PA-NAS) to jointly optimize:

model architecture,

SC/TC compute balance,

interconnect topology.

Results:

10% improvement on production DLRM after AutoML balancing SC vs TC.

2.3√ó improvement in LLM throughput by switching to a better topology (8√ó8√ó8 vs 4√ó8√ó16).

1.2√ó improvement over expert-configured GPT-3 pre-training setup.

üî∂ 5. Performance vs A100 and Graphcore IPU

Based on MLPerf Training v2.0:

Performance (same system size)

TPU v4 is 1.15√ó faster than A100 on BERT.

TPU v4 is 1.67√ó faster than A100 on ResNet.

TPU v4 is ~4.3√ó‚Äì4.5√ó faster than the Graphcore IPU Bow.

Energy

A100 uses 1.3√ó‚Äì1.9√ó more power than TPU v4 for the same MLPerf workloads.

üî∂ 6. Environmental Impact

Training on TPU v4 in Google Cloud vs on-prem DSAs results in:

2‚Äì6√ó lower machine energy

PUE of 1.10 vs global 1.57

~20√ó lower CO‚ÇÇe emissions

because TPU v4 pods run entirely on near-100% carbon-free energy in Oklahoma.

üî∂ 7. Why TPU v4 Matters

TPU v4 directly addresses the rapidly shifting ML workload landscape:

RNNs dropped from 21% ‚Üí 2%.

Transformers exploded from 21% ‚Üí 57%.

LLMs climbed to 31% of all training.

DLRMs remain ~25%.

Its flexible interconnect plus SparseCore support means it adapts well to both LLMs and massive recommender systems, something neither GPUs nor older TPUs were originally optimized for.

‚≠ê In short

TPU v4 is a reconfigurable, optical-networked, heterogeneous ML supercomputer with:

flexible mesh/torus/twisted-torus topologies,

scalable to 4096 accelerators,

dedicated hardware for sparse embeddings,

far better energy efficiency,

and strong performance on both LLMs and DLRMs.

It achieves 2‚Äì4√ó real-world gains despite only 2.2√ó peak FLOPs over TPU v3‚Äîshowing that interconnect and memory hierarchy matter as much as raw compute.

If you want, I can also produce:

‚úÖ a diagram-based summary
‚úÖ a comparison table vs H100
‚úÖ a 5-paragraph ‚Äúresearch-style‚Äù summary
‚úÖ a set of Q&A flashcards for studying the paper

* * *

**You:**

What are the primary architectural innovations introduced in TPU v4?

* * *

**ChatGPT:**

Here are the primary architectural innovations of TPU v4, drawn directly from the paper.
(Cited from TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings.)


2304.01433v3 (1)

‚≠ê Primary Architectural Innovations in TPU v4
1. Optical Circuit Switch (OCS)‚ÄìBased Reconfigurable Interconnect

The single most important innovation in TPU v4 is the introduction of optical circuit switches (MEMS-based) that connect all 4096 TPU chips with reconfigurable optical links.

Key capabilities:

Dynamic topology switching (3D torus, twisted torus, rectangular meshes, cigar shapes, etc.)

Scales to 4096 chips, 4√ó TPU v3‚Äôs maximum.

Improves availability by routing around failed CPU hosts or TPU chips.

Higher bisection bandwidth for all-to-all traffic (essential for embedding-heavy workloads).

Simplifies scheduling, since jobs no longer require contiguous chips.

Enables air-gapped multi-tenant isolation for security.

Greatly accelerates LLM training by choosing optimal communication topologies.

All of this comes at <5% total hardware cost and <3% power, despite providing enormous system-level benefits.

2. SparseCore (SC): Dedicated Embedding Accelerator

TPU v4 introduces the third-generation SparseCore, a mini-processor optimized for embedding operations (crucial for recommendation models like DLRM).

Each TPU v4 chip contains 4 SparseCores, each with:

a dataflow architecture optimized for gather/scatter,

dedicated 2.5 MiB sparse vector memory,

HBM-backed wide memory channels,

specialized units for deduplication, weighted sum, and multi-hot embeddings,

fine-grained variable-length execution.

Impact:

Embedding operations are 5√ó‚Äì7√ó faster vs CPU-based embedding lookup.

TPU v4 is 30√ó faster than CPU clusters on production DLRMs.

Embedding-side communication scales with N^(2/3) due to the 3D torus ‚Äî a massive improvement over TPU v3‚Äôs 2D torus.

SparseCore only uses ~5% of die area and power but unlocks massive end-to-end speedups.

3. 3D Torus Interconnect (Upgraded from 2D Torus)

TPU v4 shifts to an electrical 3D torus at the rack level, backed by optical links between racks.

Benefits:

2√ó‚Äì4√ó higher bisection bandwidth vs TPU v3.

Enables much faster:

all-reduce (for dense model gradients),

all-to-all (for embedding communication),

model parallelism traffic.

This is particularly important for LLM training and large-scale DLRMs.

4. Topology Engineering: Twisted Tori

Because OCS enables arbitrary rewiring, TPU v4 also introduces twisted 3D tori (from Camarero et al.).

Result:

1.31√ó‚Äì1.63√ó higher all-to-all throughput on common slice sizes.

Lower worst-case latency.

This is the first production deployment of twisted tori in a commercial supercomputer.

5. Chip-Level Improvements

Beyond the interconnect innovations, TPU v4 includes several silicon-level architectural changes:

Compute

2 TensorCores per chip, each with:

4√ó 128√ó128 MXUs (matrix multiply units),

128-lane vector unit,

improved dataflow and pipelining.

Peak compute jumps to 275 TFLOP/s (bf16/int8), 2.2√ó TPU v3.

Memory

128 MiB CMEM on-chip scratchpad (not present in TPU v3).

1.3 TB/s HBM bandwidth (vs 0.9 TB/s on TPU v3).

Larger Spmem for SparseCore.

Power & efficiency

2.7√ó performance/Watt improvement over TPU v3.

Improvements come ~40% from process shrink and ~60% from design (clock gating, pipeline balancing, etc.).

6. ML-Guided Co-Optimization (PA-NAS + Topology Search)

TPU v4 is paired with Platform-Aware Neural Architecture Search (PA-NAS) that:

co-optimizes model architecture with SparseCore and TensorCore balance,

co-optimizes the interconnect topology with model parallelism configuration.

Examples from the paper:

2.3√ó speedup on a 512-chip LLM by selecting better topology + parallelism configuration.

1.2√ó speedup over expert-designed GPT-3 parallelism configuration.

This automated co-design is now a core architectural feature for TPU deployments.

üìå In short
The key innovations of TPU v4 are:

Reconfigurable optical interconnect via OCS

SparseCore embedding accelerator

3D torus topology with twisted torus support

Massive scaling to 4096 chips

Substantial chip-level upgrades (compute, memory, bandwidth, power)

ML-assisted co-optimization of models & topology

Among these, the OCS interconnect and SparseCore are the two signature breakthroughs‚Äîboth firsts in commercial ML supercomputers.

If you‚Äôd like, I can also provide:

‚úÖ A diagram-based explanation of each innovation
‚úÖ A comparison between TPU v4 and NVIDIA A100/H100
‚úÖ A short answer version suitable for an exam
‚úÖ A deep dive into OCS architecture, SparseCore microarchitecture, or topology engineering

* * *

**You:**

Why can‚Äôt embeddings run efficiently on TensorCores or CPUs?

* * *

**ChatGPT:**

Embeddings do not run efficiently on either TensorCores or CPUs because their computational and memory-access patterns are fundamentally different from the dense linear algebra workloads those processors are built for. TPU v4‚Äôs paper explains this explicitly in Section 3.5 (SparseCore motivation).


2304.01433v3 (1)

Below is a clear breakdown of why both TensorCores and CPUs are a bad fit.

üß† Why Embeddings Run Poorly on TensorCores

TensorCores (on TPU or GPU) are designed for:

dense matrix multiplies (GEMM)

high FLOP utilization

wide SIMD execution

regular, batched memory access patterns

Embedding lookups violate all of these assumptions.

1. Embedding lookups are memory-bound, not compute-bound

Operations are small scatter/gather reads into large tables.

Very few arithmetic operations (barely any FLOPs).

FLOPs per byte (arithmetic intensity) is extremely low.

TensorCores require large matrix multiplies to reach high utilization; embeddings cannot provide that.

2. Access patterns are sparse, irregular, and variable-length

TensorCores want:

predictable strides

contiguous memory blocks

static shapes

Embeddings provide:

random access to thousands of small vectors

different numbers of lookups per sample (multivalent)

highly nonuniform access frequency (skewed distributions)

This destroys caching and SIMD efficiency.

3. TensorCores don‚Äôt have the right type of memory

Large embedding tables (100 GB ‚Äì multiple TB) cannot fit on-chip.
HBM provides bandwidth, but embedding workloads need many small accesses across the whole table ‚Üí high latency dominates.

Tensor cores aren‚Äôt designed to hide this fine-grained latency.

4. No good mechanism for efficient deduplication or multi-hot ops

Production recommenders deduplicate IDs and combine variable-length vectors.

These operations require:

dynamic instruction behavior

cross-channel reductions

small conditional ops

‚Üí Not what MXUs / dense tensor pipelines are for.

This is why embeddings placed on TensorCores lead to 5√ó‚Äì7√ó slowdowns in TPU v4‚Äôs evaluation.


2304.01433v3 (1)

üñ•Ô∏è Why Embeddings Also Run Poorly on CPUs

CPUs are also a poor match for embedding workloads.

1. DRAM bandwidth is far too small

Embedding workloads require:

massive read bandwidth (global table sharding)

dozens of random accesses per example

CPU DRAM bandwidth is nowhere close to HBM bandwidth on TPUs.

From the paper:

‚ÄúTPU v4 beats TPU v3 by 3.1√ó‚Ä¶ but when embeddings are placed in CPU memory for TPU v4, performance drops by 5√ó‚Äì7√ó due to CPU memory bandwidth bottlenecks.‚Äù


2304.01433v3 (1)

2. CPUs have few cores and limited SIMD capability

Embedding workloads require:

tens of thousands of concurrent outstanding memory requests

high-throughput scatter/gather

vector add operations over many cores

CPUs have:

24‚Äì64 cores (not thousands)

limited SIMD width

small private caches that don‚Äôt help with random access

3. CPUs cannot match the interconnect bandwidth

In distributed embedding training:

tables are sharded across devices

communication is variable-length all-to-all

CPU networking (even 100‚Äì400 GbE) is:

orders of magnitude lower bandwidth than TPU ICI

much higher latency

dependent on kernel/network stack overheads

This slows training dramatically.

4. CPU tail latency kills throughput

Because embedding lookups are random and heterogeneous, latency spikes (cache misses, page faults) cause significant pipeline stalls.

TPUs hide this by:

specialized dataflow engines (SparseCore)

many outstanding memory ops

HBM-level parallelism

CPUs do not.

üü¶ Big Picture: Why Specialized Hardware (SparseCore) Wins

All of the above leads to a fundamental mismatch:

Feature Needed for Embeddings	TensorCore	CPU	SparseCore
Fast gather/scatter	‚ùå	‚ùå	‚úÖ
High parallel memory accesses	‚ùå	‚ùå	‚úÖ (16 HBM channels + cross-channel units)
All-to-all communication	üüß limited	‚ùå	‚úÖ optimized for it
Deduplication / multi-hot fusion	‚ùå	‚ùå	‚úÖ native instructions
TB-scale table access	‚ùå	üüß possible but slow	‚úÖ globally addressable 128 TiB space
Variable-length ops	‚ùå	üüß slow	‚úÖ data-dependent CISC-like ops

This is why TPU v4 added SparseCore:

‚ÄúPlacing embeddings on the TensorCore would be suboptimal‚Ä¶ placing them on host CPUs would induce an Amdahl‚Äôs Law bottleneck‚Ä¶ This insight led to the co-design of SparseCore.‚Äù


2304.01433v3 (1)

üî• TL;DR

Embeddings run poorly on TensorCores because they are memory-bound, sparse, irregular, and low-FLOP, completely opposite of what MXUs are optimized for.

Embeddings run poorly on CPUs because CPUs have insufficient memory bandwidth, limited parallelism, and slow interconnects.

SparseCore exists precisely because embeddings require a different microarchitecture:
high-bandwidth HBM, fine-grained loads, dataflow processing, variable-length execution, and optimized all-to-all communication.

If you want, I can also produce:

‚úÖ a diagram comparing TensorCore vs SparseCore microarchitecture
‚úÖ a one-paragraph answer for notes/exams
‚úÖ a detailed explanation of how SparseCore‚Äôs fetch/scatter/flush pipeline works

* * *

**You:**

Why do A100 and IPU architectures choose such different memory hierarchies?

* * *

**ChatGPT:**

A100 and Graphcore IPU choose radically different memory hierarchies because they target fundamentally different philosophies of parallelism, workload balance, and scaling. TPU v4‚Äôs paper explicitly contrasts these choices when discussing MLPerf competitors.


2304.01433v3 (1)

Below is a clean, architectural explanation of why these differences exist.

‚≠ê High-Level Summary
A100 = ‚ÄúFew very large compute units + huge shared memory + multithreading to hide latency.‚Äù
IPU = ‚ÄúThousands of small independent cores + massive distributed local SRAM + no DRAM.‚Äù

These architectural choices reflect different answers to the same ML problem:

How do we feed data to compute at scale without stalling?

üî∂ 1. The A100 Memory Hierarchy (HBM + Caches + Huge Register File)

Design philosophy:

Maximize throughput of dense tensor math by hiding memory latency via multithreading and large on-chip SRAM/cache.

Key traits

High-bandwidth HBM2 (80 GB) is the main store for model parameters.

~40 MB on-chip SRAM (L2 cache).

A massive 27 MB register file to support:

hundreds of threads per SM (32 threads √ó 108 SMs = 3456 threads)

fast context switching

latency hiding

The A100 assumes:

Models are dense.

Compute is matrix-heavy (GEMMs, convolutions).

Threads can stall frequently ‚Üí hide via warp switching.

A large shared memory + cache hierarchy amortizes DRAM latency.

Why this structure?

Dense workloads:

benefit from data reuse,

have regular memory access patterns,

perform many FLOPs per byte,

tolerate high-latency DRAM as long as bandwidth is high.

HBM + caches + multithreading = perfect for dense LLM + CNN workloads.

üî∂ 2. The IPU Memory Hierarchy (Fully Distributed SRAM, No DRAM)

Design philosophy:

Eliminate memory latency instead of hiding it.

Key traits

~900 MB of on-chip SRAM, fully partitioned across ~1500 tiny cores.

Each core has:

small local SRAM (~600 KB per tile),

its own compute pipeline,

very fast access (single-cycle),

message-passing fabric to communicate with neighbors.

Why no DRAM?

Because IPU‚Äôs workloads assume:

irregular, fine-grained, sparse, or graph-like operations,

where HBM latency kills performance,

and where matrix multipliers cannot be fully utilized.

Graphcore‚Äôs view:

A model is a program distributed across thousands of tiny workers, not a giant matrix multiply machine.

The SRAM-based design provides:

deterministic latency,

extreme fine-grained parallelism,

efficient execution of sparse/irregular workloads.

The tradeoff

You can‚Äôt store gigantic models because SRAM is expensive ‚Üí the IPU suffers at very large model scales (reflected in MLPerf, where IPU hits 256 chips vs TPU v4/A100 at 4096).


2304.01433v3 (1)

üî∂ 3. Why Their Memory Hierarchies Diverged

This difference stems from three fundamental architectural bets:

Bet #1 ‚Äî What workload is dominant?
A100 bets on dense linear algebra

cuBLAS/FlashAttention/GEMM dominate.

FLOPs-per-byte is high.

Reuse of feature maps/activations is high.

Large DRAM is essential for giant models (LLMs up to trillions of params with tensor parallelism).

So A100 invests in:

HBM capacity + bandwidth,

large caches,

huge register file,

multithreading.

IPU bets on fine-grained graph computation

Workloads like:

Mixture-of-experts (early work),

GNNs,

RL,

sparsity-heavy models,

irregular control flow.

These require:

extremely low-latency local access,

predictable execution,

distributed control.

Thus IPU invests in:

local SRAM per tile,

many tiny cores,

message passing.

Bet #2 ‚Äî How should parallelism be expressed?
A100: SIMD / SIMT parallelism

One large instruction stream across many threads.

Hide latency with warp scheduling.

Keep MXUs fed by preloading from HBM into SM caches.

IPU: MIMD parallelism

Each tile runs its own program:

No global synchronization per instruction.

Natural for irregular control flow.

This forces the memory hierarchy to be:

completely local,

very distributed,

without DRAM,

because DRAM would force a shared bottleneck.

Bet #3 ‚Äî How to scale a system to many chips?
A100 uses NVLink + NVSwitch + HBM

Large memory pool per GPU.

Communication via packet-switched NVSwitch.

Optimal for tensor-parallel + pipeline-parallel LLMs.

But:

Latency is higher,

Scalability is limited (256 GPUs typical for DGX Superpods).

IPU uses message-passing + distributed SRAM

Everyone talks via fast fabric.

Great for irregular parallelism.

But scaling to thousands of chips becomes difficult because the model must be manually partitioned across hundreds of tiny memories.

This is exactly why TPU v4‚Äôs OCS-based torus outscales IPU by ~16√ó in MLPerf submissions.


2304.01433v3 (1)

üî∑ 4. TPU v4 Paper Explicitly Highlights These Differences

From Table 5 in the paper:

A100: 40 MB on-chip memory, 27 MB register file, 80 GB HBM.

IPU: 900 MB on-chip memory, 0 GB HBM, 1472 small cores, 6 threads/core.


2304.01433v3 (1)

The paper notes these are ‚Äústrikingly different design choices.‚Äù

It also observes:

TPU v4 and A100 scale much further (4096 & 4216 chips).

IPU tops out at 256 chips and suffers performance deficits on dense models (4.3√ó‚Äì4.5√ó slower).


2304.01433v3 (1)

‚≠ê Final Answer (Short Version)

A100 uses a large HBM + cache hierarchy because it is optimized for dense tensor math, where:

data reuse is high,

FLOPs dominate,

multithreading + caches can hide latency,

large model capacity is required.

IPU uses fully distributed SRAM with no DRAM because it is optimized for fine-grained, sparse, irregular workloads, where:

memory latency must be minimized not hidden,

tiny parallel tasks dominate,

distributed control is easier than SIMT,

deterministic local memory access beats high-bandwidth HBM.

These architectures reflect completely different philosophies for scaling ML systems.

If you want, I can also give you:

üìå A diagram comparing A100 vs IPU memory paths
üìå A GPU-like vs IPU-like pseudocode example to illustrate differences
üìå How TPU‚Äôs design sits between these two extremes (Hybrid philosophy)

* * *

**You:**

What workloads still do not scale well on TPU v4? What limitations does the paper not explicitly mention?

* * *

**ChatGPT:**

Below is a precise, paper-grounded answer (with citations) plus a deeper architectural analysis of limitations the paper does not explicitly discuss but are implied by TPU v4‚Äôs design choices.

‚úÖ 1. Workloads That Do Not Scale Well on TPU v4

The paper is candid about several workload classes that still fail to scale efficiently, even on the reconfigurable OCS-based TPU v4 supercomputer.

A. Embedding-Heavy DLRMs (Beyond ~1K chips)

The paper explicitly says that DLRM workloads stop scaling well past ~1024 chips:

‚ÄúDLRM0/1 scale to 1K chips.‚Äù


2304.01433v3 (1)

Why?

Embedding all-to-all dominates at large scale.

SparseCore overheads start to dominate at high chip count:

‚ÄúAt 1024 chips, SC overheads start to dominate, so bisection bandwidth is less important.‚Äù


2304.01433v3 (1)

Meaning: TPU v4‚Äôs SparseCore architecture has diminishing returns at very large cluster sizes.

B. BERT and Transformer Models (~2K chips limit)

The scaling graph shows:

BERT0 scales only to ~2K chips, not 3K.

‚ÄúBERT0 scales to 2K‚Äù


2304.01433v3 (1)

Why?

Communication overheads (all-reduce) start to dominate.

The model architecture itself has synchronization barriers (attention, residuals).

This limitation exists despite TPU v4‚Äôs strong topology flexibility, meaning it‚Äôs a fundamental ML parallelism limit here.

C. RNN-like Models (Small weights, low batch)

RNN1 sees great chip-level speedup but poor large-scale scaling:

RNN0 and RNN1 scale well to 3K chips (surprisingly good), but these workloads are becoming irrelevant.

The paper admits RNN usage dropped from 21% ‚Üí 2% of TPU training workload.


2304.01433v3 (1)

Interpretation: Scaling well on dying workloads is not helpful.

But the real hidden limitation:
‚û°Ô∏è Small-batch, stateful sequence models cannot fully utilize huge parallel slices because they lack parallelism.
Even if comms scale, the model does not.

D. Highly Irregular Sparse Workloads (Non-DLRM sparsity)

SparseCore accelerates embedding sparsity, not:

block-sparse attention

structured sparsity

irregular graph workloads

unstructured model pruning

These patterns do not map well to the SC (which is specialized for multi-hot embedding ops), nor to TensorCores.

The paper does not directly say this, but it is implied by SC‚Äôs hard-wired units (Fetch Units, scVPU, dedup, etc.).

‚úÖ 2. Workloads That Scale Mechanically but Hit Architectural Ceilings

Even workloads that appear to scale linearly on the curve have practical ceilings:

A. LLMs at extremely large scale (beyond 4k chips)

TPU v4 is capped at 4096 chips.

OCS architecture could scale higher, but:

host CPU availability,

optical port count,

routing complexity,

training-state checkpointing overheads,

all become exponentially more challenging.

The paper acknowledges that this scale (4k) was chosen partly due to reliability constraints.


2304.01433v3 (1)

‚úÖ 3. Limitations the Paper Does Not Explicitly Mention

These are architectural blind spots or challenges implied by the design but not spelled out.

üü¶ A. HBM Capacity Limits for LLMs

The paper briefly mentions that TPU v4 might be limited by HBM capacity for large LLMs:

‚ÄúHBM capacity could be a limiting factor in some cases.‚Äù


2304.01433v3 (1)

But they do not emphasize that:

A100 has 2.5√ó more HBM per chip (80GB vs 32GB).

This matters for trillion-parameter models where tensor parallelism becomes expensive.

This is a meaningful limitation for cutting-edge LLMs like GPT-4, Gemini Ultra, DeepSeek-V3, etc.

üü¶ B. No Hardware Support for Attention Sparsity / Mixture-of-Experts Routing

SparseCore accelerates embeddings, but:

MoE routing,

sparse attention,

block-sparse multiplications,

structured sparsity pruning

are not accelerated in hardware.

Modern LLM efficiency improvements (DeepSeek V2/V3, SNF, GQA, Mixture-of-Depth, spec-execute KV caches) increasingly revolve around sparsity patterns that TPU v4 does not explicitly accelerate.

üü¶ C. No Asynchronous Execution Model

TPU v4 execution is dominated by:

XLA static graph compilation

heavy global synchronization barriers

lack of fine-grained async hardware scheduling

This is fine for dense workloads but limits:

extremely dynamic models

reinforcement learning

agent-based workloads

non-uniform sequence lengths

ragged batching

The IPU and Cerebras systems are explicitly designed for these use cases.

Not discussed in the paper.

üü¶ D. Topology Is Static for the Duration of a Job

OCS is only reconfigured once per job.

‚ÄúTPU v4 uses a single static topology for each training job.‚Äù


2304.01433v3 (1)

Not dynamic at runtime.

This means:

Communication patterns that change across layers (e.g., mixture-of-parallelism LLMs) cannot fully exploit OCS flexibility.

The system cannot adapt topology during training to bottlenecks discovered via profiling.

This is a hidden limitation relative to the marketing hype around ‚Äúoptically reconfigurable fabric.‚Äù

üü¶ E. Weak Support for Non-NN Workloads

TPU v4 is nearly useless for:

graph algorithms

irregular HPC sims

reinforcement learning environment dynamics

symbolic workloads

multi-agent systems

search/optimization workloads

This is a consequence of:

XLA static graph assumption

lack of MIMD execution

single-threaded TensorCore pipelines

lack of a real cache hierarchy

no speculative execution or branch prediction

Paper does not highlight this.

üü¶ F. No Hardware Compression or KV Cache Optimizations for LLM Inference

TPU v4 predates modern inference optimizations like:

paged attention

hierarchical KV caching

flash-infer kernels

KV quantization and compression

RoPE cache interpolation

GQA/SQA

multi-query attention

chunked-prefill execution

sliding-window attention hardware

long-context optimizations (FIRE, H3, Medusa, Athena)

The paper focuses mostly on training, not inference.

So TPU v4 lags modern GPU pipelines in inference feature support.

üü¶ G. Scaling Limited by Host CPU Availability

The paper discusses goodput issues (99.0‚Äì99.5% host reliability) but does not say:

The host CPU ‚Üí TPU ratio (1:4) is a bottleneck.

CPU-based orchestration and input pipelines limit scaling at >4k chips.

The optical fabric does not reduce the need for CPU synchronization.

üü¶ H. Static SPMD Programming Model

TPUs fundamentally rely on large SPMD single executable programs.

This limits:

model-parallel frameworks with fine-grained heterogeneity

mixture-of-parallelism architectures (Megatron tuning complexity)

dynamic expert selection (DeepSeek-V2 style)

dynamic control flow

dynamic batch routing

adaptive computation

The paper never mentions this limitation.

‚≠ê Final Consolidated Answer
Workloads that do not scale well on TPU v4

DLRMs with extremely large embedding tables (>1k chips)

Transformers/BERT beyond ~2k chips

Models with limited inherent parallelism (e.g., RNNs)

Irregular sparse workloads (graph models, GNNs, pruning)

Dynamic-control-flow workloads (RL, agents)

Massive LLMs where HBM-exhaustion drives excessive tensor-parallelism

Limitations the paper does not explicitly mention

HBM capacity limits for very large LLMs

No hardware acceleration for MoE routing or sparse attention

Static topology during job runs (no runtime reconfiguration)

No async execution; limited support for dynamic/irregular workloads

Inference-time disadvantages vs GPUs (KV caching, compression)

CPU host bottlenecks and orchestration scaling limits

SPMD programming model limits adaptive compute

Not suitable for HPC, symbolic, graph, or multi-agent workloads

If you want, I can also provide:

üìå A diagram of where scaling breaks down on TPU v4
üìå A comparison of scaling limits across TPU v4 vs A100/H100 vs IPU
üìå A list of modifications TPU v5/v6 would need to address these gaps