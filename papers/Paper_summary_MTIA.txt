**You:**

3579371.3589348.pdf
PDF
Can you summarize this paper?

* * *

**ChatGPT:**

Hereâ€™s a concise but complete summary of the MTIA paper based entirely on the uploaded document 

3579371.3589348

.

MTIA: First Generation Silicon Targeting Metaâ€™s Recommendation Systems â€” Summary

The paper introduces MTIA, Metaâ€™s first in-house inference accelerator chip, designed specifically to run Deep Learning Recommendation Models (DLRM) efficiently in production datacenters. The work covers the architecture, software stack, kernel mapping strategies, and initial production benchmarks.

Why Meta Built MTIA

Meta historically relied on CPUs for inference, then Intel NNPI, and then increasingly GPUs (Fig. 2, p. 2). But:

DLRM models are growing quickly in both compute and memory footprint (Fig. 1, p. 2).

GPUs offer high throughput, but poor perf/W for inference, especially small-batch and memory-heavy operations.

A custom chip could match the exact needs of DLRM: huge embedding tables + many FC layers + small batches + strict latency.

High-Level Architecture
Tile-Based 2D Grid of Processing Elements (PEs)

(Fig. 3, p. 3)

64 PEs, each with:

2 RISC-V cores (one scalar, one with vector extension)

128 KB local SRAM

Fixed-function units:

MLU â€“ memory layout ops (transpose, concat, reshape)

DPE â€“ high-throughput dot-product engine

1024 INT8 MAC/cycle (32Ã—32)

512 FP16/BF16 MAC/cycle

RE â€“ reduction/accumulation engine + reduction network

SE â€“ SIMD nonlinear ops (tanh, sigmoid, quant/dequant)

FI â€“ fabric interface for DMA & NoC access

Command Processor to schedule all operations

Memory system

(p. 5â€“6)

128 KB local memory per PE

128 MB on-chip SRAM around the array

LPDDR5 off-chip memory: 64â€“128 GB total, 176 GB/s bandwidth

Custom NoC with:

Multicast (row/column)

Dedicated reduction network (unidirectional)

Circular Buffers

A core abstraction (Fig. 5, p. 5):

Provide FIFO-like views on local PE memory

Hardware handles producer/consumer correctness
(DMA fills buffers; compute ops consume them)

Eliminates explicit synchronization

Programming Model & Software Stack

(Fig. 9, p. 8)

The stack is fully integrated with PyTorch:

PyTorch Runtime

MTIA tensors

Device allocator

CUDA-like streams

FX Graph Compiler

Operator fusion

Multi-card partitioning

Tensor placement (trying to keep data on SRAM)

DSL Compiler (KNYFE)

Generates optimized C++ kernels for fixed-function units

LLVM Backend

Codegen for RISC-V cores

Firmware & Driver Layer

PCIe interface

Zephyr RTOS control firmware

PE monitors for scheduling

Example: Mapping an FC Layer

The paper deeply details mapping FC layers because they dominate DLRM time.

Key ideas (Fig. 7â€“8, p. 7):

Work is tiled into 32Ã—32Ã—32 blocks for the DPE.

Two cores inside each PE work in producerâ€“consumer fashion:

Core 0 loads data via DMA into circular buffers.

Core 1 continuously issues MML (matrix-multiply) commands.

Reduction dimension (K) is spread across PEs, accumulated via reduction network.

Multicast reduces memory bandwidth by sharing A and B tiles across rows/columns.

This makes FC layers extremely efficient, particularly for small batch sizes, where GPUs struggle.

Performance Results

(Pages 9â€“11)

Perf/W vs NNPI and A100 GPUs

MTIA vs NNPI: 1.6Ã— better perf/W

MTIA vs A100 GPU: ~0.9Ã— perf/W on average

Better at low-batch, FC-heavy models (up to 3Ã— GPU perf/W on LC2)

Slightly worse on embedding-heavy or very large models (software still maturing)

Operator Benchmarks

INT8 FC: ~2Ã— GPU perf/W (Fig. 10)

FP16 FC: similar trend (Fig. 11)

TBE (Embedding bags): 0.6â€“1.5Ã— GPU perf/W
â€“ but MTIA currently achieves only 10â€“20% of available DRAM bandwidth (software bottleneck)

Other ops (Concat, Transpose, Tanh): excellent when data fits in SRAM; performance drops when DRAM-bound.

Key Learnings / Challenges

Section 7 (pages 11â€“12) outlines many lessons for next-gen MTIA:

What worked well

Dual RISC-V cores per PE for decoupling compute & data movement

Circular buffers for implicit synchronization

Vector unit for ops not originally planned (LayerNorm, ReduceAdd)

What caused difficulty

High memory latency from PE-local SRAM arbitration

Non-uniform SRAM latency (perimeter placement)

Lack of cache coherence inside a PE â†’ software flushes required

Mapping small operators to a huge PE grid (needs sub-grid partitioning)

Automated kernel codegen is hard because every fixed-function unit requires explicit commands

Overall Conclusion

MTIA is a carefully architected DLRM-specific inference accelerator that:

Uses a spatial, PE-grid architecture with fixed-function units

Integrates deeply into PyTorch

Matches or beats GPU perf/W on several key workloads

Has a still-maturing software stack that leaves performance headroom
(especially for embedding ops and large models)

It is a strong first-generation chip with a clear roadmap for future improvement.

If you want, I can also generate:

âœ… A 1-page â€œcheat sheetâ€ of the architecture
âœ… A diagram explaining the dataflow
âœ… A deep comparison vs TPU/A100/Inferentia
âœ… A list of interview-style questions about the paper

Just tell me what you need!

* * *

**You:**

Why are recommendation models (DLRMs) so different from typical vision or language models in their hardware demands?

* * *

**ChatGPT:**

Hereâ€™s a crisp, architecture-focused explanation of why DLRMs (recommendation models) place fundamentally different demands on hardware than vision or language modelsâ€”directly reflecting the reasoning and data in the MTIA paper 

3579371.3589348

.

Why DLRMs Stress Hardware Differently Than CV/NLP Models

DLRMs combine two radically different computational regimes:

Sparse, memory-bound embedding lookups

Small, dense MLPs (FC layers) with strict latency constraints

This mix produces hardware needs that GPUs, TPUs, and AI accelerators designed for dense tensor algebra are not optimized for.

Below is a structured breakdown.

1. Massive, Irregular Embedding Tables â†’ Extreme Memory Pressure
DLRMs use enormous embedding tables:

tens to hundreds of tables

each with millions to billions of rows

32â€“512-dimensional vectors

often > TB-scale logical size (after sharding)

The MTIA paperâ€™s Fig. 1 (p. 2) shows that embedding memory footprint grows faster than compute requirements. GPUs excel at compute, but DLRMs are bottlenecked by:

A. Random access patterns

EmbeddingBag ops index rows with totally unpredictable, non-coalesced access patterns.

Compared to CV/NLP:

CNNs: sequential, stride-friendly memory access

Transformers: read contiguous K/V cache blocks

DLRMs: completely random 32â€“128 byte chunks

This devastates:

cache locality

DRAM row-buffer locality

SIMD/SIMT efficiency

prefetcher accuracy

B. Very low arithmetic intensity

The FLOPs per byte are tiny:

Just a few adds/mults per 64â€“128 bytes loaded

Memory-bound, not compute-bound

GPUs assume compute >> memory.
DLRMs invert that assumption.

2. Many Small MLPs (FC Layers), Not Huge Matrix Multiplies

Vision/NLP accelerators expect:

giant GEMMs (e.g., 4096Ã—4096)

batched matmuls

high batch sizes

DLRMs instead use:

hundreds of tiny FC layers (Fig. 9, p. 9): e.g., 64Ã—128, 128Ã—64, 256Ã—128

batch sizes often 1â€“64 for real-time inference

shallow networks, but many of them

This leads to:

A. Poor GPU utilization

GPUs thrive on:

large tiles

long-running kernels

high occupancy

DLRMs produce:

small matrix multiplies

short kernels

massive launch overhead relative to compute

underutilization of tensor cores

B. Strict latency constraints

Recommender systems serve user-facing requests (rank candidates, retrieve feed items).

Tail latency requirements often < 10â€“20 ms

Batch sizes cannot be increased to improve GPU efficiency

This forces hardware to handle:

small tensors

low batch

rapid operator switching

fine-grained scheduling

Which GPUs were not designed for.

3. Extremely High Model Diversity

DLRMs vary wildly per team or product inside companies like Meta:

number of tables

embedding dimensions

MLP widths

pooling factors

quantization schemes

feature configs

Unlike vision or language models, there is no single canonical architecture like ResNet or Transformer.

HW and compiler stacks must:

handle hundreds of custom ops

pipeline many heterogeneous kernels

optimize arbitrary gather/scatter patterns

This heterogeneity complicates:

kernel fusion

graph compilation

optimal tiling

caching strategies

scheduling

The MTIA paper explicitly notes this as a driver for the PE-grid flexibility and KNYFE DSL.

4. Workload Composition: DLRMs Mix Sparse + Dense Ops

Vision models: 99% dense compute
Language models: 90% dense compute (attention + MLP)
DLRMs: often >70% sparse memory ops, <30% dense compute.

From Table III (p. 9) in the MTIA paper:

Operator	% of runtime (batch 64)
FC	42%
EmbeddingBag	31%
Concat + Transpose + Quant/Dequant	~16%
Others	7%

Key insight:
GPU performance is optimized around GEMMs and convolutions.
DLRMs spend most of their time on everything else.

5. Scaling Behavior Diverges from CNNs/Transformers
CNN/Transformer scaling:

More layers â†’ more parallel compute â†’ GPUs scale beautifully.

DLRM scaling:

More features â†’ more tables â†’ memory scales, not compute.

Memory grows faster than compute in Metaâ€™s models (Fig. 1):

embedding tables balloon in size

compute grows slower

distribution of lookups becomes more irregular

Thus the bottleneck shifts to:

DRAM bandwidth

local SRAM capacity

NoC bandwidth

reduction networks

This is why MTIA dedicates:

enormous on-chip SRAM (128 MB)

multicast NoC

PE-local circular buffers

specialized reduction trees

which GPU architectures lack.

6. Workload Parallelism Mismatch

CV/NLP: high data-level parallelism
DLRMs: mostly model parallelism (table sharding)

Challenges:

data-parallel DLRMs require replicating gigantic embeddings
â†’ impossible for single GPU card

table sharding â†’ distributed gather-scatter

requires fanout/fanin communication, not typical AllReduce patterns

This stresses:

PCIe bandwidth

network bandwidth

collective comms libraries in very different ways

MTIA addresses this with:

lightweight PCIe interfaces

multicast NoC

fine-grained DMA engines

In One Sentence

DLRMs are dominated by massive, random-access, memory-bound embedding lookups and hundreds of tiny FC layers executed under strict latency constraintsâ€”exactly the opposite of the large, dense, compute-bound workloads that GPUs and AI accelerators are designed for.

If you want, I can also produce:

ðŸ“Œ A table comparing DLRM vs CNN vs Transformer workloads
ðŸ“Œ A hardware-software co-design diagram explaining why MTIA looks the way it does
ðŸ“Œ How next-gen accelerators (TPU v5e, MTIA v2, AWS Trainium/Inferentia) adapt to DLRMs

Just tell me!

* * *

**You:**

How does MTIA compare philosophically to GPUs and TPUs?

* * *

**ChatGPT:**

Hereâ€™s a clear, high-level comparison of the philosophies behind MTIA, GPUs, and TPUs â€” not just the raw specs â€” grounded in how each architecture fundamentally views ML workloads and how that shaped their design.

This reflects the MTIA paperâ€™s reasoning and architectural choices 

3579371.3589348

.

â­ MTIA vs GPU vs TPU â€” Philosophical Design Comparison

Below is the simplest way to think about the three:

Chip	Core Philosophy	What it Treats as â€œNormal Workloadâ€	Primary Optimization
GPU	Universal dense math engine	Big batches, huge GEMMs, high throughput	Max FLOPs scaling & utilization
TPU	Structured systolic compute for large batched tensors	Uniform blocks of matmul/conv	Matmul efficiency at cluster scale
MTIA	Fine-grained, memory-centric dataflow engine	Small batches, sparse ops, irregular access	Perf/W under DRAM pressure & latency

Letâ€™s expand each philosophy.

ðŸŽ® 1. GPUs: â€œEverything is a large dense matrix multiplyâ€
Philosophy

GPUs assume:

You have huge parallelism

You have big batches

You have large, dense tensors

Utilization is king

Architecturally:

Thousands of identical SIMT lanes

Massive register files

Large SMs that expect to run big kernels

Tensor cores that thrive on big tiles (e.g., 128Ã—128Ã—128)

Implication

GPUs shine when:

You can build work into large batched GEMMs

Memory access is mostly coalesced

There is tons of thread-level parallelism

GPUs were born from graphics â†’ rasterization â†’ extremely high throughput, structured math.

Why GPUs struggle with DLRMs

Tiny FC layers â†’ low occupancy

Random embedding reads â†’ cache misses

Batches often < 32 â†’ poor tensor core utilization

Huge model memory footprints â†’ HBM not a good fit for random access

GPU philosophy â‰ˆ FLOPs first, bandwidth second, latency last.

âœï¸ 2. TPUs: â€œThe universe is a giant systolic arrayâ€
Philosophy

TPUs assume ML = giant matrix multiplies, so they build:

Massive systolic arrays (128Ã—128 or larger)

Deterministic dataflows

Uniform tiling

Low-overhead dispatch

Assumptions baked into TPU design

You will batch your work

You will stream tensors into systolic arrays

You will tile your model to fit the fabric

You wonâ€™t branch much

You wonâ€™t do sparse random lookups (not efficiently, anyway)

TPU excels at:

Transformers

LLM training

CNNs

Very large batch matmuls

TPUs are about structured, regular computation on uniform blocks.

TPUs struggle with DLRMs

Embedding lookups = irregular memory loads

Pooling/gather = non-systolic

Operator heterogeneity = forces offloading

Batch size 1â€“64 = systolic arrays underutilized

TPU philosophy â‰ˆ â€œThe world is dense, uniform, and regular.â€

âš™ï¸ 3. MTIA: â€œThe real bottleneck is memory & irregularity, not mathâ€

MTIA starts from the exact opposite direction as GPUs/TPUs.

Philosophy

DLRMs are:

Memory-bound, not compute-bound

Sparse and irregular, not dense

Latency-constrained, not throughput-constrained

Filled with tiny kernels, not giant GEMMs

Dominated by embedding tables, not MLPs

So MTIA optimizes:

Locality

Dataflow

Bandwidth

Producerâ€“consumer synchronization

Multicast memory access

Reduction networks

Circular-buffer orchestration

Rather than one giant compute island, it uses:

Many small PEs

Each a miniature â€œsmart DMA + compute tileâ€

With multiple functional units (DPE, SE, RE, MLU)

And a Command Processor orchestrating data movement

MTIA design is built around the workload, not for generality.

This is why the chip:

Has 128 KB local SRAM per PE

Has 128 MB on-chip SRAM (huge by accelerator standards)

Uses multicast NoC (because the same embeddings are needed by many PEs)

Has a reduction network (DLRMs need reduction across dimension K)

Has circular buffers (to synchronize sparse data streams)

Uses RISC-V cores to orchestrate fine-grained dataflow

MTIA philosophy â‰ˆ â€œMove data efficiently, compute as needed.â€

Compute is not the bottleneckâ€”memory is.

ðŸ”¥ The Philosophical Triad (Simplified)
GPU:

â€œGive me giant tensor ops and Iâ€™ll run them incredibly fast.â€

TPU:

â€œGive me uniform matmuls and I will pipeline them perfectly.â€

MTIA:

â€œGive me messy, sparse, irregular feature lookups and Iâ€™ll keep everything fed efficiently while respecting latency.â€

ðŸ§  Different assumptions â†’ different hardware philosophies
Workload	GPU View	TPU View	MTIA View
Core bottleneck	Compute	Matmul/outer-product dataflow	Memory & irregularity
Efficiency requires	High occupancy	Static dataflow	Locality + scheduling
Dataflow style	SIMT	Systolic	PE-grid with autonomous units
Optimal batch	Huge	Very large	Very small (1â€“64)
Best for	LLMs, vision, diffusion, GNN training	Training massive LLMs, CNNs	DLRM inference at scale
ðŸ“ In One Sentence

GPUs maximize FLOPs, TPUs maximize structured matmul throughput, but MTIA maximizes perf/W for irregular, memory-bound, latency-sensitive recommendation workloads by prioritizing data orchestration over raw compute.

If you want, I can also give:

ðŸ”¹ A diagram contrasting the three philosophies
ðŸ”¹ A one-paragraph â€œinterview answerâ€ version
ðŸ”¹ A hardware-level breakdown (NoC vs systolic array vs SM)
ðŸ”¹ A table mapping each hardware feature to its philosophy

Just tell me!

* * *

**You:**

How does the MTIA PyTorch Runtime compare to CUDA?

* * *

**ChatGPT:**

Hereâ€™s a clean, architectural comparison of MTIAâ€™s PyTorch Runtime vs CUDAâ€™s PyTorch Runtime, focusing on their philosophies, execution models, memory systems, and compiler pathways.

This is based directly on the MTIA paperâ€™s software stack description (Fig. 9, p. 8, and surrounding text) 

3579371.3589348

 â€” but expressed in a much clearer, conceptual way.

â­ High-Level Philosophy
CUDA / GPU Runtime

A high-throughput, massively parallel kernel launcher for dense tensor operators.

CUDA assumes:

Operators will be large

GPU kernels will run for a long time

Memory access is regular and coalesced

Hardware is responsible for thread scheduling

MTIA Runtime

A fine-grained data-movement & operator orchestration layer for many small operators under strict latency constraints.

MTIA assumes:

Kernels are small and irregular

Memory locality & placement dominate performance

Software must explicitly orchestrate dataflow between many fixed-function units

Latency > throughput

So CUDA maximizes throughput, while MTIA maximizes perf/W and latency predictability.

ðŸ§± 1. Execution Model: Wide Kernels vs Fine-Grained Tasks
CUDA Runtime

Launches big GPU kernels (GEMMs, convolutions, fused ops).

Scheduling is handled entirely by hardware (SM warp scheduler).

Kernels consist of thousands of threads running SIMT.

Runtime overhead amortized over large workloads.

Best for: Giant batch matmuls, CNNs, Transformers.

MTIA Runtime (PyTorch Integration)

From the paper:

Provides MTIATensor, device memory allocator, CUDA-like streams.

But unlike CUDA, each operator becomes a coordinated sequence of:

DMA transfers

Commands to fixed units (MLU, DPE, RE, SE)

RISC-V code that orchestrates these

Work is broken into fine-grained tasks mapped to many PEs.

MTIA must:

Set up circular buffers

Schedule DMA ops

Schedule DPE dot-product ops

Trigger reductions

Place tensors in SRAM or DRAM preferable zones

Split work across PEs and across multiple MTIA cards

All of which are done by software, not hardware.

Best for: DLRMs where each operator is tiny and memory-bound.

ðŸ§  2. Memory Management & Tensor Placement
CUDA

Unified CUDA malloc-style device memory.

HW+driver handle cache hierarchy: L2 â†’ SM L1 â†’ registers.

GPUs rely on:

memory coalescing

prefetch

massive parallelism to hide latency

Programmers rarely think about:

data locality

on-chip SRAM placement

DMA pipelines

cache eviction

MTIA

The runtime has to carefully place tensors across:

Local PE SRAM (128 KB per PE)

Large on-chip SRAM (128 MB)

Off-chip LPDDR5 (up to 128GB)

And also manage:

Multicast NoC routing

Circular buffer allocation

Producer-consumer synchronization

Reduction network utilization

Memory bandwidth budgeting

The FX Compiler tries to:

â€œkeep producer-consumer data in on-chip memoryâ€
(from the paper)

This is existential for performance because DLRMs are massively memory-bound.

CUDA relies on hardware caches.
MTIA relies on software-planned locality.

ðŸ§® 3. Kernel Generation & Compilation Path
CUDA Kernel Path

PyTorch op â†’ ATen â†’ cudnn/cuBLAS or custom CUDA kernel

CUDA kernel â†’ NVCC â†’ SASS/ptx

Hardware executes SIMT cores + Tensor Cores

Most operations are:

library calls

stable, heavily optimized

fused using CUDA graph capture or TensorRT

MTIA Kernel Path

MTIA uses three compilers:

(1) PyTorch FX Graph Compiler

Performs graph transformations

Decides tensor placement

Splits model into multi-card partitions

Generates LLVM IR + MTIA-specific ops

(2) KNYFE (DSL-based kernel generator)

A high-level DSL describing kernel structure

Output: low-level C++ that encodes:

DPE invocations

SE nonlinear operations

DMA patterns

tiling for PE grid

circular buffer orchestration

reduction ops

Used for most DLRM operators

(3) LLVM â†’ RISC-V Executables

Final compilation into instructions run by the two RISC-V cores inside each PE

CUDA kernels map directly to SMs.
MTIA kernels map to:

64+ PEs

multiple fixed-function units

multiple local memories

multiple schedulers

per-PE command processors

The runtime must build an explicit schedule for hardware.

ðŸ”§ 4. Hardware Interaction / Synchronization Model
CUDA

Threads synchronize via:

warp intrinsics

block-level __syncthreads()

device-wide events & streams

Kernel boundaries define most synchronization.

Hardware warp scheduler handles dependencies.

MTIA

MTIA uses:

Circular Buffers with hardware-managed read/write pointers

DMA stalls handled transparently by hardware

Reduction networks for cross-PE partial accumulations

Command Processor dependency checking

Atomic operations for inter-PE sync

Software writes commands like:

â€œDMA 64Ã—32 tile of A into CB_Aâ€

â€œDPE multiply CB_AÃ—CB_B with tile offsetâ€

â€œReduce accumulator 2 to east neighborâ€

â€œPOP CB when consumedâ€

The hardware:

Automatically stalls operators until data is available

Resumes compute once CB conditions are met

This is much more like a specialized dataflow machine than a GPU.

ðŸš€ 5. Launch Model & Multi-Device Coordination
CUDA

Launch kernel on single GPU.

Multi-GPU: NCCL AllReduce, AllGather, etc.

Focus on data parallelism.

MTIA

The runtime supports:

Model partitioning across multiple MTIA cards

Sub-grid partitioning within a chip (important for small operators)

Fine-grained synchronization across PEs

The compiler may:

split an embedding bag across MTIA cards

replicate an MLP across cards for latency

fuse operator clusters into subgraphs

CUDA thinks in kernels.
MTIA thinks in subgraphs + dataflow regions.

ðŸ 6. Goal: Throughput vs Latency vs Perf/W
CUDA runtime => maximize raw throughput

Full SM occupancy

Tensor core utilization

Long-run kernels

Fused ops

MTIA runtime => maximize perf/W for DLRMs

Many tiny ops

Memory-bound ops

Small batch sizes

Huge tables

Irregular access patterns

Strict latency SLAs (<10â€“20 ms)

Thus MTIAâ€™s runtime:

Carefully overlaps DMA and compute

Minimizes DRAM fetches

Ensures producerâ€“consumer locality

Uses SRAM as aggressively as possible

Builds subgraphs that fit PE grids

CUDA = wide kernels, low launch overhead.
MTIA = careful data orchestration, many mini-kernels.

ðŸ§© In One Sentence

CUDA is a high-throughput GPU kernel launcher, while the MTIA PyTorch Runtime is a dataflow orchestration engine that explicitly coordinates PEs, fixed-function units, DMA pipes, and SRAM placement to meet the unique needs of memory-bound, latency-sensitive DLRM workloads.

If you want, I can also generate:

ðŸ“Œ A side-by-side diagram of the two runtimes
ðŸ“Œ A table mapping each MTIA runtime component to its CUDA equivalent (and noting where CUDA has no equivalent)
ðŸ“Œ A system design-level explanation (how FX compiler + KNYFE + RISC-V scheduling fits together)

Just tell me!

* * *

**You:**

How does MTIA compare to Google TPU v4/v5 in design philosophy?

* * *

**ChatGPT:**

Below is a clean, architectural, â€œdesign philosophyâ€ comparison of MTIA versus Google TPU v4/v5, framed around what each architecture believes about ML workloads and what problems it was built to solve.

This is not about specs â€” itâ€™s about the foundational worldview behind each chip.

Grounded in the MTIA paperâ€™s design rationale and lessons learned 

3579371.3589348

.

â­ TL;DR: The Philosophical Divide
TPU v4/v5

â€œThe world is dominated by enormous, dense matrix multiplies. Scale the systolic array, scale the datacenter, scale the training job.â€

Built for:

massive training clusters

dense matmul workloads

ultra-large batches

uniform SIMT-free execution

MTIA

â€œThe real world (ads, feeds, recommender systems) is dominated by sparse, memory-bound, irregular, tiny-kernel inference workloads. Optimize dataflow and memory, not FLOPs.â€

Built for:

DLRM inference

small batch sizes

irregular memory access

strict latency + perf/W

These chips do not try to solve the same problem.

ðŸš€ 1. What Each Architecture Optimizes For
TPU v4/v5: Peak Dense Compute at Scale

TPUs assume your workload is:

dominated by matmul/conv

large, uniform, and regular

executed in huge batches

distributed across thousands of chips

Therefore:

TPU v4 has a trillion-operations-per-second systolic array.

TPU v5 extends this to even bigger arrays with water cooling, multi-slice, SPMD programming.

Dataflow is rigid and predictable.

Philosophy:

Keep the arrays full. Keep the pipeline running. Scale out.

MTIA: Memory-Oriented, Low-Latency, Sparse Workloads

MTIA assumes your workload is:

dominated by embedding lookups

tiny MLPs (hundreds of 64Ã—128 FCs)

random memory access

low arithmetic intensity

tiny batch sizes (1â€“64)

latency-critical (<10â€“20 ms)

Therefore:

MTIA uses 64 small Processing Elements, not one huge array.

Each PE has:

RISC-V cores

dot-product engines

reduction tree

nonlinear units

circular-buffer-managed SRAM

Dataflow is flexible, not rigid.

Philosophy:

Move data intelligently. Compute only when needed. Eliminate memory stalls.

ðŸ§± 2. Compute Fabric Philosophy
TPU = One Giant Systolic Array

TPUs are built around:

massive 2D systolic arrays (128Ã—128 or larger)

global dataflow scheduling

streaming activations and weights through the array

high utilization through batching

Itâ€™s a single monolithic compute island.

This assumes:

You can tile your matrices uniformly

You have predictable access patterns

You have big batches and dense tensors

MTIA = Many Small Independent PEs

MTIA uses:

a grid of many small PEs, each with:

local memory

fixed-function compute units

RISC-V control

its own pipeline

More like:

a dataflow microprocessor array

with DMA-driven orchestration

and a reduction network for partial sums

Because DLRM ops:

are small

are heterogeneous

have irregular access patterns

cannot fill a large systolic array

rely heavily on memory locality

If a TPUâ€™s systolic array is a massive factory assembly line,
MTIA is a cluster of mini-workshops optimized for agility.

ðŸ§  3. Memory Philosophy
TPU: HBM-first Design

TPUs assume:

Most tensors are large and dense

The acceleratorâ€™s job is streaming them efficiently

Memory system is built around:

HBM (TB/s-scale)

large unified buffers feeding the systolic array

software-managed tiling

distributed sharding across pods

MTIA: SRAM-first, Data-locality-first

MTIA assumes:

Most ops are memory-bound

Embeddings require random access

Locality dictates performance

Therefore:

128 KB local SRAM per PE

128 MB on-chip SRAM total

Aggressive use of:

Multicast NoC

Circular buffers

Producer/consumer hardware synchronization

Reduction trees

If TPU memory philosophy is:

â€œFeed the array at full speed.â€

MTIA memory philosophy is:

â€œAvoid DRAM at all cost. Reuse locality. Keep data near compute. Stream only when required.â€

ðŸ“¦ 4. Workload Assumptions
TPU Assumptions

Models are uniform (Transformers, CNNs)

Ops are dense and regular

Compute is the bottleneck

Batch size can be grown

You will distribute the model across many chips

This is perfect for training LLMs, diffusion models, CNNs.

MTIA Assumptions

Models are wide, sparse, irregular

Embedding tables dominate memory footprint

Compute is not the bottleneck â€” memory is

Batch sizes are tiny (real-time inference)

Operator diversity is high (hundreds of table configs)

Model changes frequently across teams

This is perfect for feed ranking, ads ranking, large-scale production recommendation systems.

ðŸ”§ 5. Programming Model Philosophy
TPU

XLA first

JAX as front-end

SPMD programming

Global view of computation graphs

Compile the entire program into a static dataflow graph

The assumption:

You want global graph-level optimization before execution.

MTIA

PyTorch FX compiler

DSL-based kernel generator (KNYFE)

RISC-V microcode programs for PEs

Circular-bufferâ€“driven scheduling

The runtime:

orchestrates fine-grained dataflow

explicitly schedules DMA + fixed-function units

micro-optimizes locality

The assumption:

You want to handle many tiny ops in a latency-critical online inference setting.

ðŸ”‹ 6. Power & Efficiency Philosophy
TPU

Aim: maximize training throughput per rack watt.

Very high-power chips (300â€“700W depending on cooling)

Built for giant clusters

Optimized for training efficiency, not low-power inference

MTIA

Aim: maximize inference perf/W under strict TCO constraints.

Very low-power cards (~35W)

Designed for 12-per-server deployment

Focused on perf/TCO for billions of daily online requests

ðŸ§© Summary Table
Category	TPU v4/v5	MTIA
Core philosophy	Dense matmul-scale-out	Sparse, memory-bound inference
Primary workload	LLM/CNN training	DLRM inference
Compute fabric	Giant systolic array	Grid of small PEs
Memory assumption	Streaming dense tensors	Random-access embeddings
Performance focus	FLOPs/s & throughput	Latency & perf/W
Programming model	Global XLA SPMD	PyTorch + per-PE dataflow orchestration
Batch size assumption	Very large	Very small (1â€“64)
Deployment context	TPUs pods (1kâ€“4k chips)	Inference-serving racks (12 cards/server)
ðŸ§  In One Sentence

TPU v4/v5 are built to maximize dense matmul throughput at massive scale for training, whereas MTIA is built to maximize memory-locality and low-latency perf/W for sparse, irregular, small-batch DLRM inference. They solve almost opposite problems, so their philosophies and architectures diverge completely.

If you'd like, I can also provide:

ðŸ“Œ A diagram comparing TPU vs MTIA dataflow
ðŸ“Œ A â€œwhat if TPU tried to run DLRMs?â€ analysis
ðŸ“Œ A table comparing MTIA, TPU, and A100/H100 together
ðŸ“Œ A summary suitable for interview answers

Just tell me!