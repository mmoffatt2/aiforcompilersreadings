**You:**

2006.06762v5.pdf
PDF
Can you summarize this paper?

* * *

**ChatGPT:**

Here‚Äôs a clear, high-level summary of the paper ‚ÄúAnsor: Generating High-Performance Tensor Programs for Deep Learning‚Äù:

üìÑ Paper Summary: Ansor: Generating High-Performance Tensor Programs for Deep Learning

This paper introduces Ansor, an automated system for generating high-performance kernel code for deep learning operators across different hardware backends (CPUs, GPUs, ARM devices). The core problem it addresses is that writing fast tensor kernels by hand (like cuDNN or MKL kernels) is extremely time-consuming and does not scale well to new hardware or new operators.

üéØ Main Problem

Existing auto-tuning systems struggle because they either:

Use handwritten templates that limit the search space, or

Prune programs too early based on incomplete cost estimates, missing high-quality solutions.

This makes it hard to discover truly optimal implementations.

üß† Key Idea

Ansor introduces a hierarchical search strategy that separates:

High-level structure (called sketches): loop nesting, tiling, fusion, and cache structure

Low-level details (called annotations): tile sizes, unrolling, vectorization, parallelization

Instead of guessing performance from partial programs, Ansor:

‚úÖ Samples complete programs
‚úÖ Uses evolutionary search (mutations + crossover)
‚úÖ Applies a learned cost model to predict performance quickly
‚úÖ Measures only the most promising candidates on real hardware

This avoids the key weakness of prior work: unreliable estimates of incomplete programs.

‚öôÔ∏è Core Components
1. Program Sampler

Automatically builds a huge search space using derivation rules

Generates sketches and randomly fills in low-level details

Covers combinations that template-based tools cannot reach

2. Performance Tuner

Uses evolutionary algorithms (mutation + crossover) to refine programs

Uses a learned cost model (gradient-boosted trees) to rank candidates quickly

3. Task Scheduler

Allocates tuning time across multiple subgraphs of a neural network

Prioritizes the parts that most affect end-to-end latency using a gradient-based scheduling algorithm

üß™ Experimental Results

Ansor is evaluated on:

Intel CPUs

NVIDIA GPUs

ARM CPUs

Across single operators, fused subgraphs, and full neural networks, it achieves:

Hardware	Speedup vs prior state-of-the-art
Intel CPU	up to 3.8√ó
ARM CPU	up to 2.6√ó
NVIDIA GPU	up to 1.7√ó

It consistently outperforms:

PyTorch / vendor libraries

TVM AutoTVM

Halide auto-scheduler

FlexTensor

and often finds kernels outside their reachable search spaces.

üöÄ Why This Matters

Ansor makes it possible to:

Automatically generate near-hand-tuned GPU/CPU kernels

Support new operators without writing manual templates

Scale performance tuning across entire models, not just single ops

This makes performance portable across hardware without huge engineering effort.

One-Sentence Takeaway

Ansor replaces fragile, template-based kernel tuning with a hierarchical, evolutionary search that automatically discovers highly optimized tensor programs across hardware platforms.

If you want, I can also give you:

A diagram-level explanation of Ansor‚Äôs search space

A comparison against Halide / TVM internals

Or a more systems-level critique of the approach

All details are based on the uploaded paper 

2006.06762v5

* * *

**You:**

Why do we need a hierarchichal search strategy? How does this allow for non-handwritten tempaltes

* * *

**ChatGPT:**

Great question ‚Äî this gets at the core contribution of Ansor.

Why do we need a hierarchical search strategy?

The short answer:

Because the full space of valid tensor programs is combinatorially huge, and a flat search cannot explore it efficiently or safely.

Let‚Äôs unpack that.

The real search space problem

For a single operator (like matmul or conv), you‚Äôre choosing things like:

Loop tiling structure

Tile sizes

Loop order

Parallelization

Unrolling

Vectorization

Cache placement

Compute location (inline vs separate)

Reduction splitting (rfactor)

If you treat all these as flat knobs, the space looks like:

pgsql
tiling choices √ó tile sizes √ó parallel options √ó unroll depths √ó cache layouts √ó schedules ...
‚âà billions to trillions of possibilities


Older systems tried to deal with this in two flawed ways:

Approach	Problem
Templates	Human must predefine structure
Sequential search	Must prune early using unreliable estimates

Both fail to cover the true high-performance space.

What does ‚Äúhierarchical‚Äù change?

Ansor splits the search into two different abstraction levels:

Level 1: High-level structure (Sketches)

These decide the shape of the program:

How many loop levels?

Where is tiling applied?

Is reduction split?

Is a consumer fused?

Are cache stages inserted?

This is discrete structure, not numeric tuning.

At this level, Ansor enumerates possibilities using derivation rules, not templates.

Example rule from the paper:

Rule	Effect
Multi-level tiling	Adds multi-level loop nests
Fusion rule	Fuses producer + consumer
Cache rule	Inserts cache node
rfactor rule	Splits reductions for parallelism

These are generic transformations, not operator-specific code.

Level 2: Low-level parameters (Annotations)

Once a structure is chosen, Ansor fills in:

Tile sizes

Which loops are parallel

Which loops are vectorized

Unroll factors

These are sampled rather than hand-coded.

So instead of:

csharp
Human writes: ‚Äúthis is how conv2d should be tiled on V100‚Äù


Ansor does:

vbnet
Ask: ‚Äúwhat legal structures exist?‚Äù ‚Üí enumerate
Ask: ‚Äúfor this structure, what numeric settings are legal?‚Äù ‚Üí sample

How this removes handwritten templates

Traditional systems like TVM/AutoTVM work like this:

yaml
Human writes template:
  for i in blocks:
    for j in warps:
      ...
Then system tunes:
  block size, warp size, unroll depth


This is structure locked by humans.

Ansor doesn‚Äôt do that.

Instead:

1. It starts from the math of the operator, not a template

Example input:

python
C[i,j] = sum(A[i,k] * B[k,j])


No scheduling code provided.

2. It uses general program synthesis rules

These are reusable across operators:

‚ÄúIf this node has data reuse ‚Üí try tiling‚Äù

‚ÄúIf this node is elementwise ‚Üí inline it‚Äù

‚ÄúIf reduction parallelism exists ‚Üí apply rfactor‚Äù

These rules are:

‚úÖ Not handcrafted per operator
‚úÖ Not handwritten schedule templates
‚úÖ Automatically composable

3. It builds structure automatically

Instead of this:

python
schedule_conv2d(...)


Ansor creates sketches like:

text
for i0
  for j0
    for k0
      for i1
        ...


This structure was derived by rules, not written in a template.

That‚Äôs why it scales to:

New operators

New hardware

New operator combinations (fusion, caching, etc.)

Why this matters

Hierarchical search lets Ansor:

‚úÖ Explore more structures than templates
‚úÖ Avoid pruning errors from incomplete programs
‚úÖ Generalize to new operators without human intervention

Without the hierarchy, Ansor would have to:

Either prune aggressively (like Halide)

Or be constrained by templates (like AutoTVM)

Intuition

You can think of it like:

Method	Mental model
Templates	‚ÄúHumans design the assembly skeleton‚Äù
Hierarchical search	‚ÄúCompiler invents the skeleton, then tunes it‚Äù
One-sentence takeaway

The hierarchical search works because it separates ‚Äúwhat shape of program should exist‚Äù (sketch) from ‚Äúhow it should be parameterized‚Äù (annotations), allowing the system to generate structures that older systems had to hard-code by hand.

If you want, I can walk you through a concrete conv2d example showing how the structure emerges with no template.

* * *

**You:**

yes walk me through an example

* * *

**ChatGPT:**

Awesome ‚Äî let‚Äôs walk through a concrete, end-to-end example of how Ansor builds a fast kernel without handwritten templates.

I‚Äôll use a simple conv2d + ReLU subgraph because it shows everything clearly.

Step 0 ‚Äî What the user gives Ansor

You do not give a schedule template.

You only give the math:

text
Output[n, oc, y, x] =
  sum over (ic, ky, kx):
    Input[n, ic, y+ky, x+kx] * Weight[oc, ic, ky, kx]

Relu[n, oc, y, x] = max(Output[n, oc, y, x], 0)


In TVM-style pseudo-code:

python
O = compute(...)
R = compute(lambda n, oc, y, x: max(O[n, oc, y, x], 0))


That‚Äôs it.

No tiling, no CUDA blocks, no pragmas.

Step 1 ‚Äî Build the naive program

Ansor first expands the obvious loops directly from the math:

text
for n:
  for oc:
    for y:
      for x:
        for ic:
          for ky:
            for kx:
              O[n, oc, y, x] += ...
for n:
  for oc:
    for y:
      for x:
        R[n, oc, y, x] = max(O[n, oc, y, x], 0)


This is painfully slow ‚Äî it‚Äôs the starting point.

Step 2 ‚Äî Sketch generation (structure search)

Now Ansor asks structural questions using derivation rules:

Is this node elementwise?
‚úÖ Yes ‚Üí eligible for inline/fusion

Does conv2d have data reuse?
‚úÖ Yes ‚Üí eligible for tiling

Can the consumer be fused?
‚úÖ Yes ‚Üí fuse ReLU into conv2d

Is caching beneficial?
‚úÖ Yes ‚Üí insert cache stage

So Ansor automatically constructs a sketch structure like:

text
for n0, oc0, y0, x0:      # outer tiles
  cache tile into shared/cache
  for ic1, ky1, kx1:
    for n1, oc1, y1, x1:   # inner tiles
      O += Input * Weight
      inline ReLU here


Key point:

No human specified this structure.
It was assembled by generic rewrite rules.

This is now a sketch, not a full program.

Step 3 ‚Äî Randomly fill in missing details (annotations)

Now Ansor fills in low-level numeric choices:

Choice	Random example
Tile sizes	32√ó32√ó8
Threads	bind inner x to vector lanes
Unroll	unroll kx loop by 4
Parallel	parallelize y blocks

A concrete sampled version might look like:

text
parallel for oc0 in 16:
  parallel for y0 in 8:
    for x0 in 8:
      for ic0 in 16:
        unroll for ky in 3:
          vectorize for kx in 4:
            acc += Input * Weight
            acc = max(acc, 0)


This is now a complete runnable program.

Step 4 ‚Äî Evolutionary tuning

That sampled program is just decent, not optimal.

Now Ansor evolves it:

Tile mutation

Before:

java
oc tile = 16, ic tile = 16


After mutation:

java
oc tile = 32, ic tile = 8

Parallel mutation

Moves parallelism to better loop.

Crossover

Merges loop structure from another good program.

After several generations, it converges to something like:

Better cache locality

Better vector utilization

Better warp/block layout (GPU)

Step 5 ‚Äî Measure on real hardware

Only top candidates get compiled and benchmarked:

css
Program A ‚Üí 0.83 ms
Program B ‚Üí 0.61 ms  ‚úÖ
Program C ‚Üí 0.72 ms


The best one is saved.

Why this is powerful

Traditional template system:

yaml
Human writes:
  conv2d_schedule.cu
System tunes:
  thread count, block count, unroll factor


Ansor:

sql
System invents:
  cache structure
  fusion pattern
  loop tiling hierarchy
System tunes:
  numeric knobs via evolution

Final picture

In one pipeline:

java
Math definition
   ‚Üì
Naive loops
   ‚Üì
Rule-based structure synthesis (SKETCH)
   ‚Üì
Random numeric filling (ANNOTATION)
   ‚Üì
Evolutionary tuning
   ‚Üì
Hardware measurement
   ‚Üì
Final kernel

Key insight

The hierarchy lets Ansor search over program shapes, not just numbers.

Templates lock the shape.
Ansor creates the shape.

If you want, I can next show:

How this works differently for GPU kernels, or

A visual diagram of the search tree it explores

* * *

**You:**

How is this different for GPU kernels

* * *

**ChatGPT:**

Great ‚Äî this is where Ansor becomes very interesting, because GPUs force the hierarchical idea much harder than CPUs.

I‚Äôll explain:

What fundamentally changes on GPU

How the hierarchical search adapts

A step-by-step GPU example

Why templates struggle here

1. What‚Äôs fundamentally different about GPU kernels

On CPUs, the key concerns are:

Cache locality

Vector units (SIMD)

Thread-level parallelism (cores)

On GPUs, the constraints are very different:

GPU concept	Why it matters
Thread blocks	Must choose how loops map to blocks
Warps	Threads execute in lockstep
Shared memory	Explicit scratchpad needed
Registers	Very limited, spills are expensive
Memory coalescing	Access patterns must align

This means the ‚Äústructure‚Äù of loops isn‚Äôt just performance ‚Äî it‚Äôs semantics, because you must decide:

Which loop ‚Üí blockIdx

Which loop ‚Üí threadIdx

Which data loaded into shared memory

Which reduction happens across threads

This is not numeric tuning, it‚Äôs structural.

2. How hierarchical search changes for GPUs

Ansor adds GPU-specific structure rules.

On CPU, a typical tiling structure it searches is:

nginx
SSRSRS


(S = spatial loops, R = reduction loops)

On GPU it expands to:

nginx
SSSRRSRS


Why?

Because it needs extra structure for:

Level	Purpose
Block tiles	Map to CUDA blocks
Warp-level tiles	Reduce bank conflicts
Thread tiles	Map to lane-level parallelism

This is decided at the sketch level, before numeric tuning begins.

3. GPU-specific sketch rules

During sketch construction, Ansor asks extra GPU questions:

Rule: Shared memory caching

Is there reuse across threads?

‚úÖ If yes ‚Üí insert shared memory stage automatically

Pseudocode inserted by the sketch:

text
shared_input = load Input tile to shared
__syncthreads()


No template, no human schedule ‚Äî it‚Äôs a derived structure.

Rule: Cross-thread reduction

Is this reduction large enough to require warp-level parallelism?

‚úÖ If yes ‚Üí insert reduction structure like:

text
warp_shuffle_xor(acc)


Again: structure inserted by rule, not user.

4. Concrete GPU example: matmul

Suppose input math:

text
C[M, N] = sum_k A[M, K] * B[K, N]

Step 1 ‚Äì Naive version
text
for m:
  for n:
    for k:
      C[m,n] += A[m,k] * B[k,n]

Step 2 ‚Äì GPU sketch formation

Ansor constructs this skeleton:

text
for block_m:                        # blockIdx.y
  for block_n:                      # blockIdx.x
    load A_tile into shared
    load B_tile into shared
    __syncthreads()

    for warp_m:                     # virtual warp tiles
      for warp_n:
        for k_inner:
          for thread_m:             # threadIdx.y
            for thread_n:           # threadIdx.x
              C += sharedA * sharedB


Notice:

‚úÖ Block mapping is baked in
‚úÖ Shared memory stage exists
‚úÖ Warp/thread structure exists

This is structure, not tuning.

Step 3 ‚Äì Annotation (numerical filling)

Now it picks numbers:

Parameter	Example
Block tile size	(128, 128)
Warp tile size	(32, 32)
k tile	16
Threads per block	256
Vector width	4

Now it becomes real CUDA-like code.

Step 4 ‚Äì GPU evolutionary tuning

Mutation examples:

Tile mutation:

scss
block tile: 128√ó128 ‚Üí 64√ó128


Shared memory change:

arduino
double-buffer shared memory


Warp mapping mutation:

vbnet
change which loop maps to threadIdx.x


And Ansor can crossover two different GPU kernels ideas.

5. Why templates break down here

Traditional GPU frameworks:

System	Limitation
cuDNN	Hand engineered, fixed patterns
AutoTVM	Needs human-written GPU schedule templates
Halide	Prunes early due to bad partial estimates

They require:

cpp
__global__ void conv2d_template(...) {
   // hard-coded scheduling decisions
}


Ansor discovers the skeleton itself.

6. The real difference in one line

CPU version:

ini
Hierarchical = loops + cache hierarchy


GPU version:

ini
Hierarchical = blocks + warps + threads + shared memory + reductions


Both are generated automatically, not handwritten.

One-sentence intuition

On GPUs, Ansor doesn‚Äôt just tune numbers ‚Äî it synthesizes the CUDA execution structure itself using hardware-aware rewrite rules.

If useful, I can next compare how Ansor relates to Triton or FlashAttention-style kernels ‚Äî those are very close conceptually.

* * *

**You:**

Why do we use an evolutionary search?

* * *

**ChatGPT:**

Great question ‚Äî this is one of the most important design choices in Ansor, and it‚Äôs very deliberate.

I‚Äôll explain it in layers:

1. What problem evolutionary search is solving

Once Ansor builds a complete program (via sketches + annotations), it still faces this problem:

The search space is huge, irregular, and discontinuous.

This isn‚Äôt a smooth optimization problem like gradient descent. It‚Äôs more like:

Changing a tile size from 16 ‚Üí 32 can cause cliff drops in performance

Loop reorderings are nonlinear

Cache behavior is chaotic

Some combinations are invalid

So we cannot use:

‚ùå Gradient-based optimization
‚ùå Simple hill-climbing
‚ùå Exhaustive search

We need something that handles:

‚úÖ Discrete decisions
‚úÖ No gradients
‚úÖ Non-smooth fitness

That‚Äôs why evolutionary search is a good fit.

2. What makes this search space hard

Tensor program tuning is not a normal hyperparameter search.

Examples:

text
(tile_i = 32, tile_j = 16) ‚Üí 0.8 ms
(tile_i = 16, tile_j = 32) ‚Üí 2.5 ms  (3x worse)


Same parameters, different order ‚Üí massive change.

Also:

Some combinations cause register spilling ‚Üí massive slowdown

Some cause shared memory bank conflicts ‚Üí silent slowdown

Some are invalid and won‚Äôt compile

So the surface is:

nginx
Jagged + discontinuous + high-dimensional + sparse


Evolutionary algorithms are designed for exactly this.

3. Why not brute force or random sampling?
Pure random sampling is too weak

Random search:

‚úÖ Good coverage
‚ùå No memory
‚ùå No refinement

You‚Äôd waste huge amounts of work rediscovering bad patterns.

Greedy search fails

Greedy/hill climbing:

‚ùå Gets stuck in local optima
‚ùå Cannot escape valleys
‚ùå Cannot mix good ideas

4. Why evolutionary search is ideal here

Evolution gives Ansor four superpowers:

1. Selection

Keeps only promising candidates.

perl
fast kernels survive  
slow kernels die

2. Mutation

Can make small, safe tweaks:

Examples:

Tile size shift

Parallel granularity shift

Unroll pragmas change

These preserve structure while refining performance.

3. Crossover

This is the killer feature.

It allows:

Combine two good but different kernels into a better hybrid

Example:

Kernel A:

wasm
Great memory layout
Bad unrolling


Kernel B:

wasm
Great unrolling
Bad memory reuse


Crossover gives:

csharp
Kernel C = memory layout from A + unrolling from B ‚úÖ


Traditional schedulers can‚Äôt do this.

4. Cost-model acceleration

Evolution is guided by a learned cost model, so it doesn‚Äôt run blind.

Instead of:

arduino
compile + run every candidate


It does:

css
predict thousands ‚Üí measure only top few


Which makes it tractable.

5. Why sequential/beam search doesn‚Äôt work

Other systems do something like:

sql
Decide tile sizes ‚Üí prune ‚Üí decide ordering ‚Üí prune ‚Üí decide fusion ‚Üí prune


Problem:

They prune based on partial programs

Performance of partial programs ‚â† performance of full programs

Good candidates are killed early

Evolution avoids this because:

‚úÖ It only evaluates complete programs
‚úÖ It never ranks unfinished programs

6. Example intuition

Imagine tuning a CUDA kernel.

Each kernel is a ‚Äúgenome‚Äù:

Genome part	Meaning
Tile sizes	Scheduling choices
Loop order	Memory behavior
Unroll	ILP
Thread mapping	Occupancy & parallelism

Evolution does:

text
Generation 1: random kernels
Generation 2: mutate top 10%
Generation 3: crossover top performers
...


Until convergence.

7. Why Ansor specifically needed evolution

Hierarchical search gives Ansor:

‚úÖ Huge structural diversity
‚úÖ Generality
‚úÖ Automatic sketching

But without evolution, it would still be:

Random

Slow to converge

Unstable

Evolution gives it:

‚úÖ Fast local improvement
‚úÖ Escaping local optima
‚úÖ Structural recombination

One-sentence takeaway

Ansor uses evolutionary search because tensor program performance lives in a huge, discrete, jagged space where gradients don‚Äôt exist and greedy pruning kills good solutions.

If you want a sharper framing

Think of it like:

Domain	Technique
Neural weights	Gradient descent
Neural architecture	Evolution / NAS
Tensor kernels	Evolutionary kernel search ‚úÖ

If you‚Äôd like, I can explain:

Why crossover is so important

Or compare this to RL-based schedulers

* * *

**You:**

How does this compare to Triton kernels?

* * *

**ChatGPT:**

This is a really good comparison to make ‚Äî Ansor and Triton are philosophically close but operate at different abstraction layers.

I‚Äôll give you a clean, technical comparison:

1. What Triton actually is

Triton is not an auto-scheduler ‚Äî it‚Äôs a GPU kernel programming language.

You still write the kernel logic yourself:

python
pid_m = tl.program_id(0)
pid_n = tl.program_id(1)

a = tl.load(A_ptr + offsets)
b = tl.load(B_ptr + offsets)
acc += a * b
tl.store(C_ptr, acc)


Then Triton auto-tunes numeric parameters like:

Block sizes

Warp counts

Pipeline stages

But the program structure is human-written.

So Triton‚Äôs model is:

Human writes the algorithm ‚Üí System tunes parameters

2. What Ansor is doing differently

Ansor is fundamentally doing:

System synthesizes the algorithm ‚Üí System tunes parameters

The kernel structure itself is not written by a human.

You only provide the math:

text
C[m, n] = sum_k A[m,k] * B[k,n]


Everything after that ‚Äî tiling, memory staging, scheduling ‚Äî is machine-generated.

3. Side-by-side comparison
Aspect	Triton	Ansor
Who writes kernel structure?	Human	Compiler
Starting input	Imperative kernel	Declarative math
Tiling structure	Handwritten	Search-generated
Memory movement	Handcoded	Discovered by rules
Auto-tuning	Yes	Yes
Search type	Grid / heuristic	Evolutionary
Fusion across ops	Manual	Automatic
Portability	GPU only	CPU + GPU + ARM
Use of templates	Human kernels	No handwritten templates
4. How their search spaces differ

Think of the search space as:

Triton:
scss
Fixed kernel skeleton
  ‚Üì
Tune numeric parameters


Like:

text
(trainable space) = \prod block_sizes √ó \prod warp_counts √ó ...

Ansor:
sql
Search over program skeletons
  ‚Üì
Search over numeric parameters


Like:

text
(program structures) √ó (parameter space)


This is exponentially larger and more general.

5. What this means in practice
Triton shines when:

‚úÖ You already know a good kernel pattern (e.g. FlashAttention)
‚úÖ You want fast prototyping of CUDA kernels
‚úÖ You only care about GPUs

Ansor shines when:

‚úÖ You want the compiler to invent schedules
‚úÖ You need portability across devices
‚úÖ You want to fuse multiple ops automatically
‚úÖ You don‚Äôt want to hand-design kernels

6. A concrete example
Matmul in Triton

You would write something like:

python
pid_m = tl.program_id(0)
pid_n = tl.program_id(1)

for k in range(0, K, TK):
    a = tl.load(...)
    b = tl.load(...)
    acc += a * b


Then Triton tunes:

num_warp ‚àà {2, 4, 8}
block_m ‚àà {64, 128}
block_n ‚àà {64, 128}

Matmul in Ansor

You don‚Äôt write any of that.

You only say:

ini
C = A @ B


Then Ansor:

‚úÖ Invents the tiling
‚úÖ Decides shared memory
‚úÖ Decides warp/thread mapping
‚úÖ Tunes parameters via evolution

7. Where Triton still wins today

Triton still dominates for:

Hand-optimized attention kernels

Production CUDA kernels with known structure (FlashAttention, etc.)

Developer velocity for expert CUDA programmers

Ansor-style systems shine more in:

Research compilers

Portable deployment

Fully automatic performance

8. How they relate philosophically

You can think of it like:

Layer	Role
CUDA	Assembly
Triton	High-level GPU assembly
Ansor	Compiler that writes the Triton/CUDA-style kernel for you
One-sentence takeaway

Triton auto-tunes the parameters of human-written GPU kernels, while Ansor automatically discovers both the kernel structure and parameters from high-level math.

If you want, I can also compare Ansor vs FlashAttention, or explain how modern systems combine these ideas.