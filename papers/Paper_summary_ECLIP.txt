**You:**

2506.12598v1.pdf
PDF
Can you summarize this paper?

* * *

**ChatGPT:**

Here’s a clear, concise summary of the paper “ECLIP: Energy-efficient and Practical Co-Location of ML Inference on Spatially Partitioned GPUs”:

Problem

ML inference workloads often underutilize GPUs, leaving many compute units (CUs) idle and wasting energy. Co-locating multiple models on the same GPU can improve utilization, but current spatial partitioning methods (e.g., Nvidia MPS/MIG or naive CU masking) are too slow or too coarse-grained to be practical. Fine-grained, per-kernel partitioning causes huge overheads due to expensive repartitioning and synchronization.

Key Idea

The paper introduces ECLIP, a system that enables practical, low-overhead, kernel-level GPU partitioning for co-located ML inference on real hardware (AMD GPUs using ROCm), without modifying ML frameworks like PyTorch.

Instead of repartitioning for every kernel, ECLIP:

Uses pre-allocated CU-masked streams to avoid expensive runtime IOCTL calls.

Redirects kernels on the fly to the correct pre-masked stream.

Enforces correctness using lightweight barrier injection only when needed.

Optimizes CU assignments per group of kernels, not per individual kernel.

How ECLIP Works

1. Pre-allocated CU-masked streams (Runtime Scheduler)

Creates a fixed pool of streams with predefined CU masks (e.g., 15, 30, 45, 60 CUs).

Avoids the high and unpredictable latency of per-kernel CU mask system calls (shown on page 3’s timing graphs).

2. Dependency tracking

When kernels are redirected across streams, ECLIP injects barrier packets only when needed to preserve dependencies (shown in the architecture diagram on page 3).

3. Resource Allocation Optimizer
ECLIP uses an Integer Linear Programming (ILP) model to:

Choose CU allocations per group of kernels

Limit expensive CU switching using a barrier budget

Ensure fairness across co-located models

This balances:

Kernel speed

Co-location slowdown

Repartitioning overhead

Results (from figures and tables on pages 5–6)

Compared to:

No partitioning (baseline)

Model-level partitioning

Naive kernel-level partitioning

ECLIP achieves:

Up to 21% higher throughput

25% average energy-efficiency improvement

Up to 35% energy-efficiency improvement in the best case

Tail latency remains controlled (unlike naive kernel-wise switching)

Table I (page 6) shows that ECLIP dramatically reduces CU switching overhead compared to naive kernel-level approaches.

Main Contributions

First practical kernel-grain GPU partitioning system deployable on real hardware.

Eliminates most repartitioning overhead via stream pooling.

Introduces a fair, optimization-driven CU allocation model.

Transparent to existing ML frameworks.

One-Line Takeaway

ECLIP enables efficient, low-overhead co-location of ML inference workloads by combining pre-allocated GPU partitions with smart kernel grouping and optimization, significantly improving performance and energy efficiency.

If you’d like, I can also give you:

A comparison table vs MPS/MIG/KRISP

A visual pipeline diagram

Or a deeper breakdown of the ILP optimizer

Citation: 

2506.12598v1

* * *

**You:**

Can you show me a visual pipeline diagram

* * *

**You:**

What is the order of these operations in terms of optimizer and scheduler

* * *

**ChatGPT:**

Here’s the actual execution order of operations in ECLIP, focused specifically on the optimizer vs. scheduler flow:

✅ High-Level Order of Operations

ECLIP splits into two distinct phases:

Phase 1 — Offline (Before Runtime): Optimizer Runs First

This happens before the system is serving real traffic.

Order:

Kernel Profiling

Each model’s kernels are profiled to measure:

Runtime at different CU counts (15 / 30 / 45 / 60)

Minimum CU “thresholds” per kernel

Resource Allocation Optimizer (ILP Solver)

Takes profiling data + co-location scenarios

Solves the Integer Linear Program

Decides:

How to group kernels

What CU count each group should use

Where to limit CU switching (barrier budget)

Lookup Table Generation

Outputs a static lookup table:

scss
(kernel_id → CU_mask_stream_id)


Stored for runtime.

✅ This phase runs once per model/mix scenario, not per request.

Phase 2 — Online (Runtime): Scheduler Runs Per Kernel

This happens during live inference.

Order (per kernel launch):

Kernel Arrives at ROCm / HIP Runtime

From PyTorch or other ML framework.

Runtime Scheduler Intercepts Kernel

Looks up the kernel in the precomputed optimizer table:

ini
stream_id = LUT[kernel_id]


Dependency Checker Runs

Checks if previous kernel in the same user stream:

Has completed → ✅ no action

Has not completed → ❗ inject barrier packet

Kernel Redirected

Dispatched to the correct pre-allocated CU-masked stream.

Completion Signal Tracked

Used to enforce dependency correctness for future kernels.

✅ Simple Execution Timeline

Here’s the clean ordering:

markdown
OFFLINE (once per model / workload mix):
------------------------------------------------
Kernel Profiling
        ↓
ILP Resource Allocation Optimizer
        ↓
Lookup Table Generation


ONLINE (every inference request):
------------------------------------------------
Kernel enters ROCm runtime
        ↓
Scheduler intercepts kernel
        ↓
Lookup Table consulted
        ↓
Dependency / Barrier check
        ↓
Kernel dispatched to CU-masked stream
        ↓
Completion signal recorded

✅ Key Relationship Between Optimizer & Scheduler

Important principle:

The optimizer never runs at runtime.

The scheduler never makes resource decisions.

Their roles are strictly separated:

Component	Responsibility
Optimizer	Decides policies (offline)
Scheduler	Enforces policies (online)
One-Sentence Summary

The optimizer runs first (offline) to generate policies, and the scheduler runs afterward (online) to enforce those policies per kernel in real time.

If you want, I can also draw:

A time-sequenced diagram

A per-kernel pipeline

Or a comparison vs MPS/MIG flow

* * *

**You:**

so the optimizer runs during model compile time or some time else?

* * *

**ChatGPT:**

Short answer: it does not run during model compile time.

Here’s the precise timing:

When the Optimizer Runs in ECLIP

The optimizer runs offline, during system setup / profiling time, not during:

❌ Model compilation
❌ Model tracing
❌ TorchScript / JIT
❌ ROCm kernel compilation

✅ It runs during inference server provisioning / profiling phase

Concrete Timeline

Think of ECLIP’s lifecycle like this:

1. Model Setup Phase (Before Serving Requests)

This is when the optimizer runs.

Typical steps:

pgsql
Load model
Run profiling workloads
Measure kernel runtimes at {15,30,45,60} CU
Record co-location slowdowns
Run ILP optimizer (e.g., Gurobi)
Generate lookup table (kernel → CU mask stream)
Persist lookup table


This happens:

When the inference server starts

Or when a new co-location scenario is configured

2. Model “Compile Time” (What it is not)

When you think of compile time, that usually means:

Phase	Runs here?
TorchScript tracing	❌ No
XLA / TorchInductor compile	❌ No
HIP kernel compile (HSA)	❌ No
PTX / LLVM lowering	❌ No

ECLIP does not modify kernels at compile time.

3. Runtime Serving Phase

Here, the scheduler runs instead.

At runtime:

arduino
Kernel launch → Scheduler intercept → LUT → CU stream → Execute


No optimization solving happens here.

Why This Matters Architecturally

ECLIP deliberately keeps optimization out of the critical path, because:

On-the-fly ILP solving would be far too slow

Real-time kernel launches are µs-scale

So ECLIP trades:

Dynamic adaptability
for
Predictable low-overhead scheduling

Mental Model

You can think of it like:

System	When optimization happens
CUDA Graphs	Ahead-of-time
TensorRT	Ahead-of-time
XLA	Compile-time
ECLIP	Deployment-time ✅
One-Line Summary

ECLIP’s optimizer runs during model deployment / server setup time, not during model compilation.

If helpful, I can explain where in a real ML serving stack (Triton, TorchServe, vLLM) this would sit.

* * *

**You:**

where in a real ML serving stack would this sit

* * *

**ChatGPT:**

Great question — here’s exactly where ECLIP would sit in a real ML serving stack in practice.

I’ll map it to a modern production-style stack, then give concrete examples with Triton, TorchServe, and vLLM-style stacks.

✅ Actual Placement in the ML Serving Stack

ECLIP lives below the ML framework, but above the raw GPU driver.

The clean mental model:

scss
User Requests
      ↓
API Layer (HTTP / gRPC)
      ↓
Model Server
(Triton / TorchServe / vLLM)
      ↓
Framework Runtime
(PyTorch / TensorFlow / CUDA / ROCm)
      ↓
ECLIP Layer  ←––– HERE
(runtime scheduler + CU streams)
      ↓
GPU Driver (ROCm / HIP / HSA)
      ↓
GPU Hardware


ECLIP directly modifies the GPU runtime layer, not the model server.

✅ Triton Inference Server

Stack looks like this:

bash
Client
  ↓
Triton HTTP/gRPC
  ↓
Backend: PyTorch / TensorRT
  ↓
PyTorch CUDA / HIP Calls
  ↓
ROCm Runtime (HIP / HSA)
  ↓
ECLIP inserted here
  ↓
Kernel launch → CU-masked streams


Key point:
Triton doesn’t know ECLIP exists — it's transparent.

Where optimizer runs:

During Triton model load / server warmup

Or offline provisioning

✅ TorchServe

TorchServe stack:

arduino
Client
  ↓
TorchServe REST API
  ↓
Python Worker
  ↓
PyTorch
  ↓
HIP / CUDA Runtime
  ↓
ECLIP layer
  ↓
GPU


TorchServe stays unchanged.

The only modified layer is ROCm’s runtime.

✅ vLLM / custom CUDA kernel server

Example:

vbnet
Client
  ↓
Async scheduler
  ↓
vLLM engine
  ↓
Torch + custom CUDA/HIP kernels
  ↓
ROCm runtime
  ↓
ECLIP
  ↓
GPU


ECLIP intercepts:

hipLaunchKernel

Stream dispatch

CU mask routing

vLLM doesn’t need to know.

✅ Where the Optimizer Specifically Runs

Optimizer does not live in the hot path.

Typical deployment:

pgsql
Triton start
   ↓
Model loaded
   ↓
Warm-up profiling runs
   ↓
ECLIP optimizer generates LUT
   ↓
LUT stored in ROCm runtime
   ↓
Server starts serving traffic


Scheduler then runs per kernel.

Layered View (Easiest Mental Map)

Think of ECLIP as part of the GPU virtualization/resource control plane:

diff
+-------------------------+
| Model Serving Layer     |
| (Triton / TorchServe)   |
+-------------------------+
| ML Framework Runtime    |
| (PyTorch / TF)          |
+-------------------------+
|  ECLIP Scheduler Layer  |  ← runtime
+-------------------------+
|  ROCm / HIP / HSA       |
+-------------------------+
|  AMD GPU Hardware       |
+-------------------------+

✅ Summary Sentence

In a real ML serving system, ECLIP lives inside the GPU runtime layer (ROCm/HIP), below PyTorch and the model server, where it intercepts kernel launches and controls how GPU compute resources are partitioned.

If useful, I can also compare:

How this differs from MIG/MPS

What this would look like on Nvidia

Or how you’d prototype something ECLIP-like in CUDA

* * *

**You:**

Can you go into more details about how the runtime scheduler works? It does analysis across kernels for different tasks?

* * *

**ChatGPT:**

Great question — this is actually the core subtlety of ECLIP, and the answer is:

The runtime scheduler does not analyze kernels dynamically for performance.
It performs mechanical enforcement of a pre-computed plan and only tracks ordering and dependencies, not performance.

Let’s unpack exactly what it does and what it doesn’t do.

What the Runtime Scheduler Actually Does

The scheduler is essentially a fast dispatch layer inside ROCm that performs three real-time tasks:

1. Kernel Interception

Every kernel launch passes through the HIP/ROCm dispatch path.

Scheduler hooks into:

cpp
hipLaunchKernel → ROCclr → HSA queue dispatch


When a kernel arrives, it extracts:

Kernel ID / symbol

User stream ID

Worker (model) identity

2. Lookup-Based Routing (No Online Optimization)

The scheduler uses a precomputed lookup table generated offline by the optimizer.

Conceptually:

c
stream_id = LUT[model_id][kernel_id]


This table already encodes:

Kernel grouping

CU size assignment

Allowed switching budget

So the scheduler does zero performance modeling — it just enforces this mapping.

3. Cross-Stream Dependency Enforcement

This is the only thing that is truly “dynamic”.

Once kernels are redirected to different CU-masked streams, normal FIFO ordering is broken.

So the scheduler tracks:

scss
(user_stream_id, last_kernel_completion_signal)


When launching kernel K:

text
If previous kernel in same user stream is still running:
    Inject HSA barrier packet
Else:
    Launch immediately


This ensures correctness.

What It Does Not Do

The scheduler does not:

Feature	Runtime Scheduler
Dynamic kernel profiling	❌
Adaptive CU resizing	❌
Reoptimization	❌
Task migration	❌
Latency prediction	❌
Work stealing	❌

All intelligence lives in the offline optimizer.

Across-Task / Multi-Model Interactions

Here’s the critical architectural point:

The scheduler does not coordinate across tasks explicitly.

Instead, coordination happens implicitly because:

Each model’s kernels are mapped to disjoint CU-masked streams

The offline optimizer ensured those streams don’t oversubscribe too much

At runtime:

Tasks naturally coexist

Streams enforce spatial partitioning

There is no global arbitration loop

Does It Analyze Kernel Sequences Across Tasks?

✅ Offline: yes
❌ Runtime: no

During offline optimization, the ILP model:

Sees full kernel sequences from all workers

Models CU overlap

Plans fair partitions

At runtime, the scheduler:

Only sees one kernel at a time

Applies static routing rules

How It Scales Across Many Models

Imagine:

yaml
Worker A: K1 → K2 → K3
Worker B: K1 → K2 → K3
Worker C: K1 → K2 → K3


Offline optimizer decides:

makefile
A: K1 → 30 CU, K2 → 45 CU, K3 → 30 CU
B: K1 → 15 CU, K2 → 30 CU, K3 → 15 CU
C: K1 → 15 CU, K2 → 15 CU, K3 → 15 CU


Runtime scheduler behavior:

text
Kernel arrives → LUT → stream → barrier check → dispatch


No cross-model negotiation at runtime.

Minimal Pseudocode (Realistic)
c
on_kernel_launch(kernel k, stream s):
    model = get_worker_id()
    next_stream = LUT[model][k.id]

    if prev_kernel_pending(model):
        inject_barrier()

    dispatch(k, next_stream)
    record_completion_signal(k)

High-Level Mental Model

Think of ECLIP’s scheduler more like:

✅ A traffic cop following a route map
not
❌ A live traffic optimizer

Bottom Line

The runtime scheduler does not perform performance analysis or cross-task optimization. It simply enforces a precomputed kernel-to-resource mapping and preserves correctness with lightweight dependency tracking.

If helpful, I can also:

Compare this to OS schedulers

Explain how this differs from CUDA streams

Or sketch what a dynamic version of ECLIP would look like.

* * *

**You:**

Wait so is the runtime scheduler just a lookup table? or does it have actual logic

* * *

**ChatGPT:**

Short answer: it’s more than just a lookup table, but it’s not a “smart” optimizer.

Think of it as a fast, stateful dispatcher, not an analyzer.

Here’s the clear breakdown.

What the Runtime Scheduler Actually Contains

It has three real pieces of logic:

1. Lookup Logic (Static Policy Layer)

Yes — it uses a table like:

text
(kernel_id, worker_id) → cu_masked_stream_id


But this is only the policy input.

2. State Tracking Logic (Dynamic Enforcement Layer)

This is the real “runtime intelligence”.

It maintains live state, such as:

Last kernel issued per user stream

Completion signals from the GPU

Which kernels are “in flight”

This is not static:

c
state[user_stream] = last_completion_signal


This state determines whether to inject barriers.

3. Dependency Resolution Logic

On every kernel dispatch:

text
IF previous kernel from same user stream is unfinished:
    Inject HSA barrier packet
ELSE:
    Skip barrier


This decision requires live checking, not just lookup.

What It Does Not Contain

It does not contain:

Capability	Present?
ILP solver	❌
Latency predictor	❌
Adaptive resizing	❌
Cross-worker optimization	❌
Dynamic fairness adjustment	❌
So What Is It Really?

The best mental model:

System Type	Best Description
ECLIP runtime	Policy-driven state machine ✅
Pure lookup table	❌
Dynamic scheduler	❌
Real-time optimizer	❌
Visual Concept

Instead of:

nginx
kernel → lookup → dispatch


It is more like:

pgsql
kernel arrives
     ↓
lookup (policy)
     ↓
check dependency state
     ↓
maybe inject barrier
     ↓
dispatch to stream
     ↓
update completion state

Why This Design

Because per-kernel launch happens at microsecond scale.

They wanted:

✅ Near-zero overhead
✅ No optimization latency
✅ Predictable behavior

One-Line Summary

The runtime scheduler is not just a lookup table — it’s a small state machine that enforces precomputed decisions and correctness, but it does not perform live optimization.

If you want, I can diagram it formally or compare it to OS schedulers or NVIDIA stream semantics.

* * *

**You:**

I'm not too familiar with ROCm, can you explain what it is and how it fits with ECLIP?


* * *

**ChatGPT:**

Absolutely — here’s a clean, practical explanation of ROCm and how it connects to ECLIP, without assuming prior AMD/GPU-runtime knowledge.

1. What ROCm Actually Is (in plain terms)

ROCm (Radeon Open Compute) is AMD’s equivalent of CUDA.

If you already know CUDA:

NVIDIA	AMD
CUDA	ROCm
cuBLAS	rocBLAS
cuDNN	MIOpen
CUDA Streams	HIP Streams
CUDA Runtime	HIP Runtime

So when PyTorch runs on an AMD GPU, it does:

nginx
PyTorch → HIP → ROCm → GPU

2. ROCm’s Internal Layers (where ECLIP hooks)

ROCm has a layered architecture:

sql
PyTorch / TensorFlow
       ↓
HIP API   ← (CUDA-like front-end)
       ↓
ROCm Runtime / ROCclr
       ↓
HSA Runtime (Queues, Packets)
       ↓
AMD Kernel Driver
       ↓
AMD GPU Hardware


ECLIP modifies the middle two layers:

✅ HIP dispatch
✅ ROCclr
✅ HSA queue handling

That’s where kernel routing and CU masking occur.

3. Why ROCm Matters for ECLIP

The key hardware feature is:

CU Masking

AMD GPUs let software say:

“Only use this subset of Compute Units for this kernel.”

This happens in ROCm via streams with attached CU masks.

ROCm lets you do:

text
Stream A → uses CUs {0..14}
Stream B → uses CUs {15..29}
Stream C → uses CUs {30..59}


CUDA doesn’t expose this kind of fine-grained masking natively.

That’s why ECLIP is practical on ROCm/AMD GPUs.

4. How ECLIP Fits into ROCm

Here’s the real stack with ECLIP placed:

java
PyTorch Model Code
        ↓
HIP Kernel Launch
        ↓
ROCm / ROCclr
        ↓
ECLIP Scheduler (custom ROCclr layer)
        ↓
Pre-allocated CU-masked Streams
        ↓
HSA Runtime (queues)
        ↓
AMD GPU Kernel Driver
        ↓
AMD GPU Hardware


ECLIP modifies:

kernel dispatch path

how streams are selected

when barriers are injected

5. What ROCm Normally Does vs With ECLIP
Normal ROCm behavior:
scss
Kernel → default stream → all CUs used

With ECLIP:
cpp
Kernel → intercepted
       → lookup table
       → redirected to CU-masked stream
       → optional barrier injection
       → dispatched


Everything else is the same.

6. Relationship to PyTorch

Important:

✅ PyTorch does not change
✅ Model code does not change
✅ No kernel recompilation

All intelligence lives inside ROCm.

7. Mental Analogy

Think of:

ROCm = operating system for the GPU

ECLIP = a custom process scheduler inside that OS

8. One-Sentence Summary

ROCm is AMD’s GPU runtime stack (like CUDA), and ECLIP is implemented as a custom kernel-dispatch and stream-routing layer inside ROCm that controls how GPU compute units are partitioned and shared.

If helpful, I can next explain:

How HSA queues work

How HIP streams map to hardware

Or how similar ideas could work on NVIDIA GPUs

* * *

**You:**

Is there a ECLIP equivalent for CUDA?

* * *

**ChatGPT:**

Short answer: there is no direct, production-grade CUDA equivalent of ECLIP today — and that’s mostly because NVIDIA doesn’t expose the same kind of hardware control that AMD does.

That said, there are partial equivalents and research systems. Here’s the real landscape.

Why CUDA Can’t Easily Do What ECLIP Does

ECLIP depends on stream-level CU (Compute Unit) masking.

Capability	AMD ROCm	NVIDIA CUDA
Per-stream compute unit masking	✅	❌
Fine-grain SM control (official)	✅	❌
No-IOCTL CU/SM switch	✅	❌

On NVIDIA, these low-level controls are mostly not exposed.

That’s why ECLIP was built specifically on ROCm.

Closest CUDA-Side Equivalents
1. MIG (Multi-Instance GPU)

This is NVIDIA’s official spatial partitioning:

✅ Hardware-level isolation
❌ Very coarse-grained
❌ High reconfiguration overhead
❌ Cannot change per kernel

Used for:

Multi-tenant inference servers

Cloud environments

But not suitable for kernel-granularity switching.

2. MPS (Multi-Process Service)

NVIDIA’s software concurrency system:

✅ Multiple processes share GPU
❌ No real spatial partitioning
❌ Still contention-prone
❌ No per-kernel SM control

This is closer to “temporal multiplexing” than spatial.

3. libsmctrl (Research Tool)

Closest thing to CUDA-side ECLIP:

Abuses undocumented debug APIs

Can set SM masks per kernel

Can be stream-scoped

But:

❌ Not production safe
❌ Requires framework modifications
❌ Breaks with driver updates

ECLIP explicitly mentions this as impractical.

4. KRISP (Research System)

CUDA-side conceptual twin of ECLIP:

Proposes microarchitectural extensions to NVIDIA GPUs

Enables per-kernel SM masking

But requires hardware changes

So not usable on real-world deployed GPUs.

Practical Differences: AMD vs NVIDIA

Here’s the real reason:

Feature	AMD	NVIDIA
Exposed fine-grain CU/SM mask	✅	❌
Streams control physical units	✅	❌
Userspace stream masking	✅	❌

AMD exposes partitioning in software; NVIDIA hides it behind firmware.

Is It Possible to Build ECLIP-like System on CUDA?

Only with heavy compromises:

✅ You can approximate using:

CUDA Graphs

Stream priorities

Kernel fusion

MPS + controlled overlaunch

❌ You cannot do true per-kernel SM partitioning.

Conceptual CUDA-side ECLIP Imitation

You could approximate:

makefile
Offline: profile kernel overlap
Runtime:
    use stream priorities
    artificially delay kernels
    fuse small kernels


But this is temporal scheduling, not spatial.

Bottom Line

There is no real CUDA equivalent to ECLIP today because NVIDIA does not expose the per-SM masking primitives that ECLIP relies on.

Closest things are:

System	How close?
libsmctrl	Research-grade only
KRISP	Hardware-dependent
MIG	Production, but coarse
MPS	Concurrent but unpartitioned
If You Want

I can explain:

How MIG + CUDA Graphs + priorities can approximate ECLIP

What it would take to build CUDA-ECLIP with driver hooks

Or how cloud vendors try to solve this problem today

* * *

**You:**

Can you explain more about the optimizer? How does it generate the lookup table?

* * *

**ChatGPT:**

Great — this is the right place to drill down.
Here’s a clear, step-by-step explanation of how ECLIP’s optimizer works and how it produces the lookup table without hand-waving.

I’ll keep it practical, not just math.

Big Picture

The optimizer’s job is to answer this question:

For every kernel in every model, which CU configuration should it run on, and where should we intentionally avoid switching to reduce overhead?

The result is a kernel → stream routing plan, which becomes the lookup table.

Step 1: Inputs to the Optimizer

The optimizer works offline and takes these inputs:

1. Kernel Profiling Data

For every kernel in a model, they profile:

yaml
kernel_i runtime at:
  - 15 CUs
  - 30 CUs
  - 45 CUs
  - 60 CUs


This produces a table like:

text
Kernel   15CU   30CU   45CU   60CU
K1       42μs   29μs   24μs   23μs
K2       10μs    8μs    8μs    8μs
…


This data becomes the profile[...] function in the paper.

2. Kernel Sequences Per Model

Each model is represented as:

text
Worker W1: K1 → K2 → K3 → K4 ...
Worker W2: K1 → K2 → K3 → K4 ...


So the optimizer knows execution order.

3. Co-location Scenarios

The optimizer is run for specific serving mixes:

Example:

text
Mix 1 = {2x Albert, 1x DenseNet}
Mix 2 = {Albert + DenseNet}


So it can reason about interference.

4. Barrier Budget Constraint

From empirical results:

ini
switchMax = 14


That means:

“You are not allowed to change CU configs more than 14 times per request per worker.”

This forces kernel grouping.

Step 2: Decision Variables

The optimizer creates Boolean variables:

text
x[kernel, CU_config] ∈ {0,1}


Example:

x[K3, 30CU] = 1 means K3 uses 30 CU stream


Constraint:

arduino
Each kernel picks exactly 1 CU config

Step 3: Modeling Costs

For each kernel, the optimizer models estimated runtime.

Runtime includes:

a) Base runtime from profiling

Example:

mathematica
βk = runtime of K at chosen CU count

b) Co-location slowdown

Slowdown is modeled like this:

text
CUOverlap = avg_CUs_W1 + avg_CUs_W2 + ...
slowdown αk = CUOverlap / total_CUs


Then:

text
ek = βk * (1 + αk)


This simulates contention effects.

c) Cost of CU switching

Kernel switches are counted:

text
switches = |config[k] - config[k-1]| / 2


And must obey:

text
switches ≤ switchMax

Step 4: Global Optimization

Now the optimizer solves:

text
Minimize sum of ek for each worker
Subject to:
 - One CU config per kernel
 - Switch budget per worker
 - Fairness between workers


This is solved using a solver like Gurobi.

Step 5: Grouping Kernels Implicitly

Grouping happens automatically:

If switching CU count is expensive, the optimizer learns to:

text
Keep same CU config across consecutive kernels


This is why in the paper’s Figure 3 you see boxes over groups of kernels.

So “kernel groups” emerge naturally.

Step 6: Lookup Table Generation

Once solved, the optimizer converts results into:

Kernel → CU assignment

Example:

json
{
  "albert": {
    "K1": 30,
    "K2": 30,
    "K3": 45,
    "K4": 45,
    "K5": 30
  }
}

CU → Stream Mapping

Each CU count corresponds to a pre-allocated stream:

text
15CU → Stream_1
30CU → Stream_2
45CU → Stream_3
60CU → Default_Stream

Final Lookup Table

At runtime, the table looks like:

c
LUT[worker_id][kernel_id] = stream_id


This is what the scheduler uses.

Runtime vs Optimizer Responsibilities
Component	Role
Optimizer	Figures out what should happen
Scheduler	Executes that plan as fast as possible
Key Insight

The optimizer doesn’t explicitly “group kernels.”
It discourages switching, and grouping emerges as the cheapest solution.

One-Sentence Summary

ECLIP’s optimizer profiles kernel runtimes, models co-location slowdown and switching overhead as an ILP problem, then outputs a static kernel→CU→stream lookup table that the runtime scheduler enforces.

If helpful, I can also show:

sample ILP pseudocode

a visual flow diagram

or how this compares to scheduling in Triton or TVM

* * *

**You:**

show me an example ILP and how it deals with this

* * *

**ChatGPT:**

Below is a concrete, worked example of the kind of Integer Linear Program (ILP) ECLIP uses, simplified so you can “see it work” without drowning in symbols.

I’ll walk through a tiny model and show how the lookup table comes out.

Setup: Tiny Example

Assume:

1 model (worker W)

4 kernels: K1, K2, K3, K4

Possible CU configs: {15, 30, 45, 60}

Switch budget = 2

Total GPU CUs = 60

Profiling Data (μs)
Kernel	15 CU	30 CU	45 CU	60 CU
K1	50	30	25	24
K2	20	12	11	11
K3	40	26	21	20
K4	10	7	6	6
Step 1: Define Decision Variables

Binary variables:

$$x_{k,c} ∈ \{0,1\}$$

Meaning:

x[K1,30] = 1  → K1 uses 30 CUs

Step 2: Constraints
A. Exactly One CU Choice per Kernel
$$\sum_{c∈\{15,30,45,60\}} x_{k,c} = 1 \quad ∀k$$

Meaning:

text
Each kernel picks exactly one CU config

B. Switching Budget Constraint

Define switching penalty:

$$switches = \sum_k \sum_c \frac{|x_{k,c} - x_{k-1,c}|}{2}$$

Constraint:

$$switches ≤ 2$$

Meaning:
You can change CU config at most twice.

C. Execution Time Model

For each kernel:

$$β_k = \sum_c x_{k,c} \cdot profile[k][c]$$

Example:

text
If K2 chooses 30 CU → β₂ = 12 μs

D. Co-location Slowdown (optional for this example)

Skip for now since single worker:

$$e_k = β_k$$
Step 3: Objective Function

Minimize total runtime:

$$\min \sum_k e_k$$
Step 4: What the Solver “Learns”

Let’s reason heuristically like the solver:

If there were no switching cost:

The best per-kernel picks:

Kernel	Best CU
K1	60 CU
K2	30 CU or 45 CU
K3	60 CU
K4	45 CU

This gives:

60 → 30 → 60 → 45


Switches:

scss
60→30 (1), 30→60 (2), 60→45 (3) ❌ exceeds budget

Solver Must Respect Switch Budget = 2

It tries smoother patterns, like:

Option A:
45 → 45 → 45 → 45


Switches = 0 ✅
Time = 25 + 11 + 21 + 6 = 63

Option B:
30 → 30 → 45 → 45


Switches = 1 ✅
Time = 30 + 12 + 21 + 6 = 69

Option C:
60 → 60 → 60 → 60


Switches = 0 ✅
Time = 24 + 11 + 20 + 6 = 61 ✅ BEST

Solution the ILP Picks

Final assignments:

Kernel	Chosen CU
K1	60
K2	60
K3	60
K4	60
Step 5: Generate Lookup Table

Now the optimizer emits:

json
{
  "worker_0": {
    "K1": 60,
    "K2": 60,
    "K3": 60,
    "K4": 60
  }
}


Then maps CU → stream:

json
{
  "K1": "stream_60cu",
  "K2": "stream_60cu",
  "K3": "stream_60cu",
  "K4": "stream_60cu"
}


This becomes the runtime LUT.

How This “Deals” With Real Trade-offs

The solver balances:

Force	Effect
Kernel speed	Wants bigger CU counts
Switch cost	Discourages frequent changes
Co-location slowdown	Avoids CU overlap spikes
Fairness	Keeps workers similar runtime

Grouping naturally emerges from these constraints.

TL;DR in Plain English

The ILP tries every kernel/CU assignment combination, penalizes switching, and picks a pattern that is slightly slower per kernel but globally faster once overheads are included.

If you want, I can next show:

a version with multi-worker slowdown

or actual Gurobi-style pseudocode