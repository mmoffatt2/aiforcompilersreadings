**You:**

1805.00907v3 (1).pdf
PDF
Can you summarize this paper?

* * *

**ChatGPT:**

Hereâ€™s a clear, concise summary of the paper you uploaded:

Paper Summary: â€œGlow: Graph Lowering Compiler Techniques for Neural Networksâ€
What problem does this paper tackle?

The paper introduces Glow, a compiler designed to efficiently run neural networks on heterogeneous hardware (CPUs, GPUs, DSPs, and custom accelerators). The core challenge is that modern ML frameworks donâ€™t scale well: supporting many operators Ã— many hardware backends leads to a combinatorial explosion of implementations.

Glow solves this by acting as a graph-level neural network compiler that performs aggressive optimizations before code reaches hardware-specific backends. 

1805.00907v3 (1)

Core Idea

Glow is built around graph lowering, which means:

Start with a high-level neural network graph

Progressively lower it through multiple intermediate representations (IRs)

End with hardware-specific optimized code

The key innovation:
Instead of requiring each backend to implement hundreds of NN operators, Glow reduces everything down to a small set of linear algebra primitives (like MatMul, Add, etc.). 

1805.00907v3 (1)

Architecture Overview

Glow uses a two-level IR system:

1. High-Level IR (Graph IR)

A strongly typed dataflow graph

Nodes represent high-level ops like Convolution, ReLU, FullyConnected

Enables:

Constant folding

Graph rewriting

Automatic differentiation

2. Low-Level IR (Instruction IR)

Address-based instruction stream

Explicit memory handling

Enables:

Memory scheduling

Buffer reuse

Copy elimination

Asynchronous DMA modeling

Backends can optimize or replace parts of this pipeline. 

1805.00907v3 (1)

Key Techniques
ğŸ”¹ Node Lowering

High-level operators are decomposed into simpler ops:

FullyConnected â†’ MatMul + Add

Training nodes (like SGD) â†’ Add, Sub, Mul, etc.

This dramatically reduces the backend implementation burden. 

1805.00907v3 (1)

ğŸ”¹ Quantization

Glow supports profile-guided INT8 quantization:

Runs instrumented model to collect activation ranges

Recompiles using scale/offset per tensor

Minimizes floatâ€“int conversions

Folds â€œrescaleâ€ ops when possible

This allows efficient integer-only execution. 

1805.00907v3 (1)

ğŸ”¹ CPU Backend Innovations

The CPU backend:

Uses LLVM for codegen

Specializes kernels using tensor sizes and constant addresses

Performs operator stacking:

Fuses elementwise ops (e.g., Add + ReLU) into single loops

Reduces memory traffic and cache misses

The paper shows generated x86 SIMD assembly for this fusion. 

1805.00907v3 (1)

Runtime System

Glow provides a runtime with:

Partitioner â€“ splits graphs across devices

Provisioner â€“ assigns partitions to hardware

Device Manager â€“ abstracts each physical accelerator

Executor â€“ schedules and runs the computation graph

This enables multi-device execution and model sharding. 

1805.00907v3 (1)

Performance Results

Compared against TensorFlow XLA and TVM (on Intel Kaby Lake CPU):

Up to 2.7Ã— faster than TensorFlow

Up to 1.3Ã— faster than TVM

Key reasons:

Direct convolution (avoids im2col)

Shape-specialized codegen

Aggressive fusion and memory planning 

1805.00907v3 (1)

Big Picture: Why This Paper Matters

Glow introduced ideas that later influenced modern compiler stacks like MLIR and torch.compile:

Key contributions:
âœ… Multi-level IR for ML
âœ… Systematic operator lowering
âœ… Profile-guided quantization
âœ… Backend-agnostic optimizations
âœ… Whole-graph memory planning

It helped move ML frameworks away from â€œoperator librariesâ€ toward real compiler-driven ML stacks. 

1805.00907v3 (1)

One-sentence takeaway

Glow is an early, practical ML compiler that showed how lowering high-level neural network graphs into structured IRs enables scalable, hardware-agnostic optimization and code generation.

If youâ€™d like, I can also give you:

A visual pipeline diagram of Glow vs MLIR-style stacks

Comparison with XLA / TVM / MLIR

Or help relate this to modern torch.compile and Triton

* * *

**You:**

Can you compare GLOW with MLIR, XLA, TVM ect

* * *

**ChatGPT:**

Hereâ€™s a clear, systems-level comparison of Glow vs MLIR vs XLA vs TVM, focusing on what actually matters architecturally.

1. High-Level Positioning
System	What it is	Where it sits in the stack
Glow	Neural network compiler	Sits below frameworks, compiles NN graphs
MLIR	Compiler infrastructure	Defines extensible IRs used to build compilers
XLA	TensorFlowâ€™s compiler	Deeply integrated into TF/JAX
TVM	End-to-end ML compiler stack	Framework-agnostic compiler + autotuner
2. Core Architectural Differences
ğŸ”¹ Glow

Philosophy:
Lower everything into linear algebra as early as possible and optimize aggressively.

Key traits:

Two main IRs:

High-level graph IR

Low-level instruction IR

Emphasizes node lowering to simplify backend work

Built as a pragmatic, production compiler, not a research IR system

From the paper: Glowâ€™s goal is to reduce backend complexity by lowering NN ops into a small set of primitive ops. 

1805.00907v3 (1)

Strengths:
âœ… Simple backend model
âœ… Strong memory planning
âœ… Profile-guided quantization
âœ… Easy for hardware teams to target

Weaknesses:
âŒ Not very extensible today
âŒ Less active ecosystem (compared to MLIR/TVM)

ğŸ”¹ MLIR (Multi-Level IR)

Philosophy:
Build a unified compiler infrastructure with many dialects.

MLIR is NOT a compiler by itself â€” itâ€™s a toolkit used to build compilers.

Key features:

Feature	MLIR
Multiple IRs?	âœ… Many dialects
Strong typing	âœ…
Custom ops	âœ…
Progressive lowering	âœ…

Unlike Glow, MLIR is meant to be embedded inside many systems:

TensorFlow

XLA

IREE

Torch-MLIR

Think of it like:
Glow = full compiler
MLIR = compiler construction framework

ğŸ”¹ XLA (Accelerated Linear Algebra)

Philosophy:
Make TensorFlow graphs run fast on TPU/GPU/CPU.

Key traits:

Requires static shapes

Performs HLO graph rewriting

Lowers into LLVM / CUDA / TPU

Architecture:

markdown
TensorFlow graph
      â†“
    HLO IR
      â†“
Backend-specific codegen


Where it differs from Glow:

Aspect	Glow	XLA
Framework focus	PyTorch + ONNX	TensorFlow / JAX
IR design	Multi-level	Mostly single HLO
Extensibility	Moderate	Lower
Quantization	Profile-guided INT8	Supported, less central
Operator lowering	Very aggressive	Less early lowering

XLA tries to preserve larger fused ops longer; Glow lowers early. 

1805.00907v3 (1)

ğŸ”¹ TVM

Philosophy:
â€œLet the compiler search for the best scheduleâ€ instead of hand-writing kernels.

TVM is unique because it has:

A graph compiler

A tensor-level scheduling language

An autotuner

Pipeline:

markdown
Framework graph
    â†“
Relay IR
    â†“
Tensor Expression / Schedule search
    â†“
Codegen


Strengths:
âœ… Cross-hardware portability
âœ… Auto-tuning
âœ… Research-friendly
âœ… Massive operator coverage

Weaknesses:
âŒ Can be slow without tuning
âŒ Harder to debug
âŒ Scheduling can be fragile

3. IR Philosophy Comparison
System	IR Style	How many IRs?	Strength
Glow	Graph â†’ Instruction	2 main	Simplicity & memory control
MLIR	Dialect-based	Arbitrary	Extreme extensibility
XLA	HLO graph	Mostly 1	Strong algebraic fusion
TVM	Relay + Tensor IR	2+	Schedule search + autotuning
4. Performance Strategy Differences
Glow

Relies on:

Shape specialization

Operator stacking (fusion across passes)

Manual backend tuning

Glow shows strong CPU wins because it directly generates memory-efficient loops and avoids common overheads like im2col. 

1805.00907v3 (1)

XLA

Relies heavily on:

Large kernel fusion

Backend libraries (cuDNN, Eigen)

Static shapes

TVM

Uses:

Brute-force or evolutionary schedule search

Caches best kernels

Hardware-aware cost models

MLIR

Depends entirely on the compiler built on top of it.

MLIR itself doesnâ€™t define performance â€” it enables it.

5. Ecosystem Reality (2024â€“2025)
System	Real-world usage
Glow	Mostly legacy / research (Meta has moved toward other stacks)
MLIR	Dominant infrastructure layer (LLVM for ML)
XLA	Very widely used (TF + JAX + TPU ecosystems)
TVM	Strong in research + embedded + edge AI

Reality today:

MLIR has â€œwonâ€ as the IR infrastructure

XLA dominates Googleâ€™s stack

TVM is king for research + autotuning

Glow was more of a proof of concept that shaped later systems

6. Quick Intuition Map

Think of them like this:

Glow = â€œHardcode the right compiler structure for neural netsâ€

MLIR = â€œLet people build any compiler they wantâ€

XLA = â€œMake TensorFlow/JAX not suck performance-wiseâ€

TVM = â€œSearch the space of kernels automaticallyâ€

7. When Youâ€™d Choose Each
Use case	Best choice
Building a new compiler infrastructure	MLIR
Optimizing JAX / TensorFlow workloads	XLA
Auto-tuned kernels on weird hardware	TVM
Simple, controlled backend + memory optimization	Glow
8. Key Historical Impact

Glow didnâ€™t â€œwinâ€ long-term, but it influenced:

MLIRâ€™s multi-level lowering ideas

TorchScript â†’ torch.compile

Modern compiler-driven ML stacks

It essentially helped move the field away from â€œoperator librariesâ€ toward true compilers. 

1805.00907v3 (1)

9. One-Sentence Distillation

Glow pioneered practical graph lowering for NN compilers, MLIR generalized that idea into a universal compiler framework, XLA productionized it for TensorFlow/JAX, and TVM turned kernel optimization into a search problem.

If you want, I can also give you:

A visual pipeline diagram comparing them

A â€œMLIR vs XLA vs TVMâ€ deep IR comparison

Or how PyTorch 2.x fits into this landscape

* * *

**You:**

Lower everything into linear algebra as early as possible and optimize aggressively

Why is linear algebra useful for optimizations

* * *

**ChatGPT:**

Great question â€” this is exactly the right thing to probe, because this is the core philosophy behind Glow, XLA, MLIRâ€™s linalg dialect, and TVM.

Iâ€™ll explain why â€œlowering to linear algebra earlyâ€ unlocks optimization power in a way that normal compiler IRs (like LLVM) cannot.

1. Linear Algebra Ops Carry Semantic Information

At the graph level, an operation like:

scss
MatMul(A, B)
Conv(X, W)
BatchNorm(X)


is explicitly telling the compiler what mathematical object it is working with.

Compare this to LLVM:

c
for (i)
  for (j)
    for (k)
      C[i][j] += A[i][k] * B[k][j];


Now the compiler only sees:

loops

pointer arithmetic

memory loads/stores

It no longer knows that this is matrix multiplication.

Why that matters

With explicit linear algebra semantics, the compiler can safely apply transformations like:

âœ… tiling
âœ… vectorization
âœ… fusion
âœ… reordering
âœ… associativity-based rewrites

Without risking correctness issues.

This is why Glow moves to linear algebra early and keeps it explicit. 

1805.00907v3 (1)

2. Linear Algebra Is Structurally Regular

Linear algebra ops are:

Dense

Regular memory access patterns

Highly predictable

Data-parallel by nature

This gives the compiler optimization guarantees:

Operation	What compiler knows for free
MatMul	High arithmetic intensity
Conv	Windowed data reuse patterns
Elementwise ops	Safe SIMD across all elements

That makes it safe to:

Generate SIMD instructions

Pipeline ops

Tile into cache-friendly blocks

Low-level IR canâ€™t safely infer this structure.

3. Legal Transformations Are Much Easier to Prove

With linear algebra, compiler rules become mathematical identities, not complex memory aliasing proofs.

Example optimizations that are trivial at LA level:

âœ… Fusion
css
ReLU(MatMul(A,B))


can become:

css
FusedMatMulReLU(A,B)

âœ… Reordering
mathematica
(A @ B) @ C  â†’  A @ (B @ C)


At the loop/pointer level, this is dangerous; at the math level, itâ€™s legal by definition.

This is exactly why Glow/XLA avoid relying on C++ compilers for NN optimization. 

1805.00907v3 (1)

4. Linear Algebra Enables Static Memory Planning

Once everything is expressed as tensor ops, the compiler knows:

Tensor sizes

Lifetimes

Aliasing guarantees

That allows:

âœ… global memory allocation
âœ… buffer reuse
âœ… lifetime shrinking
âœ… copy elimination

Glowâ€™s low-level IR only becomes possible after this algebraic lowering. 

1805.00907v3 (1)

5. Hardware Maps Naturally to Linear Algebra

Modern hardware:

GPUs

TPUs

systolic arrays

vector engines

â€¦are literally built for matrix and tensor operations.

When operations are expressed as:

nginx
MatMul / GEMM / Conv / Reduction


the compiler can:

Map directly to CUDA cores

Use Tensor Cores

Generate TPU systolic schedules

Emit SIMD AVX/SVE code

If the IR is just loops and pointers, the backend has to rediscover that structure â€” often impossible.

6. Why Not Stay at High-Level NN Ops?

High-level NN ops (Softmax, Attention, BatchNorm) are:

Too diverse

Too domain-specific

Hard for new hardware teams to implement all variants

When lowered to linear algebra + elementwise ops, the backend only has to support:

GEMM / Conv

Pointwise ops

Reductions

Glow explicitly makes this design tradeoff. 

1805.00907v3 (1)

7. Comparison to Traditional Compilers
Traditional IR	Linear Algebra IR
Memory-centric	Math-centric
Hard alias analysis	Built-in structure
Conservative transforms	Aggressive legal transforms
Hardware-agnostic	Hardware-aligned

LLVM struggles because it loses the meaning of operations too early.

8. One Intuition

Think of the difference as:

Without linear algebra lowering
â¡ The compiler is reverse-engineering what you mean.

With early linear algebra lowering
â¡ The compiler starts from meaning and works downward.

9. One-Sentence Core Reason

Linear algebra makes program meaning explicit, so compilers can legally apply aggressive mathematical, memory, and hardware-level transformations that are impossible once operations degrade into generic loops and pointer arithmetic.

If you want, I can explain:

How MLIRâ€™s linalg dialect formalizes this

How XLAâ€™s HLO relies on the same trick

Or where this approach starts breaking down (e.g., sparse attention, control flow, dynamic shapes)

* * *

**You:**

I don't get how this is novel, weren't other compilers doing this. MLIR focuses on ops

* * *

**ChatGPT:**

This is a really sharp instinct â€” and youâ€™re basically right. The math itself was not novel. The novelty was about where and how aggressively this idea was applied in ML systems.

Let me be very direct and honest.

Short Honest Answer

Youâ€™re right:
Compilers have always optimized loop-based linear algebra.
GEMM has been hand-tuned since the 1980s.

What was actually novel in Glow/XLA/MLIR wasnâ€™t â€œlinear algebra is goodâ€ â€” it was:

treating entire neural networks as compiler programs and pushing that style of optimization up into the graph level, before LLVM ever sees a loop.

What Was NOT Novel

Youâ€™re correct about all of this:

BLAS had tiled matrix multiply decades ago

FFTW, ATLAS, MKL existed

Polyhedral compilation (e.g., Pluto) was already handling loop nests

Halide already separated algorithm from schedule

So no: Glow did not invent linear algebra optimization.

What Was Novel (and why people cared)

The novelty shows up at the systems level, not at the math level.

1. Neural Networks Werenâ€™t Treated as â€œCompiler Programsâ€ Before

Older ML frameworks (pre-2015-ish):

css
for op in graph:
    call_library_kernel(op)


This is interpreter-style execution.

Glow/XLA/TVM said:

âœ… Treat the whole network like a compiler IR
âœ… Perform whole-graph rewrites
âœ… Generate code instead of calling libraries

That shift was new at scale.

2. Early, Aggressive Lowering Was Unusual

Traditional stack before:

nginx
NN op â†’ CUDA kernel â†’ cuDNN â†’ opaque kernel


The compiler had:

no visibility into memory layout

no chance to fuse kernels

no global memory planning

Glow said:

css
NN graph
â†’ aggressively lower to LA immediately
â†’ own memory planning
â†’ own scheduling
â†’ then codegen


The timing of the lowering was the novelty.

3. MLIR Isnâ€™t â€œFocused on Opsâ€ â€” Itâ€™s Focused on IR Infrastructure

This is the key misunderstanding:

You said:

MLIR focuses on ops

But actually:

âœ… MLIR is not about which ops exist
âœ… Itâ€™s about how to build hierarchies of IRs

MLIR lets you define:

Graph-level dialects

Linear algebra dialects (linalg)

Buffer dialects

Lowering pipelines

So MLIR generalized what Glow hard-coded.

Glow did it like:

pgsql
High-level graph
â†“
Lowering
â†“
Instruction IR


MLIR said:

â€œWhat if users could define their own layers of representation?â€

Why MLIR Feels More â€œRealâ€ Than Glow

Glow was:

hardwired

purpose-built

fixed architecture

MLIR is:

extensible

dialect-driven

reusable by many stacks

So it feels more â€œcompiler-yâ€ because it is.

Glow is closer to a productized research compiler.

XLA vs Glow: Another Important Point

XLA did similar things, but with a different philosophy:

Glow	XLA
Lower early	Keep HLO longer
Simpler IRs	Fewer but richer ops
Reduce backend burden	Emphasize fusion

So even there, â€œlinear algebraâ€ itself wasnâ€™t new â€” the compiler integration was.

The Real Shift: Domain Semantics in IR

This is the true leap.

Before:

ini
IR = loops + memory + SSA


After:

ini
IR = tensors + shapes + algebra


That let compilers legally do:

operator fusion

layout changes

buffer reuse

â€¦in a way LLVM could never safely assume.

Thatâ€™s what MLIR formalized.

Your Core Insight (which is correct)

You basically said:

â€œWasnâ€™t this already done?â€

Yes â€” at the level of kernels.
No â€” at the level of whole-program compilers for deep learning graphs.

Thatâ€™s the difference.

Clear One-Liner

Linear algebra itself wasnâ€™t novel â€” treating entire neural networks as compiler programs and pushing linear algebra reasoning into the IR before codegen was the real innovation.

If you want, I can show you:

How MLIR made Glow basically obsolete (architecturally)

Where TVM goes further than MLIR

Or how torch.compile today sits on top of all this

Just tell me which direction you want to go.

* * *

**You:**

Is Glow used nowadays? If so, what replaced it?

* * *

**ChatGPT:**

Short, honest answer:

No â€” Glow is not widely used today in production.
But its ideas absolutely survived and now live inside MLIR-based stacks and PyTorch 2.x.

Hereâ€™s the real story.

1. Is Glow used today?

Practically: no.

Glow was:

Actively developed at Facebook/Meta ~2017â€“2021

Open-sourced

Used internally for research and some deployment experiments

But today:

âœ… It is no longer a primary production compiler
âœ… Meta largely moved away from it
âœ… The ecosystem shifted toward MLIR and Triton

So Glow itself didnâ€™t â€œwin.â€

2. What replaced Glow?

Glowâ€™s role has been usurped by three main systems:

ğŸ”¹ A. MLIR (LLVMâ€™s ML stack)

This is the real structural replacement.

MLIR generalized everything Glow hardcoded:

Glow	MLIR
Fixed high-level IR	Dialects (mhlo, linalg, tensor, etc.)
Fixed low-level IR	Bufferization + affine + LLVM dialect
Built-in lowering pipeline	User-defined lowering pipelines

MLIR gave the ecosystem:
âœ… reusable infrastructure
âœ… composable dialects
âœ… much larger community

This is now the backbone for:

TensorFlow

XLA

Torch-MLIR

IREE

ğŸ”¹ B. XLA (still very alive)

XLA became:

The compiler for TensorFlow

The engine behind JAX

The path to TPU

Glowâ€™s â€œcompile the graphâ€ model survived, but:

XLA scaled better

Had first-party TPU support

Was better integrated with major frameworks

ğŸ”¹ C. PyTorch 2.x: torch.compile + Triton

This is the direct PyTorch successor to Glowâ€™s goals.

Instead of Glow, PyTorch now uses:

mathematica
TorchDynamo â†’ FX Graph â†’ TorchInductor â†’ Triton / CUDA / LLVM


Glowâ€™s ideas that survived here:
âœ… graph capture
âœ… whole-graph optimization
âœ… operator fusion
âœ… memory planning

But the backends are now:

Triton (custom GPU kernels)

LLVM

Vendor libraries

This made Glow redundant.

3. Why Glow Lost

Glow failed technically? â†’ No
Glow lost strategically â†’ Yes

Main issues:

âŒ Fixed architecture

Glow could not evolve as flexibly as MLIR.

âŒ Ecosystem gravity

Google poured massive resources into XLA + MLIR.

âŒ Backend strategy

Triton made writing GPU kernels vastly easier than Glow-style backends.

âŒ PyTorchâ€™s direction changed

PyTorch moved toward:

FX graphs

TorchInductor

Triton

4. Where Glowâ€™s DNA Lives Today

You can see Glowâ€™s ideas in:

Modern system	Glow idea it inherited
MLIR	Multi-level IR
XLA	Graph lowering + fusion
TorchInductor	Whole-graph compilation
IREE	Heterogeneous lowering
TVM	Memory planning + scheduling

Glow is less a â€œused productâ€ and more a historical stepping stone.

5. Current â€œWinningâ€ Stack (2024â€“2025)

If you asked: â€œWhat actually runs production today?â€

Google stack
markdown
TensorFlow / JAX
      â†“
     XLA
      â†“
     MLIR
      â†“
 CUDA / TPU / CPU

PyTorch stack
markdown
torch.compile
      â†“
  TorchDynamo
      â†“
 TorchInductor
      â†“
Triton / CUDA / LLVM


Glow is absent from both.

6. One brutally honest line

Glow didnâ€™t survive as a product, but it deeply shaped the mental model that modern ML compilers are built on.

7. If you wantâ€¦

I can explain:

Why Triton basically killed custom compiler backends

How MLIR became the LLVM of ML

Or where TVM still wins over MLIR in research

* * *

**You:**

Was it because glow was just focused on neural networks? 

* * *

**ChatGPT:**

Short answer: yes â€” and that focus was both its strength and its downfall.

Let me break down the real reason in a clean, honest way.

1. Glow Was Overfit to Neural Networks

Glow was designed assuming the world looked like this:

Mostly feed-forward neural nets

Dense tensors

Static shapes

Simple control flow

Linear algebra dominates

That worked great for:

âœ… CNNs (ResNet, VGG)
âœ… Early RNNs
âœ… Classic inference workloads

But the world shifted.

2. The World Changed After Glow

Modern ML is no longer just â€œneural networksâ€ in the classic sense.

New dominant patterns:

Transformers

Dynamic shapes

Ragged tensors

Sparse attention

MoE routing

Control flow inside models

Glow struggled here because its model was:

sql
NN model â†’ static graph â†’ full compilation â†’ static memory plan


But modern workloads want:

csharp
Dynamic graphs â†’ shape polymorphism â†’ partial compilation â†’ runtime specialization


Glow wasnâ€™t built for this world.

3. Glow vs MLIR: Core Contrast

This is the real architectural mismatch:

Glow	MLIR
â€œNeural network = programâ€	â€œAny computation = programâ€
Fixed IR layers	Extensible dialects
Tensor-only worldview	Arbitrary abstractions
Compiler tightly bound to domain	Compiler is a toolkit

Glow assumed:

â€œWe know what the domain looks like.â€

MLIR assumed:

â€œWe donâ€™t know the domain in advance.â€

MLIR won.

4. The Fatal Strategic Bet Glow Made

Glow bet on:

Neural nets remain a stable, narrow domain

Reality turned out to be:

ML became broader, messier, more dynamic

Glow could have evolved â€” but it would have required:

Redesigning the IR

Making it extensible

Supporting mixed abstraction levels

Supporting dynamic compilation

MLIR already existed to do exactly that.

5. XLA Didnâ€™t Have This Problem â€” Why?

XLA survived because:

It was tied to Googleâ€™s TPUs

It had strong MLIR integration later

It supported dynamic shape tracing via JAX

Glow had:
âŒ no hardware moat
âŒ no framework lock-in
âŒ fixed compiler design

6. PyTorch Didnâ€™t Want Another â€œHard Compilerâ€

PyTorchâ€™s philosophy shifted toward:

FX graphs

Python-level tracing

Triton kernels

Incremental adoption

Glow required:

large model rewrites

strict compilation pipeline

That clashed with PyTorchâ€™s design culture.

7. Your Instinct Is Right

You suspected:

â€œWas it because Glow was too focused on neural networks?â€

Thatâ€™s basically correct, but more precisely:

âœ… It was too focused on static neural nets
âœ… Too rigid in architecture
âœ… Not extensible to new domains

8. One-sentence, honest answer

Glow died not because it was wrong, but because it assumed neural networks would stay simple and predictable â€” and the field moved toward dynamic, heterogeneous, and extensible systems.

If you want, I can also explain:

Why TVM survived that transition better

Why Triton was the real PyTorch â€œGlow replacementâ€

Or how MLIR managed to be both general and low-level

* * *

**You:**

Is TVM still being used? why so

* * *

**ChatGPT:**

Short answer first:

Yes â€” TVM is still actively used, and it survives because it picked a very different niche than Glow.

Hereâ€™s the honest breakdown.

1. Is TVM still used today?

Yes, absolutely â€” but not everywhere.

TVM did not become the â€œuniversal ML compiler,â€ but it is:

âœ… Actively developed
âœ… Widely used in research
âœ… Heavily used in edge / embedded AI
âœ… Adopted inside some production stacks (especially where custom hardware exists)

Itâ€™s especially strong in:

Microcontrollers (microTVM)

ARM / RISC-V

NPUs / DSPs

Non-CUDA accelerators

2. Why TVM Survived When Glow Didnâ€™t

This is the real contrast:

Glow	TVM
Rigid compiler	Compiler + scheduler/search system
Static pipeline	Auto-tuning / search-based
Focused on NN structure	Focused on hardware specialization
Fixed lowering design	Exposed scheduling as a first-class concept

TVM said:

â€œThe hard part isnâ€™t graphs â€” itâ€™s schedules.â€

That turned out to be right.

3. TVMâ€™s Superpower: â€œSchedule as a Search Problemâ€

Unlike Glow and XLA, TVM doesnâ€™t just lower and optimize once.

It does:

pgsql
Graph â†’ Relay IR
      â†’ Tensor IR
      â†’ Search space of schedules
      â†’ Benchmark
      â†’ Cache best kernel


This is why it thrives on:

âœ… FPGA
âœ… ASIC
âœ… DSP
âœ… weird memory hierarchies
âœ… mobile NPUs

Glow and XLA were much more â€œstatic.â€

4. Where TVM Is Actually Used

Today, TVM is common in:

ğŸ”¹ Edge / Embedded

microTVM

ARM Cortex-M

RISCV systems

ğŸ”¹ Custom accelerators

Many startups use TVM to support their chips because:

âœ… It doesnâ€™t assume CUDA
âœ… It can autotune odd hardware
âœ… It exposes schedules directly

ğŸ”¹ Academic + Industry Research

TVM is basically:

the â€œLLVM for experimental ML hardwareâ€.

5. Why TVM Didnâ€™t Win the Datacenter

TVM struggled in big datacenters because:

Datacenter requirement	TVM tradeoff
Fast startup	Long autotuning times
Stable kernels	Schedule search can be brittle
Huge CUDA ecosystem	NVIDIA dominates here

In datacenters:

XLA dominates Google

Triton dominates PyTorch

6. How TVM Survives Alongside MLIR

They are no longer enemies.

Today:

Layer	Used tool
High-level IR	Relay, sometimes MLIR
Lowering infra	MLIR often
Scheduling	TVM still uniquely strong

Youâ€™ll see hybrid stacks where:

MLIR handles structure

TVM handles kernel tuning

7. One-line truth

TVM survived because it attacked the hardest unsolved problem â€” generating fast kernels for weird hardware â€” instead of trying to be the universal deep learning compiler.

8. When people still choose TVM

If you are:

âœ… building an NPU
âœ… targeting edge devices
âœ… working with non-standard memory hierarchies
âœ… optimizing for latency + energy

TVM is often the best tool.

9. Simple mental model
Stack	Role
MLIR	Compiler infrastructure
XLA	Production compiler for Googleâ€™s world
TVM	Research + hardware-specialization compiler
Triton	GPU kernel authoring system
10. If you wantâ€¦

I can explain:

How TVM compares to Triton

Why Triton beat TVM on GPUs

How microTVM works

Or what projects in 2024â€“2025 still build on TVM