**You:**

2301.13062v1.pdf
PDF
Can you summarize this paper

* * *

**ChatGPT:**

Here’s a structured summary of the paper “Operator Fusion in XLA: Analysis and Evaluation”

2301.13062v1

:

Overview

This paper investigates how the XLA compiler (widely used with TensorFlow and JAX) performs operator fusion, a key optimization that reduces memory transfers and kernel launch overhead on GPUs. While XLA is central to Google’s AI stack, its internal fusion mechanisms are poorly documented. The authors analyze XLA’s source code, benchmark it using a reinforcement learning task (Cartpole), and experiment with alternative strategies to understand its limitations and potential improvements.

Key Contributions

Analysis of Fusion Mechanisms

Detailed study of XLA’s fusion strategies:

Instruction Fusion (fusing dependent operations)

Fusion Merger (merging fusion ops to reduce memory bandwidth)

Multi-Output Fusion (sibling and producer-consumer fusion)

Horizontal Fusion (fusing many small elementwise ops to reduce launch overhead).

Identification of conservative rules in XLA that block potentially useful fusions (e.g., concatenation ops with multiple users, or custom kernels like cuRAND and cuDNN).

Case Study on JAX-Cartpole

The authors benchmark XLA on the Cartpole environment and trace how its HLO graph evolves during compilation.

They explore why certain fusions fail and implement optimizations:

Removing cuRAND kernels (1.87× speedup).

Modifying XLA to allow more aggressive fusion (small gains).

Rewriting code to remove unnecessary concatenation, enabling deeper fusion (3.41× speedup).

Loop unrolling to reduce launch overhead, achieving 3.5× speedup.

Comparisons

TorchScript: Achieves a 1.97× speedup over naive PyTorch due to its fusion compiler.

XLA-CPU: Outperforms GPU for small numbers of simulators (<70), but GPU wins at scale.

CUDA: A hand-written CUDA kernel remains 2.7× faster than their best XLA version, due to XLA framework overhead and extra kernel launches.

Findings

XLA’s rule-based fusion is conservative, avoiding potential performance wins to ensure general safety.

Performance depends heavily on the frontend code quality: poor Python implementations (e.g., excessive concatenations, Python loops) lead to suboptimal fusions.

XLA’s reliance on third-party libraries (cuDNN, cuBLAS) blocks further fusion opportunities.

Compilation overhead from JIT and loop tracing can dominate for small or irregular workloads.

Limitations Identified

Performance tied to frontend code quality.

Custom CUDA calls (e.g., cuRAND, cuDNN) prevent fusion.

Rule-based heuristics are rigid, sometimes missing safe optimizations.

JIT compilation introduces overhead that harms lightweight workloads.

Future Work

Extend fusion analysis to training workloads (where rematerialization and gradient storage complicate fusion).

Explore automatic loop unrolling.

Investigate fusion of simulation + neural network inference, possibly with multi-stream parallelism.

Explore autotuning-based strategies (as in TVM or DeepCuts) for more flexible fusion at the cost of longer compile times.

Conclusion

The study provides one of the first in-depth looks at XLA’s fusion internals, showing how it makes conservative trade-offs between safety and speed. While kernel fusion in XLA can yield up to 10.5× speedup, hand-optimized CUDA still outperforms it. The paper emphasizes that automatic kernel fusion remains an open research challenge in ML compilers

2301.13062v1

.

Would you like me to also make a 1–2 paragraph “student-style reflection” on this paper (similar to your earlier assignments), instead of just the structured summary?

* * *

**You:**

Can you explain why XLA fusion is so conservative and why they chose to use CuBLAS and CuDNN kernels

* * *

**ChatGPT:**

Great question — this gets at the philosophy behind XLA’s design choices.

Why XLA Fusion is Conservative

XLA uses rule-based fusion strategies instead of search-based autotuning (like TVM or DeepCuts). That means every fusion decision must be guaranteed not to harm performance across diverse models and hardware.

Avoiding Kernel Bloat: If too many operations are fused, the resulting kernel can exceed GPU hardware limits (threads per block, registers, shared memory). Oversized kernels can actually slow down execution.

Code Duplication Risks: Some fusions (e.g., concatenation with multiple consumers) require duplicating computations. XLA’s rules forbid these cases because they could balloon instruction count and hurt performance.

Safety Across Models: Since XLA is used in TensorFlow and JAX by millions of users, it must avoid pathological cases where an aggressive fusion strategy breaks performance. A conservative approach ensures “good enough” performance in most cases, even if it leaves performance on the table.

Engineering Simplicity: Rule-based fusion is faster to compile (no hours-long autotuning), so users benefit from XLA in interactive JAX/Python workflows without long delays.

Why XLA Uses cuBLAS and cuDNN

For heavy operations like matrix multiplications (GEMMs) and convolutions, XLA delegates to vendor-tuned kernels (cuBLAS, cuDNN).

Performance: NVIDIA has spent years hand-optimizing these libraries for each GPU generation (register tiling, memory layouts, tensor cores, etc.). It’s nearly impossible for a general-purpose compiler like XLA to consistently beat them.

Reliability: These libraries are well-tested and production-grade, so XLA inherits correctness and stability guarantees.

Time-to-Market: Implementing and tuning its own GEMM/conv kernels would be a huge engineering effort. Leveraging cuBLAS/cuDNN allows XLA to focus on fusion, scheduling, and graph-level optimization.

Portability: By calling into vendor libraries, XLA benefits from automatic updates when NVIDIA releases improvements, without needing to reimplement optimizations.

✅ In short:

Conservative fusion = ensures kernels don’t get too large, avoid unsafe duplication, keep compile time fast, and remain reliable across many workloads.

Using cuBLAS/cuDNN = lets XLA piggyback on NVIDIA’s highly tuned libraries for performance-critical ops, while focusing compiler effort on “glue code” fusion around them.

Do you want me to also break down what kinds of ops XLA fuses itself vs. what it offloads to cuBLAS/cuDNN? That could give you a clearer picture of the boundary between compiler-optimized and vendor-optimized kernels.

* * *

**You:**

But why does cuBLAS and cuDNN cause issues with fusion

* * *

**ChatGPT:**

Ah, good follow-up — this is one of the subtle pain points highlighted in the paper.

Why cuBLAS/cuDNN Break Fusion in XLA

XLA treats calls into cuBLAS (for GEMMs) and cuDNN (for convolutions) as “custom calls” in its HLO (High-Level Optimizer) IR. These are essentially opaque function calls that hand execution off to NVIDIA’s precompiled kernels.

The problem is:

Opaque Boundaries

Once an operation is lowered into a cuBLAS/cuDNN call, XLA no longer “sees inside” it. It can’t fuse elementwise ops that occur just before or after, because fusion requires generating a single GPU kernel where the producer and consumer share registers or shared memory.

Example: relu(matmul(A, B)) becomes → [cuBLAS GEMM] → [separate relu kernel]. XLA can’t inline the relu into the GEMM kernel because it doesn’t control GEMM’s implementation.

Memory Layout & Copies

cuBLAS/cuDNN impose strict layout requirements (e.g., NHWC for convolutions, column-major for GEMMs).

If XLA’s IR doesn’t match these layouts, it has to insert layout conversion ops before/after the library call. These conversions become separate kernels and block fusion with surrounding operations.

No Multi-Op Fusion

XLA’s multi-output fusion strategies (sibling fusion, producer-consumer fusion) can’t cross a library boundary. For example:

y1 = matmul(A, B)

y2 = softmax(y1)

In principle, XLA could fuse them into one kernel, but since matmul is a cuBLAS call, y1 must be materialized in GPU memory → softmax reads it back. That extra memory round trip kills the fusion benefit.

Limited Scheduling Flexibility

Vendor kernels are black boxes: XLA can’t reorder, tile, or pipeline them with surrounding ops. In contrast, if XLA controlled the kernel code, it could do things like “fuse convolution + bias-add + activation” into one pass.

Why XLA Accepts This Limitation

Performance tradeoff: cuBLAS/cuDNN are so well-optimized that the loss of fusion often costs less than replacing them with compiler-generated fused kernels.

Engineering cost: re-implementing and tuning GEMM/conv at XLA-level would be massive work and would still lag behind NVIDIA’s hand-tuned kernels.

Future direction: TPU-XLA doesn’t have this issue because Google owns both the hardware and compiler, so ops like convs can be fused directly. On GPUs, XLA is stuck at the mercy of vendor libraries.

✅ Bottom line:
cuBLAS/cuDNN cause fusion issues because they are opaque, precompiled kernels with strict layout requirements. Once XLA lowers to them, it loses the ability to inline, fuse, or optimize around them — leading to extra memory transfers and missed fusion opportunities.

Would you like me to sketch out a before/after diagram of how fusion looks with and without a cuBLAS/cuDNN boundary? That might make the blocking effect visually clearer.

* * *

**You:**

How do other ml compilers deal with using cuDNN/cuBLAS? Like pytorch2 for example

* * *

**ChatGPT:**

Great question — this is where different ML compilers make different tradeoffs between performance and flexibility. Let’s compare XLA with others like TorchInductor (PyTorch 2.0), TVM, etc.

1. XLA’s Approach (TensorFlow / JAX)

Default: Uses cuBLAS/cuDNN for GEMM/Conv because they’re highly optimized.

Problem: These custom calls block fusion with surrounding ops (e.g., activation, bias-add).

Result: XLA often has to launch multiple kernels with extra memory traffic.

2. PyTorch 2.0 (TorchInductor)

PyTorch 2.0 introduced TorchInductor as the default compiler backend. Its handling of cuBLAS/cuDNN is more flexible:

Elementwise Fusion: Like XLA, Inductor fuses elementwise ops aggressively (bias-add, ReLU, normalization).

Choice Between Library vs. Generated Kernels:

If a matmul/conv is large, Inductor calls cuBLAS/cuDNN (just like XLA).

If it’s small or if surrounding fusions are profitable, Inductor can generate custom Triton kernels that include the matmul/conv plus the fused ops.

Triton Integration: Since Inductor can JIT kernels in Triton (a DSL for GPU kernels), it’s able to fuse GEMM/Conv with adjacent elementwise ops in cases where XLA would defer to cuBLAS/cuDNN and lose fusion.

✅ Result: PyTorch 2.0 has more flexibility — it doesn’t always “lock in” to vendor libraries if it thinks it can do better with a custom kernel.

3. TVM / Ansor

Search-Based Strategy: TVM generates custom kernels for everything (including GEMM/Conv) and uses autotuning to approach vendor performance.

Fusion-Friendly: Because TVM “owns” the kernels, it can fuse arbitrary ops (e.g., Conv + BatchNorm + ReLU) into one kernel.

Tradeoff: Compile times are much longer (minutes to hours for tuning), making it less interactive compared to XLA or PyTorch 2.

4. TensorRT (NVIDIA’s Inference Compiler)

Aggressive Fusion with cuDNN/cuBLAS Integration: TensorRT can sometimes rewrite graphs to take advantage of special fused cuDNN kernels (e.g., Conv+Bias+Activation fused kernels NVIDIA provides).

Hybrid: Where no fused cuDNN kernel exists, TensorRT generates custom kernels.

Inference Focused: Optimized for static graphs and deployment, not interactive development.

Comparison Table
Compiler	Uses cuBLAS/cuDNN?	Can Fuse Around Them?	Notes
XLA	Yes (always for GEMM/Conv)	❌ No (opaque custom call)	Conservative, ensures correctness but loses fusion.
PyTorch 2.0 (Inductor)	Yes (large ops) / No (small ops)	✅ Sometimes (via Triton)	Hybrid strategy; can bypass vendor libs if fusion helps.
TVM/Ansor	No (generates custom kernels)	✅ Yes	Autotuning makes it slower to compile.
TensorRT	Yes, but uses special fused kernels	✅ Limited (via NVIDIA’s fused kernels)	Deployment-oriented, exploits vendor-provided fusions.
✅ Bottom Line

XLA = safe, conservative → always uses vendor libs, loses fusion opportunities.

PyTorch 2.0 = hybrid → can choose between vendor libraries or its own fused Triton kernels.

TVM = flexible, fully owns kernels → best fusion opportunities, but high compile cost.

TensorRT = middle ground → leans on vendor’s fused kernels when available.

Would you like me to go deeper into how TorchInductor decides when to fall back to cuBLAS/cuDNN vs. generating its own Triton kernel? That’s where its strategy really differs from XLA.

* * *

**You:**

so there is a tradeoff between compile time and fusion (speed ups during forward passes)?

* * *

**ChatGPT:**

Exactly ✅ — that’s the central tradeoff ML compilers face.

Fusion vs. Compile Time: The Tradeoff

Why Fusion Helps

Fusing ops into one kernel reduces:

Memory traffic (values stay in registers instead of being written to GPU DRAM and read back).

Kernel launch overhead (fewer launches, less scheduling latency).

This leads to higher throughput during the forward/backward pass.

Why Fusion Costs Compile Time

To decide what to fuse, a compiler might:

Run pattern-matching rules (fast, but conservative — like XLA).

Explore a design space of kernel schedules (slow, but finds better kernels — like TVM/Ansor).

Generate new kernels JIT (moderate cost — like TorchInductor with Triton).

More aggressive fusion requires more analysis and sometimes autotuning, which can take seconds → minutes → hours.

Different Compiler Strategies

XLA (rule-based, conservative): Fast compile time, but misses fusion opportunities. Good for interactive use (JAX notebooks).

TorchInductor (PyTorch 2.0): Middle ground — compiles in seconds, can fuse aggressively when profitable. Uses heuristics to decide if custom Triton kernels are worth it.

TVM/Ansor: Optimizes aggressively with autotuning → best fusion opportunities, but compile times can be hours. Best for deployment, not research iteration.

TensorRT: Relies on NVIDIA’s pre-fused kernels where possible, so compile time is short but fusion is limited to what the vendor provides.

Rule of Thumb

Short compile time → less fusion → slower runtime.

Longer compile time → more fusion → faster runtime.

Which side you prefer depends on your use case:

Research & prototyping (e.g., JAX, PyTorch dev mode): compile time matters more → conservative fusion.

Production inference at scale: one-time cost of long compile is acceptable → aggressive fusion/autotuning pays off.

✅ So yes, there’s always a balance: compilers trade developer iteration speed against maximum kernel efficiency.

Do you want me to sketch an example timeline (compile cost vs. runtime gain) showing when longer compile time is worth it and when it isn’t? That might make the tradeoff very concrete.