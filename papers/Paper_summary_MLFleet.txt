**You:**

2502.06982v2.pdf
PDF
Can you summarize this paper?

* * *

**ChatGPT:**

Here is a clear, concise summary of the paper ‚ÄúMachine Learning Fleet Efficiency: Analyzing and Optimizing Large-Scale Google TPU Systems with ML Productivity Goodput‚Äù 

2502.06982v2

üìÑ Summary

This paper proposes a new, holistic way to measure and improve the efficiency of modern ML supercomputing infrastructure‚ÄîML fleets‚Äîusing Google‚Äôs TPU production environment as the case study. Rather than focusing on traditional metrics like FLOPS, utilization, or duty cycle, the authors argue that ML fleets are too heterogeneous and dynamic for these to be meaningful.

To address this, the paper introduces ML Productivity Goodput (MPG), a metric that captures real forward progress of ML jobs across the entire system stack‚Äîfrom hardware, to schedulers, to runtime, to compiler, to model code. MPG is decomposable into three measurable components:

1. Scheduling Goodput (SG)

How often a job simultaneously has all the required accelerators available.
Captures fragmentation, topology mismatches, and scheduling churn.

2. Runtime Goodput (RG)

Given that resources are allocated, how often the job is actively making progress.
Captures overhead from checkpointing, compilation, data input bottlenecks, or job preemptions.

3. Program Goodput (PG)

Given the program is running, how close it is to theoretical peak performance.
Uses an HLO-level roofline model based on FLOPs, avoiding distortions caused by compiler fusions or rematerialization.

Together:

$$\text{MPG} = \text{SG} \times \text{RG} \times \text{PG}$$

This mirrors the classical "iron law" of CPU performance, but redesigned for distributed ML systems.

üß† Key Ideas
1. ML fleets are fundamentally different from WSCs and HPC systems

They have:

Rapidly changing hardware (multiple accelerator generations)

Rapidly shifting ML workloads (training vs bulk inference vs serving; LLMs; MoE)

Complex heterogeneity (embedding-heavy, compute-heavy, communication-bound)

Strong hardware‚Äìsoftware co-design requirements

Traditional metrics hide or obscure fleet-level inefficiencies.

2. MPG reveals bottlenecks that utilization metrics cannot

Examples:

High occupancy but low progress due to constant preemptions

High duty cycle but poor compiler efficiency

Hardware capacity that is unusable because topology does not match job requirements

Jobs ‚Äústuck‚Äù on input pipeline bottlenecks while accelerators sit idle

MPG separates these effects cleanly.

3. MPG allows targeted optimizations

The paper shows how Google used MPG to:

Optimize job scheduling for large TPU topologies (reduce fragmentation)

Improve runtime efficiency using:

Pathways (asynchronous dataflow)

AOT compilation and compile caching

Faster data input pipelines

Async checkpointing

Improve program efficiency with:

XLA algebraic simplification

Overlapping communication and compute

XTAT autotuning (general-purpose compiler autotuning framework)

Custom parallelization strategies for LLMs requiring 1000+ chips

Each improvement can be localized to SG, RG, or PG depending on where its effects appear.

4. MPG helps track hardware maturity over time

New accelerators initially show low PG because compilers/frameworks are not optimized yet.
As the compiler/runtime improves, PG increases.
As hardware is decommissioned, PG drops (drift, low usage, less tuning effort).

üéØ Why This Matters

The paper argues that modern ML infrastructure cannot be optimized using only local or hardware-centric metrics. Instead:

Efficiency must be understood end-to-end across the system stack.

Metrics must reflect productive work, not simply ‚Äúbusy hardware.‚Äù

Optimization must be guided by data that captures the reality of distributed ML execution.

MPG provides a unified framework to do this.

‚úîÔ∏è TL;DR

The paper introduces ML Productivity Goodput (MPG)‚Äîa decomposable, end-to-end metric for measuring and optimizing ML fleet performance‚Äîdemonstrated using Google‚Äôs TPU fleet. MPG captures true ‚Äúforward progress‚Äù across scheduling, runtime, and compiler/model execution, exposing inefficiencies hidden by traditional utilization metrics and enabling targeted optimization across the entire ML system stack. 

2502.06982v2

If you'd like, I can also provide:

A diagram-style summary

A comparison vs traditional metrics

A critical analysis of the paper

Potential research questions the paper opens up

A 5-minute ‚Äúexplain this to an ML systems interviewer‚Äù version

* * *

**You:**

Why are traditional metrics like Utilization, Occupancy, and Duty Cycle insufficient for ML fleets?

* * *

**ChatGPT:**

Traditional metrics like Utilization, Occupancy, and Duty Cycle are insufficient for ML fleets because they measure whether hardware is busy, not whether it is making meaningful forward progress on ML workloads. The paper makes this point very clearly, and highlights deep structural reasons why ML fleets break these classical metrics. Below is a clean breakdown.

üö® The Core Problem: "Busy" ‚â† "Productive" in ML Fleets

ML fleets (TPUs/GPUs running modern LLMs, recommendation systems, MoEs, etc.) are dominated by:

Distributed jobs requiring all chips to be ready at once

Complex runtimes with massive orchestration overhead

Rapid hardware heterogeneity

Massive compiler/runtime effects

Load imbalance, communication bottlenecks, checkpointing stalls

Traditional metrics completely miss these.

üîç Why Each Traditional Metric Fails
1. Utilization (e.g., % time device is doing work)
‚ùå What it misses

A TPU/GPU could be ‚Äúutilized‚Äù while:

Waiting on input pipelines

Running incorrectly fused kernels

Stalled on collectives during all-reduce

Running unnecessary computation

Doing work that will be discarded after a preemption (lost checkpoint)

üìå Key insight

Utilization cannot distinguish productive forward progress from wasted cycles.

2. Occupancy (fraction of chips allocated to jobs)

This is the scheduler‚Äôs favorite metric‚Äîbut it is misleading for ML.

‚ùå What it misses

A job can be "allocated" but:

Fragmented topology prevents real progress

Compiler JIT is still running

Input pipeline is starved

Checkpointing stalled training for minutes

One worker is down ‚Üí entire distributed job is blocked

üìå Key insight

Occupancy counts allocated chips, not whether any progress is being made at the ML job level.

Even with 100% occupancy, the fleet could be making near-zero useful progress.

3. Duty Cycle (fraction of time hardware is non-idle)

Duty cycle is typically used for TPUs/GPUs to see how often MXUs or SMs are active.

‚ùå What it misses

Kernel quality created by the compiler

Inefficient rematerialization

Excessive memory stalls

Suboptimal tile sizes

Communication bottlenecks

Variability in code generation across models/hardware

Time spent in ‚Äúuseless‚Äù computation (e.g., un-fused ops)

üìå Key insight

Duty cycle only measures whether hardware lanes are active ‚Äî not whether they are executing the right work efficiently.

üß® The ‚ÄúBig Three‚Äù Reasons These Metrics Fail in ML Fleets
1. ML Fleets Are Distributed & Bulk-Synchronous

Training jobs on TPUs/GPUs require all chips to be available and alive:

If one chip is down or slow, the entire job stops.

Traditional metrics treat chips independently.
ML workloads are interdependent.

2. ML Workloads Have Huge Runtime Overheads

Examples that traditional metrics ignore:

JIT compilation

Model initialization

Sharded weight loading

Checkpoint save/restore

Data pipeline stalls

Preemptions and lost work

Graph slicing / SPMD partitioning

A chip can be "utilized" during JIT or waiting on data, but that contributes zero training progress.

3. Hardware heterogeneity + software heterogeneity = chaos

ML fleets constantly change with:

New hardware generations (TPUv4 ‚Üí v5e ‚Üí v6?)

New runtimes (TensorFlow ‚Üí JAX ‚Üí Pathways)

New compiler passes (XLA optimizations)

New model classes (Mixture-of-Experts, LLMs, diffusion, recommenders)

Traditional metrics collapse all this complexity into a single number, which makes it impossible to diagnose real fleet bottlenecks.

üéØ The Paper‚Äôs TL;DR on Why These Metrics Are Bad

The paper explicitly says:

Utilization, Occupancy, and Duty Cycle:

Do not differentiate productive vs. unproductive work

Hide bottlenecks in data pipelines, checkpointing, or communication

Ignore topological scheduling constraints

Don‚Äôt reflect the fact that distributed workloads must progress synchronously

Cannot attribute inefficiency to the correct layer (scheduler vs runtime vs compiler vs program)

‚Üí Therefore they cannot guide optimization.

‚≠ê What ML Fleets Actually Need

A metric that measures:

Are the chips available? (Scheduling Goodput)

Are they doing productive work? (Runtime Goodput)

Is the executed program near its theoretical peak? (Program Goodput)

This is why the paper introduces ML Productivity Goodput (MPG) ‚Äî a real ‚Äúforward progress‚Äù metric.

‚úîÔ∏è Short Answer

Traditional metrics like Utilization, Occupancy, and Duty Cycle are insufficient for ML fleets because they measure how ‚Äúbusy‚Äù hardware is but do not measure actual ML progress. They hide bottlenecks from distributed execution, pipeline stalls, compiler inefficiencies, topology mismatches, and preemption waste. ML workloads are highly interdependent and dynamic, so productivity must be measured end-to-end‚Äînot just at the hardware level.

If you want, I can also provide:

A one-slide summary for interview prep

A comparison table: traditional metrics vs MPG

A diagram you can paste in a presentation

* * *

**You:**

How does the rapid evolution of ML models influence fleet optimization?

* * *

**ChatGPT:**

Rapid evolution of ML models fundamentally reshapes every layer of an ML fleet‚Äîhardware planning, scheduling, runtime design, compiler optimizations, and even datacenter architecture. The paper emphasizes that model evolution is one of the most destabilizing forces in ML fleet optimization.

Here‚Äôs a clear explanation of how and why evolving models constantly break existing optimizations.

üö® Why ML Model Evolution Disrupts Fleet Optimization

Modern ML fleets (like Google‚Äôs TPU fleet) operate at a massive scale and run fast-changing workloads:

LLMs replace CNNs

Diffusion models replace older image models

Recommendation models get larger embeddings

MoEs replace dense transformers

Training ‚Üí bulk inference ‚Üí real-time serving workloads shift rapidly

These transitions cause dramatic changes in compute/memory/communication requirements, breaking previously optimized configurations.

üß© 1. Model architectures radically shift hardware needs
When models change, the fleet suddenly needs:

Larger meshes (e.g., LLMs using 1k+ chips)

More communication bandwidth (e.g., attention-heavy models)

More memory per chip (e.g., giant embeddings)

Different numerical formats (bfloat16 ‚Üí FP8)

Different accelerator types (SparseCore vs MXU-heavy workloads)

Implication:
Schedulers, cluster topology, and accelerator mix must continuously adjust. What was optimal last quarter may be suboptimal today.

üì° 2. Different models stress different parts of the system

Examples:

Model Type	System Bottleneck
LLMs	Communication-bound (all-reduce, kernel launches)
MoEs	Routing imbalance, expert load distribution, collective ops
Recommendation models	Sparse lookups ‚Üí memory bandwidth and large embeddings
Diffusion models	High flop utilization but latency-sensitive
Real-time serving	Jitter + latency constraints rather than throughput

Implication:
Optimization strategies must be tailored per workload class‚Äînot per accelerator.

üîÑ 3. Workload composition shifts rapidly and unpredictably

The paper notes that fleet-wide demand changes dramatically within months, for example:

A sudden surge in bulk inference workloads

Growing dominance of extra-large training jobs

Adoption of new frameworks (JAX, Pathways)

New research models being deployed in production

This means:

‚ùå You cannot rely on stable, long-term optimization strategies.
‚úîÔ∏è Fleet optimization must be adaptive and reactive.
üõ†Ô∏è 4. Models evolve faster than compilers and runtimes can adapt

New models cause sudden inefficiencies:

Compiler passes tuned for CNNs might regress on Transformers

Pathways or JAX graph structures might break old XLA fusions

Unused HLO patterns previously untested might appear

Rematerialization heuristics could become wrong for LLM graphs

Implication:
Fleet operators must constantly tune:

XLA passes

Kernel fusions

Tensor layouts

Parallelization strategies

The paper shows examples of compiler optimizations that increased Program Goodput (PG) fleet-wide once adapted to new workloads.

‚ù§Ô∏è 5. Model evolution drives hardware‚Äìsoftware co-design

Examples from the paper:

TPUv4 added SparseCore because recommendation models shifted toward giant embeddings.

TPU runtimes added optimized communication/compute overlap for 500B-parameter LLMs.

Pathways was adopted fleet-wide because distributed workloads exploded.

Implication:
The fleet must adapt not just software but hardware deployment strategy.

‚öôÔ∏è 6. Rapid model evolution causes scheduling complexity

As models scale:

Job sizes vary from 1 chip ‚Üí thousands of chips

Topology constraints become tighter (must be on same pod or interconnected pods)

Fragmentation grows

Preemption becomes far more expensive (checkpointing large sharded models)

This directly impacts Scheduling Goodput (SG).

Large LLM training jobs create ‚Äúgravity wells‚Äù in the fleet‚Äîconsuming massive blocks of topology and forcing schedulers to defragment constantly.

üì¶ 7. Model evolution affects runtime Goodput (RG)

New models often require:

Larger or more frequent checkpoints

Different input pipelines

More complex sharding or SPMD partitioning

New collective patterns

Larger compilation graphs (e.g., multi-hour JIT compilation)

These introduce runtime stalls that weren‚Äôt issues for older models.

üéØ The Paper‚Äôs Core Message on This Topic

The model and data layer is the most volatile part of the ML system stack.
Its rapid changes propagate downward and cause instability in every other fleet layer ‚Äî hardware, scheduling, runtime, and compilers.

Thus, fleet optimization must be:

Model-aware

Continuous

End-to-end

Measured with a decomposable metric like MPG (SG √ó RG √ó PG)

‚úîÔ∏è Short Answer

The rapid evolution of ML models constantly changes the compute, memory, communication, and scheduling requirements of workloads. This destabilizes all lower layers of the ML fleet‚Äîhardware allocation, topology constraints, runtime overheads, compiler optimizations, and checkpointing behavior. As a result, fleet optimization must be dynamic, model-aware, and end-to-end. Traditional static optimization strategies fail because the workloads themselves change faster than the hardware and software stack can adapt.

If you want, I can also generate:

A diagram showing how model evolution propagates down the stack

A 5-line interview-ready answer

A hypothetical example of how introducing a new LLM disrupts a TPU fleet

How MPG (SG/RG/PG) specifically captures these changes

* * *

**You:**

What are the layers of the ML system stack shown in Figure 3, and how do they interact?

* * *

**ChatGPT:**

Below is a clean, structured explanation of the layers in the ML system stack from Figure 3 in the paper, along with how they interact and why each layer matters. The content is directly grounded in the uploaded paper (Google‚Äôs ‚ÄúML Productivity Goodput‚Äù paper) 

2502.06982v2

.

üî∑ Layers of the ML System Stack (from Figure 3)

Figure 3 decomposes the end-to-end ML fleet into five interacting layers, from hardware up to the user-level ML models.

These layers are:

Accelerators (Hardware Layer)

Scheduler

Runtime & Compiler

Framework

ML Model & Data

Below is a full explanation of each.

üß± 1. Accelerators (Hardware Layer)

Examples: TPUv4, TPUv5e, GPUs, CPUs, ASICs

This is the physical compute backbone of the fleet.
It includes:

Matrix multiply units (MXUs)

SparseCore (for embedding-heavy models)

Local HBM

Interconnects (optical or electrical mesh links)

Responsibilities

Provide raw compute, memory bandwidth, and interconnect bandwidth

Support distributed training topologies (1 chip ‚Üí multi-pod scale)

Interaction Upward

The hardware layer determines:

What kinds of models run efficiently

How large the topologies can be

How the scheduler must allocate resources

What the compiler must optimize for (tile sizes, fusions, layouts)

üì¶ 2. Scheduler

Examples: Borg, Kubernetes, MAST

The scheduler manages resource allocation across the fleet.

Responsibilities

Place jobs into chip meshes that meet topology requirements

Avoid fragmentation

Manage preemptions and evictions

Maintain high availability and fault tolerance

This layer must solve an NP-hard bin packing problem because ML workloads may require thousands of interconnected chips.

Interaction Upward

Determines when/where a job runs

Controls Scheduling Goodput (SG)

Interaction Downward

Must know hardware topology and health

Chooses which accelerators to allocate or leave idle

‚öôÔ∏è 3. Runtime & Compiler

Examples: Pathways runtime, XLA compiler

This layer actually executes ML programs on accelerators.

Runtime responsibilities

Initialize chips and connect them into a slice

Feed input data

Trigger JIT or load cached compiled programs

Handle checkpointing

Manage distributed execution (e.g., Pathways)

Compiler responsibilities

Translate framework graphs (TensorFlow/JAX) into HLO and then executable code

Perform operator fusion, rematerialization, layout selection

Generate optimized kernels for each accelerator type

Overlap compute & communication

Interaction Upward

Runs the model graphs produced by ML frameworks

Determines execution efficiency (Runtime Goodput + Program Goodput)

Interaction Downward

Emits code specialized to the hardware layer

Communicates device setup to the scheduler

üß∞ 4. Framework Layer

Examples: TensorFlow, JAX, PyTorch

This is the user-facing API layer, where ML practitioners define models and training loops.

Responsibilities

Provide abstractions like tf.data, nn.Module, jax.jit, pmap, pjit

Manage distributed training strategies (single-client vs multi-client)

Assemble computation graphs for compiler optimization

Integrate with datacenter services (storage, monitoring, RPCs)

Interaction Upward

The primary interface for model authors

Determines how models express parallelism, device placement, and data flow

Interaction Downward

Emits graphs or traced programs to the runtime/compiler layer

Determines sharding strategies, SPMD partitioning, or replication

üß† 5. ML Model & Data (Application Layer)

This is the topmost layer representing actual ML workloads that users care about:

LLMs

Vision models

Recommendation systems

Diffusion models

Reinforcement learning

Training, real-time serving, bulk inference

Responsibilities

Define the computational patterns (dense compute vs sparse lookups vs heavy all-reduce)

Drive data pipeline requirements

Determine memory and communication patterns

Interaction Downward

The model architecture dictates what frameworks/layers will emit

Model characteristics heavily influence runtime/compile behavior

Model size & parallelism drive scheduler decisions

üîó How All Layers Interact (Holistic View)

Here‚Äôs a simplified flow from bottom to top.

Hardware ‚Üí Scheduler

Scheduler must allocate accelerators in the right mesh shape

Hardware failures or topology constraints propagate upward

Scheduler ‚Üí Runtime/Compiler

Determines which chips execute the program

Fragmentation or preemption directly affects runtime efficiency

Runtime/Compiler ‚Üí Framework

Compilers expect framework graphs in certain forms

Frameworks may need to adapt code to avoid inefficient fusions or communication patterns

Framework ‚Üí Model

The framework defines:

how the model is parallelized

sharding strategies

input pipeline setup

Model ‚Üí Entire Stack

Model architecture choices

change compiler behavior

change runtime overhead

change scheduling needs

change hardware provisioning strategy

This is why the paper emphasizes hardware/software co-design and decomposability of MPG across stack layers.

‚≠ê In One Sentence

The ML system stack consists of Accelerators ‚Üí Scheduler ‚Üí Runtime/Compiler ‚Üí Framework ‚Üí ML Model & Data, where each layer depends on and heavily influences the performance of the layers above and below it‚Äîmaking end-to-end fleet optimization a multilayer co-design problem rather than an isolated hardware or software tuning task.

If you want, I can also generate:

A diagram summarizing the stack (ASCII or mermaid)

A ‚Äústack explainer‚Äù suitable for slides/interview prep

An explanation of which MPG component each layer affects

* * *

**You:**

Why is hardware heterogeneity such a challenge for ML fleet operations?

* * *

**ChatGPT:**

Hardware heterogeneity is one of the central, system-wide challenges in ML fleet operations because it makes every layer of the ML stack harder to optimize: scheduling becomes a constraint-satisfaction nightmare, compilers must target multiple architectures simultaneously, runtimes must abstract incompatible behaviors, and models must adapt to hardware capabilities that differ in performance, memory, communication bandwidth, and numerical formats.

The paper emphasizes that heterogeneity is fundamentally destabilizing and growing quickly, especially in ML fleets with many generations of TPUs or GPUs. Below is a clear breakdown of why.

üß® 1. Each hardware generation has different capabilities

Different accelerators vary in:

FLOPs and tensor core throughput

Memory capacity (HBM size)

Memory bandwidth

Interconnect topology / bandwidth

Numerical format support (BF16, FP8, INT4, etc.)

Specialized units (e.g., TPU SparseCore for giant embeddings)

Because these vary widely:

‚Üí The same model may run well on one accelerator and poorly on another.
‚Üí Compiler passes must be re-tuned per generation.
‚Üí Frameworks may generate different HLO graphs depending on hardware.

This creates non-uniform performance, making it nearly impossible to have one-size-fits-all optimizations.

üß© 2. Heterogeneous hardware shatters scheduling simplicity

ML workloads‚Äîespecially training‚Äîrequire all chips for a job to be:

Available

Healthy

In the correct topology

From the same hardware generation

Even two similar accelerators (e.g., TPUv4 vs TPUv5e) may not be substitutable.

This leads to:

Fragmentation (unusable islands of chips)

Long queue delays for jobs requiring homogeneous meshes

Preemption difficulty (cannot simply move a job to a new chip type)

Eviction cascading failures for large training jobs

The paper explicitly calls this an NP-hard bin-packing problem.

üöö 3. Heterogeneity complicates fleet-wide optimization rollouts

When hardware differs:

A compiler optimization might improve performance on TPUv4 but hurt TPUv3 or v5e.

A runtime feature (e.g., new SPMD partitioning) may only work best on newer interconnect topologies.

Input pipeline throughput may mismatch hardware bandwidth.

A kernel fusion on one generation may overflow register files on another.

This means any meaningful improvement must be validated across all hardware types, dramatically increasing operational overhead.

üîß 4. Co-design between hardware and software breaks down

Modern ML accelerators are increasingly co-designed with workloads:

TPUv4 introduced SparseCore to accelerate large embedding tables.

New generations may optimize for LLMs, or for lower-power inference.

Some accelerators are optimized for dense compute; others for sparse operations.

When fleets contain multiple generations, model authors cannot rely on a stable hardware target.

Result:

Models must be written with multiple hardware capabilities in mind.

Compilers must maintain multiple backend codegen paths.

Frameworks must produce different sharding strategies depending on which hardware a job lands on.

This increases operational complexity dramatically.

üõ∞Ô∏è 5. Heterogeneity breaks uniform distributed training assumptions

Distributed training (e.g., JAX pjit, TensorFlow DTensor, Pathways) assumes:

Uniform device speeds

Uniform communication bandwidth

Uniform memory capacity

With heterogeneous accelerators:

Slow chips become bottlenecks for the entire distributed job.

Since large meshes must wait for the slowest device:

Step time increases

Checkpointing delays progress

Sync collectives stall

Thus, heterogeneity directly lowers Program Goodput and Runtime Goodput.

üìä 6. Hardware heterogeneity leads to uneven adoption and maturity

The paper notes a consistent pattern:

When a new chip type is added, its Program Goodput (PG) starts low.

PG increases as compiler + runtime + framework optimizations catch up.

PG decreases when hardware approaches retirement due to drift and lower tuning investment.

This means different hardware generations have different levels of software maturity, further complicating fleet-wide tuning.

üí∞ 7. Cost-efficiency optimization becomes multi-objective

Different chip types have different:

Performance per watt

Performance per dollar

Failure rates

Network performance

Cooling and power constraints

This makes placement and scheduling a multi-dimensional optimization problem.

üéØ In One Sentence

Hardware heterogeneity is a challenge because ML workloads require tightly synchronized, homogeneous execution across large accelerators, and differences across hardware generations amplify fragmentation, scheduling complexity, compiler instability, runtime inefficiencies, and model-level constraints‚Äîmaking end-to-end fleet optimization exponentially harder.

If you‚Äôd like, I can also provide:

A visual diagram showing how heterogeneity propagates up the stack

A concise ‚Äúinterview-ready‚Äù answer

Example scenarios: how heterogeneity breaks training or serving workloads

How MPG (SG/RG/PG) specifically captures heterogeneity‚Äôs impact

* * *

**You:**

How do different frameworks (JAX, PyTorch, TensorFlow) impact fleet efficiency?

* * *

**ChatGPT:**

Different ML frameworks‚ÄîJAX, PyTorch, and TensorFlow‚Äîhave major, measurable impacts on ML fleet efficiency because they differ in how they:

Express computation graphs

Handle distributed execution

Interact with compilers (XLA / Glow / etc.)

Manage input pipelines

Trigger compilation and runtime overhead

Integrate with schedulers and datacenter services

The paper highlights that framework choice materially affects Runtime Goodput (RG) and Program Goodput (PG), and shifts the operational behavior of the entire ML stack. Below is a clear breakdown of why and how.

üî∑ 1. Frameworks differ in how they express computation graphs ‚Üí affects Program Goodput (PG)
JAX

Uses pure functional, staged computation (via jit, pmap, pjit)

Produces clean, optimization-friendly HLO graphs

Encourages fusion-friendly code because of immutability and tracing

Leads to higher PG on many workloads (as reflected in the paper)

TensorFlow

Supports both static (Graph mode) and dynamic (eager) execution

Large ecosystem of ops ‚Üí some may not lower efficiently to XLA

Older models rely heavily on custom ops with less predictable fusion behavior

More variability in PG depending on how the graph was built

PyTorch

Historically eager-first; PyTorch/XLA and torch.compile are improving this

Dynamic graphs often harder to optimize for TPU

Model code may generate many small kernels causing lower PG

More overhead in bridging dynamic control flow ‚Üí runtime stalls

Bottom line:
Frameworks with cleaner, more static, more optimizable graphs (like JAX) tend to achieve higher program efficiency in a TPU fleet.

üî∑ 2. Frameworks differ in distributed training architecture ‚Üí impacts Runtime Goodput (RG)
Single-client vs Multi-client Execution

The paper highlights this directly:

JAX + Pathways = single-client, centralized control
‚Üí Less orchestration overhead
‚Üí Faster startup
‚Üí Better checkpoint coordination
‚Üí Higher RG

TensorFlow‚Äôs multi-client strategies (e.g., ParameterServerStrategy, MirroredStrategy)
‚Üí More distributed control paths
‚Üí More network chatter
‚Üí More synchronization overhead
‚Üí Worse performance under large-scale workloads

PyTorch often mimics TF multi-client behavior
‚Üí Worker divergence
‚Üí More sensitivity to stragglers
‚Üí More checkpoint variability

Concrete example from the paper:

Pathways-based workloads tend to have higher Runtime Goodput because Pathways is single-client and tuned for large distributed jobs.


2502.06982v2

üî∑ 3. Framework input pipeline efficiency varies ‚Üí impacts RG

TensorFlow has tf.data, which is powerful but can become a bottleneck. Framework-level improvements (like Plumber) help identify stalls.

JAX often relies on NumPy-style pipelines or TFRecord readers; Pathways allows sharded data ingestion.

PyTorch uses DataLoader, which may underperform on TPU without hand-tuning.

The paper shows that input pipeline bottlenecks directly reduce Runtime Goodput.

üî∑ 4. Compilation behavior differs massively across frameworks ‚Üí impacts both RG and PG
JAX

Heavy use of just-in-time compilation

Graph tracing produces large, monolithic HLO graphs

Can cause long initial compile times

But results in high-performance, fused kernels

TensorFlow

Graph mode can compile efficiently

Eager mode may fall back to slower paths

Some ops are not XLA-compatible

PyTorch

Historically least integrated with XLA

PyTorch/XLA adds overhead translating PyTorch IR ‚Üí HLO

More unstable fusion behavior

Often lower PG unless models are written with TPU-optimized patterns

In fleet operations, compilation time counts as off-duty time, lowering RG.

üî∑ 5. Framework maturity affects stability and adoption ‚Üí impacts Scheduling Goodput (SG)

Frameworks differ in:

Startup/shutdown time

Checkpoint consistency

Failure recovery

Interaction with orchestration layers

For example:

JAX+Pathways has predictable startup and strong single-client semantics

TF multi-client jobs may have staggered readiness ‚Üí longer ‚Äúall-allocated‚Äù convergence time

PyTorch/XLA jobs may fail in rare edge cases, causing scheduler churn

These differences affect the window during which all chips are ready simultaneously, which SG measures.

üî∑ 6. Frameworks dictate model authoring style ‚Üí influences fleet-wide patterns

This is subtle but important:

JAX users tend to write code with SPMD partitioning (pjit, xmap), which scales well to large TPU meshes.

TensorFlow users rely more on canned distribution strategies, which may not be performant across model types.

PyTorch users often port GPU-centric architectures that do not map optimally to TPU interconnects.

As a result:

Framework choice shapes the entire population of workload topologies in the fleet.

This directly affects scheduling patterns, fragmentation, and long-term fleet planning.

‚≠ê Final Summary: How Frameworks Impact ML Fleet Efficiency

Framework choice affects all three components of ML Productivity Goodput (MPG):

Framework Effect	Scheduling Goodput (SG)	Runtime Goodput (RG)	Program Goodput (PG)
Startup synchronization	‚úî Impacts all-allocated time	‚Äî	‚Äî
Graph structure & sharding	‚Äî	‚Äî	‚úî Huge impact
Input pipeline efficiency	‚Äî	‚úî Major impact	‚Äî
Compilation dynamics	‚Äî	‚úî Impacts off-duty time	‚úî Impacts HLO quality
Distributed architecture (single vs multi-client)	‚úî Affects scheduler coordination	‚úî Affects runtime stalls	‚Äî
Integration with XLA/TPU kernels	‚Äî	‚Äî	‚úî Big differences

JAX tends to give the best RG + PG on TPU fleets because its pure functional semantics and tracing integrate cleanly with XLA and Pathways.

TensorFlow is mature and flexible, but graph vs eager mode introduces variability.

PyTorch provides great usability but often yields lower PG and sometimes lower RG in TPU environments due to dynamic graphs and extra indirection.

If you want, I can also generate:

A one-page table of differences

A diagram of how frameworks map onto XLA/TPU execution

A deeper dive into why Pathways improves RG so much

A hypothetical scenario comparing framework performance on an LLM training job

* * *

**You:**

What are the limitations of the MPG metric?

* * *

**ChatGPT:**

Here are the key limitations of the ML Productivity Goodput (MPG) metric as implied throughout the paper. Even though the metric is extremely useful, it is not perfect‚Äîand the authors hint at several structural challenges in using MPG at fleet scale.

‚≠ê 1. MPG depends heavily on accurate definitions of ‚Äúproductive work‚Äù

MPG measures forward progress, but what counts as ‚Äúprogress‚Äù is:

Checkpoint-dependent

Model-dependent

Framework-dependent

Phase-dependent (training vs inference)

For example:

If a training step completes but does not checkpoint before preemption ‚Üí MPG counts it as zero progress.

For inference, ‚Äúprogress‚Äù must be defined relative to serving throughput or batch completion.

Limitation:
MPG relies on somewhat subjective or workload-specific definitions of productivity.

‚≠ê 2. MPG cannot fully capture model-level algorithmic efficiency

MPG‚Äôs Program Goodput (PG) uses a compute-based roofline derived from HLO-level FLOP counts. But:

The FLOP count assumes a na√Øve version of the program

It does not capture algorithmic inefficiencies (e.g., attention scaling tricks, sparse models, KV cache reuse)

It does not reflect memory-bound effects explicitly

It does not model communication-heavy architectures precisely

So:

Limitation:
MPG cannot distinguish whether low PG is caused by algorithmic choices or compiler/runtime issues.

‚≠ê 3. MPG ignores cross-job interference and cluster-level contention

MPG breaks efficiency into scheduling, runtime, and program goodput per job, but:

Interference between jobs (network hotspots, I/O contention)

Shared interconnect congestion

Cross-model cache partitioning

Host-level CPU saturation

are not directly reflected in MPG.

The metric only measures job-level effects, not fleet-level emergent behavior.

‚≠ê 4. MPG is highly sensitive to checkpointing behavior

A significant flaw:

If a job fails right before checkpointing, all progress since the last checkpoint is discarded from RG.

Different frameworks checkpoint at different frequencies.

Larger models (e.g., sharded LLMs) checkpoint far more slowly.

Because of this:

Limitation:
MPG can underestimate progress for workloads with infrequent or expensive checkpoints.

‚≠ê 5. Difficulty isolating shared responsibility across layers

MPG decomposes the stack into:

Scheduling Goodput

Runtime Goodput

Program Goodput

But performance problems frequently span layers. Example:

A compiler-induced imbalance may cause straggler workers ‚Üí interpreted as runtime inefficiency (RG).

A framework graph change may produce fusions that degrade PG, but MPG cannot directly tell whether the blame lies in the compiler, framework, or runtime.

Limitation:
MPG cannot fully separate multi-layer causal effects‚Äîroot cause analysis still requires deeper telemetry.

‚≠ê 6. MPG assumes uniform device progress in distributed workloads

In synchronized training (e.g., JAX SPMD, TF DTensor):

One slow device slows the entire step.

But MPG attributes the slowdown to RG or PG without revealing which subset of devices were stragglers.

That is:

Limitation:
MPG hides internal heterogeneity of progress across replicas or partitions.

‚≠ê 7. MPG does not capture latency metrics for real-time serving

MPG is built around ‚Äúproductive chip-time,‚Äù but:

Real-time serving workloads care about:

P99/P999 latency

Jitter

Burst response

Cold-start cost

These are not naturally encoded in MPG.

Limitation:
MPG is throughput-centric, making it less suitable for evaluating high-SLA serving workloads.

‚≠ê 8. MPG does not consider energy efficiency directly

While MPG measures productivity, it does not measure:

Energy per step

Energy per token

Power spikes

Thermal throttling

Carbon-aware scheduling

In an era of massive fleets, these matter.

Limitation:
MPG is not an energy-aware metric.

‚≠ê 9. MPG may lag behind rapid model evolution

As model architectures shift (LLMs ‚Üí MoEs ‚Üí diffusion ‚Üí agents):

The underlying roofline model assumptions evolve

The data pipeline patterns change

New forms of runtime overhead appear

MPG may need constant retuning to accurately model new workloads.

Limitation:
MPG is not architecture-neutral; it must evolve with model classes.

‚≠ê 10. MPG does not capture the cost/performance tradeoff

MPG measures efficiency in technical terms, not economic ones.

Two workloads can have identical MPG but massively different:

cost per training step

cost per inference batch

cost per model checkpoint

power cost

Limitation:
MPG alone cannot guide cost optimization.

üéØ Summary: Key Limitations of MPG

Here is the concise version:

Relies on subjective definitions of ‚Äúproductive work.‚Äù

Cannot differentiate algorithmic inefficiency from compiler/runtime issues.

Ignores cross-workload contention effects.

Penalizes workloads with infrequent checkpointing.

Struggles to separate multi-layer root causes.

Assumes uniform progress across distributed devices.

Not designed for latency-sensitive serving workloads.

Lacks energy-awareness.

Needs continuous updating as model architectures evolve.

Not connected to economic cost models.

If you want, I can also produce:

‚úÖ A comparison table: MPG vs traditional metrics vs cost metrics
‚úÖ A diagram showing MPG blind spots
‚úÖ Recommendations for improving MPG in future ML fleets